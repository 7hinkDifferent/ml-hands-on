{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bd5187",
   "metadata": {},
   "source": [
    "# PyTorch深度学习完全教程\n",
    "\n",
    "PyTorch是当前最流行的深度学习框架之一，以其动态计算图、直观的API设计和强大的GPU加速能力而著称。本教程将带你从基础到高级，全面掌握PyTorch在实际项目中的应用。\n",
    "\n",
    "## 🎯 学习目标\n",
    "\n",
    "通过本教程，你将掌握：\n",
    "- **PyTorch基础**：张量操作、自动微分、计算图\n",
    "- **模型构建**：自定义网络架构、层的组合、参数管理\n",
    "- **数据处理**：自定义数据集、数据加载器、数据增强\n",
    "- **训练流程**：完整的训练/验证/测试循环、损失函数、优化器\n",
    "- **模型管理**：断点保存恢复、模型版本控制、最佳模型选择\n",
    "- **实验跟踪**：TensorBoard可视化、日志记录、超参数跟踪\n",
    "- **生产部署**：模型导出、推理优化、性能监控\n",
    "\n",
    "## 📋 完整内容大纲\n",
    "\n",
    "### 1. 环境配置与PyTorch基础\n",
    "- CUDA环境检查\n",
    "- 张量创建与操作\n",
    "- 自动微分机制\n",
    "\n",
    "### 2. 自定义神经网络模型\n",
    "- 基础模型架构\n",
    "- 卷积神经网络（CNN）\n",
    "- 高级网络组件\n",
    "\n",
    "### 3. 自定义数据集与数据加载\n",
    "- MNIST数据集处理\n",
    "- 自定义Dataset类\n",
    "- 数据预处理与增强\n",
    "\n",
    "### 4. 完整训练流程\n",
    "- 训练、验证、测试划分\n",
    "- 损失函数与优化器\n",
    "- 学习率调度\n",
    "\n",
    "### 5. 断点保存与恢复\n",
    "- 模型状态保存\n",
    "- 训练中断恢复\n",
    "- 最佳模型管理\n",
    "\n",
    "### 6. TensorBoard可视化\n",
    "- 损失与指标跟踪\n",
    "- 模型结构可视化\n",
    "- 参数分布监控\n",
    "\n",
    "### 7. 高级日志与实验管理\n",
    "- 结构化日志记录\n",
    "- 超参数实验跟踪\n",
    "- 性能分析工具\n",
    "\n",
    "### 8. 实际项目案例\n",
    "- MNIST手写数字识别\n",
    "- 端到端项目流程\n",
    "- 最佳实践总结\n",
    "\n",
    "**重点案例**：基于MNIST数据集的手写数字分类，从零构建完整的深度学习项目，包含所有生产级特性。\n",
    "\n",
    "PyTorch以其Pythonic的设计哲学和研究友好的特性，成为学术界和工业界的首选深度学习框架。让我们开始这个精彩的学习之旅！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PyTorch深度学习环境配置 ===\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# 设置警告过滤\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子，确保结果可重现\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"设置随机种子以确保实验可重现性\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 环境信息检查\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"Torchvision版本: {torchvision.__version__}\")\n",
    "print(f\"NumPy版本: {np.__version__}\")\n",
    "\n",
    "# CUDA环境详细检查\n",
    "print(f\"\\n=== GPU环境信息 ===\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN版本: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"GPU设备数量: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  显存总量: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"未检测到CUDA设备，将使用CPU训练\")\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n使用设备: {device}\")\n",
    "\n",
    "# 创建项目目录结构\n",
    "project_dirs = {\n",
    "    'models': 'saved_models',\n",
    "    'logs': 'logs', \n",
    "    'tensorboard': 'runs',\n",
    "    'data': 'data',\n",
    "    'checkpoints': 'checkpoints',\n",
    "    'outputs': 'outputs'\n",
    "}\n",
    "\n",
    "print(f\"\\n=== 创建项目目录结构 ===\")\n",
    "for name, path in project_dirs.items():\n",
    "    Path(path).mkdir(exist_ok=True)\n",
    "    print(f\"✓ {name}: {path}/\")\n",
    "\n",
    "# 配置matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# PyTorch设置优化\n",
    "if torch.cuda.is_available():\n",
    "    # 启用自动混合精度训练\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"✓ 启用cuDNN基准测试模式以优化性能\")\n",
    "\n",
    "print(f\"\\n=== 环境配置完成 ===\")\n",
    "print(f\"项目初始化时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 验证PyTorch基本功能\n",
    "print(f\"\\n=== PyTorch功能验证 ===\")\n",
    "# 创建测试张量\n",
    "test_tensor = torch.randn(2, 3, device=device)\n",
    "print(f\"测试张量创建成功: {test_tensor.shape} on {test_tensor.device}\")\n",
    "\n",
    "# 测试自动微分\n",
    "x = torch.tensor([2.0], requires_grad=True, device=device)\n",
    "y = x ** 2 + 3 * x + 1\n",
    "y.backward()\n",
    "print(f\"自动微分测试: f(2) = {y.item():.2f}, f'(2) = {x.grad.item():.2f}\")\n",
    "\n",
    "print(f\"✓ PyTorch环境验证完成，准备开始深度学习之旅！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63662bc",
   "metadata": {},
   "source": [
    "## 2. PyTorch张量基础\n",
    "\n",
    "张量（Tensor）是PyTorch的核心数据结构，类似于NumPy的数组但支持GPU加速和自动微分。掌握张量操作是深度学习的基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 张量创建与基本操作\n",
    "print(\"=== PyTorch张量基础操作 ===\")\n",
    "\n",
    "# 2.1.1 张量创建的多种方式\n",
    "print(\"1. 张量创建方式:\")\n",
    "\n",
    "# 从Python列表创建\n",
    "tensor_from_list = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=device)\n",
    "print(f\"从列表创建: \\n{tensor_from_list}\")\n",
    "\n",
    "# 创建特殊张量\n",
    "zeros_tensor = torch.zeros(3, 4, device=device)\n",
    "ones_tensor = torch.ones(2, 3, device=device)\n",
    "random_tensor = torch.randn(2, 3, device=device)\n",
    "arange_tensor = torch.arange(0, 10, 2, device=device)\n",
    "\n",
    "print(f\"\\n零张量 (3x4): \\n{zeros_tensor}\")\n",
    "print(f\"\\n单位张量 (2x3): \\n{ones_tensor}\")\n",
    "print(f\"\\n随机张量 (2x3): \\n{random_tensor}\")\n",
    "print(f\"\\n等差数列张量: {arange_tensor}\")\n",
    "\n",
    "# 根据现有张量创建\n",
    "like_tensor = torch.zeros_like(tensor_from_list)\n",
    "rand_like_tensor = torch.randn_like(tensor_from_list)\n",
    "\n",
    "print(f\"\\n类似形状的零张量: \\n{like_tensor}\")\n",
    "print(f\"\\n类似形状的随机张量: \\n{rand_like_tensor}\")\n",
    "\n",
    "# 2.1.2 张量属性\n",
    "print(f\"\\n2. 张量属性:\")\n",
    "sample_tensor = torch.randn(2, 3, 4, device=device)\n",
    "print(f\"张量形状: {sample_tensor.shape}\")\n",
    "print(f\"张量维度: {sample_tensor.dim()}\")\n",
    "print(f\"张量数据类型: {sample_tensor.dtype}\")\n",
    "print(f\"张量设备: {sample_tensor.device}\")\n",
    "print(f\"张量大小: {sample_tensor.size()}\")\n",
    "print(f\"元素总数: {sample_tensor.numel()}\")\n",
    "\n",
    "# 2.1.3 张量运算\n",
    "print(f\"\\n3. 张量运算:\")\n",
    "\n",
    "# 基本数学运算\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device=device)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"张量 a: \\n{a}\")\n",
    "print(f\"张量 b: \\n{b}\")\n",
    "\n",
    "# 元素级运算\n",
    "print(f\"\\n加法: \\n{a + b}\")\n",
    "print(f\"减法: \\n{a - b}\")\n",
    "print(f\"乘法: \\n{a * b}\")\n",
    "print(f\"除法: \\n{a / b}\")\n",
    "print(f\"幂运算: \\n{a ** 2}\")\n",
    "\n",
    "# 矩阵运算\n",
    "print(f\"\\n矩阵乘法: \\n{torch.mm(a, b)}\")\n",
    "print(f\"矩阵乘法 (操作符): \\n{a @ b}\")\n",
    "\n",
    "# 统计运算\n",
    "sample_data = torch.randn(100, device=device)\n",
    "print(f\"\\n统计运算 (100个随机数):\")\n",
    "print(f\"均值: {sample_data.mean().item():.4f}\")\n",
    "print(f\"标准差: {sample_data.std().item():.4f}\")\n",
    "print(f\"最大值: {sample_data.max().item():.4f}\")\n",
    "print(f\"最小值: {sample_data.min().item():.4f}\")\n",
    "print(f\"求和: {sample_data.sum().item():.4f}\")\n",
    "\n",
    "# 2.1.4 张量变形\n",
    "print(f\"\\n4. 张量变形操作:\")\n",
    "original = torch.arange(12, device=device)\n",
    "print(f\"原始张量: {original}\")\n",
    "\n",
    "# 重塑\n",
    "reshaped = original.view(3, 4)\n",
    "print(f\"重塑为 3x4: \\n{reshaped}\")\n",
    "\n",
    "# 添加维度\n",
    "unsqueezed = original.unsqueeze(0)  # 在第0维添加\n",
    "print(f\"添加维度后: {unsqueezed.shape}\")\n",
    "\n",
    "# 移除维度\n",
    "squeezed = unsqueezed.squeeze(0)  # 移除第0维\n",
    "print(f\"移除维度后: {squeezed.shape}\")\n",
    "\n",
    "# 转置\n",
    "matrix = torch.randn(3, 4, device=device)\n",
    "transposed = matrix.t()\n",
    "print(f\"原矩阵形状: {matrix.shape}\")\n",
    "print(f\"转置后形状: {transposed.shape}\")\n",
    "\n",
    "# 2.1.5 张量索引与切片\n",
    "print(f\"\\n5. 张量索引与切片:\")\n",
    "data = torch.arange(24, device=device).view(4, 6)\n",
    "print(f\"原始数据 (4x6): \\n{data}\")\n",
    "\n",
    "# 基本索引\n",
    "print(f\"第一行: {data[0]}\")\n",
    "print(f\"第一列: {data[:, 0]}\")\n",
    "print(f\"左上角2x2: \\n{data[:2, :2]}\")\n",
    "\n",
    "# 布尔索引\n",
    "mask = data > 10\n",
    "print(f\"大于10的元素: {data[mask]}\")\n",
    "\n",
    "# 高级索引\n",
    "indices = torch.tensor([0, 2], device=device)\n",
    "print(f\"选择第0和第2行: \\n{data[indices]}\")\n",
    "\n",
    "# 2.1.6 就地操作与内存管理\n",
    "print(f\"\\n6. 就地操作与内存管理:\")\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, device=device)\n",
    "print(f\"原始张量: {x}\")\n",
    "print(f\"内存地址: {x.data_ptr()}\")\n",
    "\n",
    "# 非就地操作\n",
    "y = x + 1\n",
    "print(f\"非就地操作结果: {y}\")\n",
    "print(f\"原始张量不变: {x}\")\n",
    "\n",
    "# 就地操作\n",
    "x.add_(1)  # 等价于 x += 1\n",
    "print(f\"就地操作后: {x}\")\n",
    "\n",
    "# 内存连续性\n",
    "non_contiguous = torch.randn(3, 4, device=device).t()\n",
    "print(f\"是否内存连续: {non_contiguous.is_contiguous()}\")\n",
    "contiguous = non_contiguous.contiguous()\n",
    "print(f\"连续化后: {contiguous.is_contiguous()}\")\n",
    "\n",
    "# 2.1.7 与NumPy的互操作\n",
    "print(f\"\\n7. 与NumPy的互操作:\")\n",
    "if device.type == 'cpu':\n",
    "    # 只有CPU张量可以与NumPy直接互转\n",
    "    torch_tensor = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "    numpy_array = torch_tensor.numpy()\n",
    "    print(f\"PyTorch张量: {torch_tensor}\")\n",
    "    print(f\"转换为NumPy: {numpy_array}\")\n",
    "    \n",
    "    # 从NumPy创建张量\n",
    "    new_torch = torch.from_numpy(numpy_array)\n",
    "    print(f\"从NumPy创建: {new_torch}\")\n",
    "else:\n",
    "    # GPU张量需要先移到CPU\n",
    "    gpu_tensor = torch.tensor([1, 2, 3, 4], dtype=torch.float32, device=device)\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    numpy_array = cpu_tensor.numpy()\n",
    "    print(f\"GPU张量转NumPy: {numpy_array}\")\n",
    "\n",
    "print(f\"\\n✓ PyTorch张量基础操作学习完成！\")\n",
    "\n",
    "# 可视化一些张量操作\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. 随机张量可视化\n",
    "random_2d = torch.randn(10, 10, device='cpu')\n",
    "axes[0, 0].imshow(random_2d.numpy(), cmap='viridis')\n",
    "axes[0, 0].set_title('随机张量可视化')\n",
    "axes[0, 0].colorbar = plt.colorbar(axes[0, 0].imshow(random_2d.numpy(), cmap='viridis'), ax=axes[0, 0])\n",
    "\n",
    "# 2. 张量运算结果\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y1 = torch.sin(x)\n",
    "y2 = torch.cos(x)\n",
    "y3 = torch.exp(-x**2)\n",
    "\n",
    "axes[0, 1].plot(x.numpy(), y1.numpy(), label='sin(x)')\n",
    "axes[0, 1].plot(x.numpy(), y2.numpy(), label='cos(x)')\n",
    "axes[0, 1].plot(x.numpy(), y3.numpy(), label='exp(-x²)')\n",
    "axes[0, 1].set_title('张量数学函数')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. 矩阵乘法可视化\n",
    "A = torch.randn(5, 3)\n",
    "B = torch.randn(3, 4)\n",
    "C = torch.mm(A, B)\n",
    "\n",
    "axes[1, 0].imshow(A.numpy(), cmap='RdBu', aspect='auto')\n",
    "axes[1, 0].set_title('矩阵 A (5x3)')\n",
    "\n",
    "axes[1, 1].imshow(B.numpy(), cmap='RdBu', aspect='auto')\n",
    "axes[1, 1].set_title('矩阵 B (3x4)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 性能基准测试\n",
    "print(f\"\\n=== 性能基准测试 ===\")\n",
    "def benchmark_operation(operation_func, tensor_size=(1000, 1000), iterations=100):\n",
    "    \"\"\"基准测试函数\"\"\"\n",
    "    tensor = torch.randn(tensor_size, device=device)\n",
    "    \n",
    "    # 预热\n",
    "    for _ in range(10):\n",
    "        _ = operation_func(tensor)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result = operation_func(tensor)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / iterations * 1000  # 毫秒\n",
    "    return avg_time\n",
    "\n",
    "# 测试不同操作的性能\n",
    "operations = {\n",
    "    '矩阵乘法': lambda x: torch.mm(x, x.t()),\n",
    "    '元素级加法': lambda x: x + x,\n",
    "    '三角函数': lambda x: torch.sin(x),\n",
    "    '指数函数': lambda x: torch.exp(x),\n",
    "    '求和': lambda x: torch.sum(x)\n",
    "}\n",
    "\n",
    "print(f\"设备: {device}\")\n",
    "print(f\"张量大小: 1000x1000\")\n",
    "print(f\"迭代次数: 100\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for name, op in operations.items():\n",
    "    avg_time = benchmark_operation(op)\n",
    "    print(f\"{name:12}: {avg_time:.2f} ms\")\n",
    "\n",
    "print(f\"\\n✓ 张量基础操作和性能测试完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabd08d",
   "metadata": {},
   "source": [
    "## 3. 自动微分（Autograd）\n",
    "\n",
    "自动微分是PyTorch的核心特性之一，它能够自动计算梯度，是深度学习训练的基础。理解autograd机制对于掌握PyTorch至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 自动微分基础\n",
    "print(\"=== 自动微分（Autograd）详解 ===\")\n",
    "\n",
    "# 3.1.1 requires_grad参数\n",
    "print(\"1. requires_grad参数和计算图构建:\")\n",
    "\n",
    "# 创建需要梯度的张量\n",
    "x = torch.tensor([2.0], requires_grad=True, device=device)\n",
    "y = torch.tensor([3.0], requires_grad=True, device=device)\n",
    "\n",
    "print(f\"x = {x}, requires_grad = {x.requires_grad}\")\n",
    "print(f\"y = {y}, requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# 定义计算图\n",
    "z = x * y  # z = 2 * 3 = 6\n",
    "w = z + x  # w = 6 + 2 = 8\n",
    "loss = w ** 2  # loss = 8^2 = 64\n",
    "\n",
    "print(f\"z = x * y = {z}\")\n",
    "print(f\"w = z + x = {w}\")\n",
    "print(f\"loss = w^2 = {loss}\")\n",
    "\n",
    "# 查看计算图信息\n",
    "print(f\"\\nloss.grad_fn: {loss.grad_fn}\")\n",
    "print(f\"w.grad_fn: {w.grad_fn}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "\n",
    "# 3.1.2 反向传播\n",
    "print(f\"\\n2. 反向传播计算梯度:\")\n",
    "loss.backward()\n",
    "\n",
    "print(f\"∂loss/∂x = {x.grad}\")\n",
    "print(f\"∂loss/∂y = {y.grad}\")\n",
    "\n",
    "# 手动验证梯度计算\n",
    "# loss = (x*y + x)^2 = (2*3 + 2)^2 = 8^2 = 64\n",
    "# ∂loss/∂x = 2*(x*y + x) * (y + 1) = 2*8*(3+1) = 64\n",
    "# ∂loss/∂y = 2*(x*y + x) * x = 2*8*2 = 32\n",
    "print(f\"手动计算 ∂loss/∂x = 2*8*4 = {2*8*4}\")\n",
    "print(f\"手动计算 ∂loss/∂y = 2*8*2 = {2*8*2}\")\n",
    "\n",
    "# 3.1.3 梯度清零\n",
    "print(f\"\\n3. 梯度累积和清零:\")\n",
    "print(f\"第一次反向传播后 x.grad: {x.grad}\")\n",
    "\n",
    "# 再次反向传播（梯度会累积）\n",
    "loss.backward()\n",
    "print(f\"第二次反向传播后 x.grad: {x.grad}\")\n",
    "\n",
    "# 梯度清零\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "print(f\"清零后 x.grad: {x.grad}\")\n",
    "\n",
    "# 3.1.4 梯度上下文管理\n",
    "print(f\"\\n4. 梯度上下文管理:\")\n",
    "\n",
    "# 禁用梯度计算\n",
    "x = torch.randn(3, 3, requires_grad=True, device=device)\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(f\"在 no_grad 上下文中，y.requires_grad: {y.requires_grad}\")\n",
    "\n",
    "# 临时启用梯度计算\n",
    "x = torch.randn(3, 3, device=device)\n",
    "with torch.enable_grad():\n",
    "    x.requires_grad_(True)\n",
    "    y = x * 2\n",
    "    print(f\"在 enable_grad 上下文中，y.requires_grad: {y.requires_grad}\")\n",
    "\n",
    "# 3.1.5 detach()方法\n",
    "print(f\"\\n5. detach()方法使用:\")\n",
    "x = torch.randn(3, requires_grad=True, device=device)\n",
    "y = x * 2\n",
    "\n",
    "# 分离张量，停止梯度传播\n",
    "y_detached = y.detach()\n",
    "print(f\"原始 y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"分离后 y_detached.requires_grad: {y_detached.requires_grad}\")\n",
    "\n",
    "# 3.1.6 函数的自动微分\n",
    "print(f\"\\n6. 复杂函数的自动微分:\")\n",
    "\n",
    "def complex_function(x):\n",
    "    \"\"\"复杂函数示例\"\"\"\n",
    "    return torch.sin(x) * torch.exp(-x**2) + torch.cos(x**2)\n",
    "\n",
    "# 创建输入\n",
    "x = torch.linspace(-2, 2, 100, requires_grad=True, device=device)\n",
    "y = complex_function(x)\n",
    "\n",
    "# 计算某点的梯度\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"在区间[-2, 2]上的梯度范围: [{x.grad.min().item():.4f}, {x.grad.max().item():.4f}]\")\n",
    "\n",
    "# 可视化函数和梯度\n",
    "if device.type == 'cpu':\n",
    "    x_np = x.detach().numpy()\n",
    "    y_np = y.detach().numpy()\n",
    "    grad_np = x.grad.numpy()\n",
    "else:\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    grad_np = x.grad.cpu().numpy()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "ax1.plot(x_np, y_np, 'b-', linewidth=2, label='f(x)')\n",
    "ax1.set_title('复杂函数 f(x) = sin(x)·exp(-x²) + cos(x²)')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x_np, grad_np, 'r-', linewidth=2, label=\"f'(x)\")\n",
    "ax2.set_title('函数的导数')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel(\"f'(x)\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.1.7 自定义autograd函数\n",
    "print(f\"\\n7. 自定义autograd函数:\")\n",
    "\n",
    "class SquareFunction(torch.autograd.Function):\n",
    "    \"\"\"自定义平方函数，演示如何实现自定义的前向和反向传播\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 保存输入以供反向传播使用\n",
    "        ctx.save_for_backward(input)\n",
    "        return input ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        # 获取保存的输入\n",
    "        input, = ctx.saved_tensors\n",
    "        # 计算梯度：d(x²)/dx = 2x\n",
    "        grad_input = 2 * input * grad_output\n",
    "        return grad_input\n",
    "\n",
    "# 使用自定义函数\n",
    "square = SquareFunction.apply\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True, device=device)\n",
    "y = square(x)\n",
    "y.backward()\n",
    "\n",
    "print(f\"自定义平方函数: f(3) = {y.item()}\")\n",
    "print(f\"自定义函数梯度: f'(3) = {x.grad.item()}\")\n",
    "print(f\"理论梯度: 2*3 = 6\")\n",
    "\n",
    "# 3.1.8 高阶梯度\n",
    "print(f\"\\n8. 高阶梯度计算:\")\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True, device=device)\n",
    "y = x ** 3  # y = x³\n",
    "\n",
    "# 一阶导数\n",
    "grad_1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"一阶导数 dy/dx = {grad_1.item()}\")  # 3x² = 12\n",
    "\n",
    "# 二阶导数\n",
    "grad_2 = torch.autograd.grad(grad_1, x)[0]\n",
    "print(f\"二阶导数 d²y/dx² = {grad_2.item()}\")  # 6x = 12\n",
    "\n",
    "# 3.1.9 雅可比矩阵\n",
    "print(f\"\\n9. 雅可比矩阵计算:\")\n",
    "\n",
    "def vector_function(x):\n",
    "    \"\"\"向量函数 f(x,y) = [x²+y, xy, y²]\"\"\"\n",
    "    return torch.stack([\n",
    "        x[0]**2 + x[1],\n",
    "        x[0] * x[1], \n",
    "        x[1]**2\n",
    "    ])\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True, device=device)\n",
    "y = vector_function(x)\n",
    "\n",
    "# 计算雅可比矩阵\n",
    "jacobian = torch.autograd.functional.jacobian(vector_function, x)\n",
    "print(f\"输入: {x}\")\n",
    "print(f\"输出: {y}\")\n",
    "print(f\"雅可比矩阵:\\n{jacobian}\")\n",
    "\n",
    "# 理论雅可比矩阵:\n",
    "# f₁ = x² + y  →  ∂f₁/∂x = 2x, ∂f₁/∂y = 1\n",
    "# f₂ = xy      →  ∂f₂/∂x = y,  ∂f₂/∂y = x  \n",
    "# f₃ = y²      →  ∂f₃/∂x = 0,  ∂f₃/∂y = 2y\n",
    "theoretical_jacobian = torch.tensor([\n",
    "    [2*x[0], 1],      # [4, 1]\n",
    "    [x[1], x[0]],     # [3, 2]\n",
    "    [0, 2*x[1]]       # [0, 6]\n",
    "], device=device)\n",
    "print(f\"理论雅可比矩阵:\\n{theoretical_jacobian}\")\n",
    "\n",
    "print(f\"\\n✓ 自动微分机制学习完成！\")\n",
    "\n",
    "# 3.1.10 梯度检查工具\n",
    "print(f\"\\n10. 梯度检查（数值验证）:\")\n",
    "\n",
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    \"\"\"数值方法计算梯度\"\"\"\n",
    "    grad = torch.zeros_like(x)\n",
    "    for i in range(x.numel()):\n",
    "        x_pos = x.clone()\n",
    "        x_neg = x.clone()\n",
    "        x_pos.view(-1)[i] += h\n",
    "        x_neg.view(-1)[i] -= h\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            grad.view(-1)[i] = (f(x_pos) - f(x_neg)) / (2 * h)\n",
    "    return grad\n",
    "\n",
    "# 测试函数\n",
    "def test_function(x):\n",
    "    return (x**2).sum()\n",
    "\n",
    "x = torch.randn(3, requires_grad=True, device=device)\n",
    "\n",
    "# 自动微分梯度\n",
    "loss = test_function(x)\n",
    "loss.backward()\n",
    "auto_grad = x.grad.clone()\n",
    "\n",
    "# 数值梯度\n",
    "x.grad.zero_()\n",
    "numerical_grad = numerical_gradient(test_function, x)\n",
    "\n",
    "# 比较两种梯度\n",
    "difference = torch.abs(auto_grad - numerical_grad)\n",
    "print(f\"自动微分梯度: {auto_grad}\")\n",
    "print(f\"数值计算梯度: {numerical_grad}\")\n",
    "print(f\"差异: {difference}\")\n",
    "print(f\"最大差异: {difference.max().item():.2e}\")\n",
    "\n",
    "# 绘制梯度检查可视化\n",
    "if len(x) <= 10:  # 只对小张量进行可视化\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    indices = range(len(auto_grad.flatten()))\n",
    "    \n",
    "    if device.type == 'cpu':\n",
    "        auto_grad_np = auto_grad.numpy().flatten()\n",
    "        numerical_grad_np = numerical_grad.numpy().flatten()\n",
    "    else:\n",
    "        auto_grad_np = auto_grad.cpu().numpy().flatten()\n",
    "        numerical_grad_np = numerical_grad.cpu().numpy().flatten()\n",
    "    \n",
    "    ax.plot(indices, auto_grad_np, 'bo-', label='自动微分', markersize=8)\n",
    "    ax.plot(indices, numerical_grad_np, 'r^-', label='数值计算', markersize=8)\n",
    "    ax.set_xlabel('参数索引')\n",
    "    ax.set_ylabel('梯度值')\n",
    "    ax.set_title('梯度验证：自动微分 vs 数值计算')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n自动微分系统验证完成！梯度计算正确性: {'✓' if difference.max() < 1e-4 else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e6113",
   "metadata": {},
   "source": [
    "## 4. 自定义神经网络模型\n",
    "\n",
    "在PyTorch中，构建神经网络模型主要通过继承`nn.Module`类来实现。我们将从简单的全连接网络开始，逐步构建复杂的卷积神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 基础神经网络模型\n",
    "print(\"=== 自定义神经网络模型构建 ===\")\n",
    "\n",
    "# 4.1.1 简单的全连接网络\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"简单的全连接神经网络\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 创建并测试简单网络\n",
    "simple_model = SimpleNN(input_size=784, hidden_size=128, output_size=10).to(device)\n",
    "print(f\"1. 简单全连接网络:\")\n",
    "print(simple_model)\n",
    "\n",
    "# 查看模型参数\n",
    "total_params = sum(p.numel() for p in simple_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)\n",
    "print(f\"\\n总参数数量: {total_params:,}\")\n",
    "print(f\"可训练参数: {trainable_params:,}\")\n",
    "\n",
    "# 测试前向传播\n",
    "test_input = torch.randn(32, 784, device=device)  # batch_size=32\n",
    "output = simple_model(test_input)\n",
    "print(f\"输入形状: {test_input.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "\n",
    "# 4.1.2 使用Sequential构建网络\n",
    "print(f\"\\n2. 使用Sequential构建网络:\")\n",
    "\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ").to(device)\n",
    "\n",
    "print(sequential_model)\n",
    "\n",
    "# 4.1.3 卷积神经网络（CNN）\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"卷积神经网络用于图像分类\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # 卷积层\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 池化层\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # 28->14->7->3 (after 3 pooling)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # 激活函数和正则化\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 第一个卷积块\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # 28x28 -> 14x14\n",
    "        \n",
    "        # 第二个卷积块\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # 14x14 -> 7x7\n",
    "        \n",
    "        # 第三个卷积块\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # 7x7 -> 3x3\n",
    "        \n",
    "        # 展平\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 全连接层\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 创建CNN模型\n",
    "cnn_model = ConvNet(num_classes=10).to(device)\n",
    "print(f\"\\n3. 卷积神经网络:\")\n",
    "print(cnn_model)\n",
    "\n",
    "# 测试CNN\n",
    "test_image = torch.randn(32, 1, 28, 28, device=device)  # MNIST格式\n",
    "cnn_output = cnn_model(test_image)\n",
    "print(f\"\\nCNN测试:\")\n",
    "print(f\"输入形状: {test_image.shape}\")\n",
    "print(f\"输出形状: {cnn_output.shape}\")\n",
    "\n",
    "# 计算参数量\n",
    "cnn_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "print(f\"CNN参数数量: {cnn_params:,}\")\n",
    "\n",
    "# 4.1.4 残差块和ResNet风格网络\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"残差块\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 如果输入输出通道数不同，需要调整维度\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # 添加残差连接\n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SimpleResNet(nn.Module):\n",
    "    \"\"\"简化的ResNet\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # 残差块\n",
    "        self.layer1 = self._make_layer(16, 16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(16, 32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(32, 64, 2, stride=2)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建ResNet模型\n",
    "resnet_model = SimpleResNet(num_classes=10).to(device)\n",
    "print(f\"\\n4. 简化ResNet:\")\n",
    "print(resnet_model)\n",
    "\n",
    "# 测试ResNet\n",
    "resnet_output = resnet_model(test_image)\n",
    "print(f\"\\nResNet测试:\")\n",
    "print(f\"输入形状: {test_image.shape}\")\n",
    "print(f\"输出形状: {resnet_output.shape}\")\n",
    "\n",
    "resnet_params = sum(p.numel() for p in resnet_model.parameters())\n",
    "print(f\"ResNet参数数量: {resnet_params:,}\")\n",
    "\n",
    "# 4.1.5 自定义激活函数\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish激活函数: x * sigmoid(x)\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"GELU激活函数的近似实现\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "# 测试自定义激活函数\n",
    "print(f\"\\n5. 自定义激活函数测试:\")\n",
    "x = torch.linspace(-3, 3, 100, device=device)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "swish = Swish().to(device)\n",
    "gelu = GELU().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_relu = relu(x)\n",
    "    y_swish = swish(x)\n",
    "    y_gelu = gelu(x)\n",
    "\n",
    "# 可视化激活函数\n",
    "if device.type == 'cpu':\n",
    "    x_np = x.numpy()\n",
    "    y_relu_np = y_relu.numpy()\n",
    "    y_swish_np = y_swish.numpy()\n",
    "    y_gelu_np = y_gelu.numpy()\n",
    "else:\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_relu_np = y_relu.cpu().numpy()\n",
    "    y_swish_np = y_swish.cpu().numpy()\n",
    "    y_gelu_np = y_gelu.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_np, y_relu_np, label='ReLU', linewidth=2)\n",
    "plt.plot(x_np, y_swish_np, label='Swish', linewidth=2)\n",
    "plt.plot(x_np, y_gelu_np, label='GELU', linewidth=2)\n",
    "plt.plot(x_np, np.tanh(x_np), label='Tanh', linewidth=2, linestyle='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('不同激活函数的比较')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4.1.6 模型信息和可视化工具\n",
    "def model_summary(model, input_size):\n",
    "    \"\"\"模型摘要信息\"\"\"\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "            \n",
    "            m_key = f\"{class_name}-{module_idx+1}\"\n",
    "            summary[m_key] = {}\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"output_shape\"] = list(output.size())\n",
    "            \n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            \n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "        \n",
    "        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "    \n",
    "    # 检查设备类型\n",
    "    device_type = next(model.parameters()).device\n",
    "    \n",
    "    # 创建测试输入\n",
    "    if isinstance(input_size, tuple):\n",
    "        x = torch.rand(1, *input_size).to(device_type)\n",
    "    else:\n",
    "        x = torch.rand(input_size).to(device_type)\n",
    "    \n",
    "    summary = {}\n",
    "    hooks = []\n",
    "    \n",
    "    model.apply(register_hook)\n",
    "    model(x)\n",
    "    \n",
    "    # 移除hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Layer (type)':>25} {'Output Shape':>15} {'Param #':>10}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for layer in summary:\n",
    "        output_shape = str(summary[layer][\"output_shape\"])\n",
    "        nb_params = summary[layer][\"nb_params\"]\n",
    "        \n",
    "        total_params += nb_params\n",
    "        print(f\"{layer:>25} {output_shape:>15} {nb_params:>10,}\")\n",
    "    \n",
    "    # 计算可训练参数\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - trainable_params:,}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\n6. 模型摘要信息:\")\n",
    "print(f\"\\nCNN模型摘要:\")\n",
    "model_summary(cnn_model, (1, 28, 28))\n",
    "\n",
    "print(f\"\\nResNet模型摘要:\")\n",
    "model_summary(resnet_model, (1, 28, 28))\n",
    "\n",
    "# 4.1.7 模型初始化\n",
    "def init_weights(m):\n",
    "    \"\"\"自定义权重初始化\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "print(f\"\\n7. 模型权重初始化:\")\n",
    "\n",
    "# 创建新模型进行初始化演示\n",
    "test_model = ConvNet().to(device)\n",
    "\n",
    "# 查看初始化前的权重\n",
    "first_conv_weight = test_model.conv1.weight.data.clone()\n",
    "print(f\"初始化前第一层卷积权重统计:\")\n",
    "print(f\"  均值: {first_conv_weight.mean().item():.6f}\")\n",
    "print(f\"  标准差: {first_conv_weight.std().item():.6f}\")\n",
    "\n",
    "# 应用自定义初始化\n",
    "test_model.apply(init_weights)\n",
    "\n",
    "# 查看初始化后的权重\n",
    "after_conv_weight = test_model.conv1.weight.data\n",
    "print(f\"初始化后第一层卷积权重统计:\")\n",
    "print(f\"  均值: {after_conv_weight.mean().item():.6f}\")\n",
    "print(f\"  标准差: {after_conv_weight.std().item():.6f}\")\n",
    "\n",
    "print(f\"\\n✓ 神经网络模型构建学习完成！\")\n",
    "\n",
    "# 4.1.8 模型比较表\n",
    "print(f\"\\n8. 模型复杂度对比:\")\n",
    "models_comparison = {\n",
    "    'Simple NN': {'model': simple_model, 'params': sum(p.numel() for p in simple_model.parameters())},\n",
    "    'Sequential': {'model': sequential_model, 'params': sum(p.numel() for p in sequential_model.parameters())},\n",
    "    'CNN': {'model': cnn_model, 'params': sum(p.numel() for p in cnn_model.parameters())},\n",
    "    'ResNet': {'model': resnet_model, 'params': sum(p.numel() for p in resnet_model.parameters())}\n",
    "}\n",
    "\n",
    "print(f\"{'模型':>12} {'参数数量':>15} {'相对复杂度':>12}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "min_params = min(info['params'] for info in models_comparison.values())\n",
    "\n",
    "for name, info in models_comparison.items():\n",
    "    params = info['params']\n",
    "    relative_complexity = params / min_params\n",
    "    print(f\"{name:>12} {params:>15,} {relative_complexity:>12.1f}x\")\n",
    "\n",
    "# 可视化模型复杂度\n",
    "model_names = list(models_comparison.keys())\n",
    "param_counts = [models_comparison[name]['params'] for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, param_counts, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "plt.yscale('log')\n",
    "plt.ylabel('参数数量 (对数刻度)')\n",
    "plt.title('不同模型的参数复杂度对比')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 添加数值标签\n",
    "for i, (bar, count) in enumerate(zip(bars, param_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n模型架构设计要点:\")\n",
    "print(f\"✓ 根据任务复杂度选择合适的模型规模\")\n",
    "print(f\"✓ 使用批归一化稳定训练过程\")\n",
    "print(f\"✓ 适当使用Dropout防止过拟合\")\n",
    "print(f\"✓ 残差连接有助于训练深层网络\")\n",
    "print(f\"✓ 合适的权重初始化很重要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef177fd5",
   "metadata": {},
   "source": [
    "## 5. 自定义数据集与数据加载\n",
    "\n",
    "在深度学习项目中，数据处理是关键环节。PyTorch提供了灵活的数据加载机制，我们将学习如何创建自定义数据集、进行数据增强，以及高效的数据加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 MNIST数据集加载和预处理\n",
    "print(\"=== 自定义数据集与数据加载 ===\")\n",
    "\n",
    "# 5.1.1 标准MNIST数据集加载\n",
    "print(\"1. 标准MNIST数据集加载:\")\n",
    "\n",
    "# 定义数据变换\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量，范围[0,1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST的均值和标准差\n",
    "])\n",
    "\n",
    "# 加载MNIST数据集\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=basic_transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=basic_transform\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")\n",
    "print(f\"图像形状: {train_dataset[0][0].shape}\")\n",
    "print(f\"标签范围: {min([label for _, label in train_dataset])} - {max([label for _, label in train_dataset])}\")\n",
    "\n",
    "# 5.1.2 自定义数据集类\n",
    "class CustomMNIST(Dataset):\n",
    "    \"\"\"自定义MNIST数据集类\"\"\"\n",
    "    \n",
    "    def __init__(self, data, targets, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.data[idx], self.targets[idx]\n",
    "        \n",
    "        # 如果是PIL图像或numpy数组，转换为PIL\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            image = transforms.ToPILImage()(image.squeeze())\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# 从原始数据集创建自定义数据集\n",
    "custom_dataset = CustomMNIST(\n",
    "    data=train_dataset.data[:1000],  # 使用前1000个样本\n",
    "    targets=train_dataset.targets[:1000],\n",
    "    transform=basic_transform\n",
    ")\n",
    "\n",
    "print(f\"\\n自定义数据集大小: {len(custom_dataset)}\")\n",
    "\n",
    "# 5.1.3 数据增强\n",
    "print(f\"\\n2. 数据增强技术:\")\n",
    "\n",
    "# 定义各种数据增强变换\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # 随机旋转±10度\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 随机平移\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 更激进的数据增强\n",
    "aggressive_augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))  # 随机擦除\n",
    "])\n",
    "\n",
    "# 创建增强数据集\n",
    "augmented_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=data_augmentation\n",
    ")\n",
    "\n",
    "# 可视化数据增强效果\n",
    "def visualize_augmentation(dataset, original_dataset, num_samples=8):\n",
    "    \"\"\"可视化数据增强效果\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(16, 6))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # 原始图像\n",
    "        orig_img, label = original_dataset[i]\n",
    "        if orig_img.dim() == 3 and orig_img.shape[0] == 1:\n",
    "            orig_img = orig_img.squeeze(0)\n",
    "        \n",
    "        axes[0, i].imshow(orig_img, cmap='gray')\n",
    "        axes[0, i].set_title(f'原始 ({label})')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # 增强后图像\n",
    "        aug_img, _ = dataset[i]\n",
    "        if aug_img.dim() == 3 and aug_img.shape[0] == 1:\n",
    "            aug_img = aug_img.squeeze(0)\n",
    "        \n",
    "        axes[1, i].imshow(aug_img, cmap='gray')\n",
    "        axes[1, i].set_title(f'增强 ({label})')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('数据增强效果对比', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"数据增强效果可视化:\")\n",
    "visualize_augmentation(augmented_dataset, train_dataset)\n",
    "\n",
    "# 5.1.4 数据加载器配置\n",
    "print(f\"\\n3. 数据加载器配置:\")\n",
    "\n",
    "# 不同的数据加载器配置\n",
    "dataloaders = {}\n",
    "\n",
    "# 基础配置\n",
    "dataloaders['basic'] = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 高性能配置\n",
    "dataloaders['optimized'] = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# 测试不同配置的加载速度\n",
    "def benchmark_dataloader(dataloader, name, num_batches=50):\n",
    "    \"\"\"测试数据加载器性能\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        # 模拟数据传输到GPU\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    samples_per_second = (num_batches * dataloader.batch_size) / total_time\n",
    "    \n",
    "    print(f\"{name:12}: {total_time:.2f}s, {samples_per_second:.0f} samples/s\")\n",
    "\n",
    "print(\"数据加载器性能测试:\")\n",
    "for name, dataloader in dataloaders.items():\n",
    "    benchmark_dataloader(dataloader, name)\n",
    "\n",
    "# 5.1.5 数据集分割\n",
    "print(f\"\\n4. 数据集分割:\")\n",
    "\n",
    "# 将训练集分割为训练集和验证集\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_subset, val_subset = random_split(\n",
    "    train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"原始训练集: {len(train_dataset)}\")\n",
    "print(f\"分割后训练集: {len(train_subset)}\")\n",
    "print(f\"验证集: {len(val_subset)}\")\n",
    "\n",
    "# 创建对应的数据加载器\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"训练批次数: {len(train_loader)}\")\n",
    "print(f\"验证批次数: {len(val_loader)}\")\n",
    "print(f\"测试批次数: {len(test_loader)}\")\n",
    "\n",
    "# 5.1.6 数据分析和可视化\n",
    "print(f\"\\n5. 数据分析:\")\n",
    "\n",
    "# 分析类别分布\n",
    "def analyze_dataset(dataset, name):\n",
    "    \"\"\"分析数据集的类别分布\"\"\"\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = dataset.targets\n",
    "    else:\n",
    "        # 对于subset，需要提取targets\n",
    "        targets = [dataset.dataset.targets[i] for i in dataset.indices]\n",
    "        targets = torch.tensor(targets)\n",
    "    \n",
    "    unique, counts = torch.unique(targets, return_counts=True)\n",
    "    \n",
    "    print(f\"\\n{name} 类别分布:\")\n",
    "    for digit, count in zip(unique.tolist(), counts.tolist()):\n",
    "        percentage = count / len(targets) * 100\n",
    "        print(f\"  数字 {digit}: {count:5d} 样本 ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return targets\n",
    "\n",
    "# 分析各个数据集\n",
    "train_targets = analyze_dataset(train_subset, \"训练集\")\n",
    "val_targets = analyze_dataset(val_subset, \"验证集\")\n",
    "test_targets = analyze_dataset(test_dataset, \"测试集\")\n",
    "\n",
    "# 可视化类别分布\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "datasets_info = [\n",
    "    (\"训练集\", train_targets),\n",
    "    (\"验证集\", val_targets), \n",
    "    (\"测试集\", test_targets)\n",
    "]\n",
    "\n",
    "for idx, (name, targets) in enumerate(datasets_info):\n",
    "    unique, counts = torch.unique(targets, return_counts=True)\n",
    "    \n",
    "    axes[idx].bar(unique.numpy(), counts.numpy())\n",
    "    axes[idx].set_title(f'{name}类别分布')\n",
    "    axes[idx].set_xlabel('数字')\n",
    "    axes[idx].set_ylabel('样本数量')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5.1.7 批次数据可视化\n",
    "def visualize_batch(dataloader, num_samples=16):\n",
    "    \"\"\"可视化一个批次的数据\"\"\"\n",
    "    data_iter = iter(dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # 选择要显示的样本数\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    \n",
    "    # 计算网格大小\n",
    "    grid_size = int(np.ceil(np.sqrt(num_samples)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img = images[i]\n",
    "        if img.dim() == 3 and img.shape[0] == 1:\n",
    "            img = img.squeeze(0)\n",
    "        \n",
    "        # 反归一化以便显示\n",
    "        img = img * 0.3081 + 0.1307\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {labels[i].item()}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # 隐藏多余的子图\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('训练批次数据样本', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n6. 批次数据可视化:\")\n",
    "visualize_batch(train_loader, 16)\n",
    "\n",
    "# 5.1.8 数据统计分析\n",
    "print(f\"\\n7. 数据统计分析:\")\n",
    "\n",
    "def compute_dataset_stats(dataloader):\n",
    "    \"\"\"计算数据集的统计信息\"\"\"\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for data, _ in dataloader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "    \n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# 计算训练集统计信息\n",
    "train_mean, train_std = compute_dataset_stats(train_loader)\n",
    "print(f\"训练集统计信息:\")\n",
    "print(f\"  均值: {train_mean.item():.4f}\")\n",
    "print(f\"  标准差: {train_std.item():.4f}\")\n",
    "\n",
    "# 验证预定义的归一化参数\n",
    "print(f\"预定义MNIST归一化参数:\")\n",
    "print(f\"  均值: 0.1307\")\n",
    "print(f\"  标准差: 0.3081\")\n",
    "\n",
    "# 5.1.9 内存和性能优化\n",
    "print(f\"\\n8. 内存和性能优化:\")\n",
    "\n",
    "# 内存映射数据集\n",
    "class MemoryMappedDataset(Dataset):\n",
    "    \"\"\"内存映射数据集，适用于大数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, transform=None):\n",
    "        # 这里简化示例，实际应用中会使用内存映射文件\n",
    "        self.data = train_dataset.data\n",
    "        self.targets = train_dataset.targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 懒加载，只在需要时加载数据\n",
    "        image = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # 转换为PIL图像以应用transform\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = image.float() / 255.0\n",
    "            image = image.unsqueeze(0)  # 添加通道维度\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# 预取数据加载器\n",
    "class PrefetchLoader:\n",
    "    \"\"\"数据预取加载器\"\"\"\n",
    "    \n",
    "    def __init__(self, loader):\n",
    "        self.loader = loader\n",
    "        self.stream = torch.cuda.Stream() if torch.cuda.is_available() else None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        loader_iter = iter(self.loader)\n",
    "        self.preload(loader_iter)\n",
    "        \n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream) if self.stream else None\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload(loader_iter)\n",
    "            yield input, target\n",
    "    \n",
    "    def preload(self, loader_iter):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(loader_iter)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        \n",
    "        if self.stream:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                self.next_input = self.next_input.cuda(non_blocking=True)\n",
    "                self.next_target = self.next_target.cuda(non_blocking=True)\n",
    "\n",
    "# 测试优化后的数据加载\n",
    "optimized_dataset = MemoryMappedDataset('./data', basic_transform)\n",
    "optimized_loader = DataLoader(\n",
    "    optimized_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(\"数据加载优化技术:\")\n",
    "print(\"✓ 内存映射减少内存占用\")\n",
    "print(\"✓ pin_memory加速GPU传输\")\n",
    "print(\"✓ persistent_workers减少进程创建开销\")\n",
    "print(\"✓ non_blocking传输提高并行度\")\n",
    "\n",
    "print(f\"\\n✓ 数据集和数据加载学习完成！\")\n",
    "\n",
    "# 保存数据加载器配置信息\n",
    "data_config = {\n",
    "    'train_size': len(train_subset),\n",
    "    'val_size': len(val_subset),\n",
    "    'test_size': len(test_dataset),\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': torch.cuda.is_available(),\n",
    "    'normalize_mean': 0.1307,\n",
    "    'normalize_std': 0.3081\n",
    "}\n",
    "\n",
    "print(f\"\\n数据配置信息:\")\n",
    "for key, value in data_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 将数据加载器保存为全局变量供后续使用\n",
    "globals()['train_loader'] = train_loader\n",
    "globals()['val_loader'] = val_loader\n",
    "globals()['test_loader'] = test_loader\n",
    "globals()['data_config'] = data_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265743c8",
   "metadata": {},
   "source": [
    "## 6. 完整训练流程\n",
    "\n",
    "实现一个完整的深度学习训练流程，包括训练、验证、测试的循环，损失函数选择，优化器配置，以及学习率调度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 训练配置和初始化\n",
    "print(\"=== 完整训练流程 ===\")\n",
    "\n",
    "# 6.1.1 训练配置\n",
    "class TrainingConfig:\n",
    "    \"\"\"训练配置类\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 模型配置\n",
    "        self.model_name = \"CNN_MNIST\"\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        # 训练配置\n",
    "        self.epochs = 20\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 0.001\n",
    "        self.weight_decay = 1e-4\n",
    "        \n",
    "        # 优化器配置\n",
    "        self.optimizer_type = \"Adam\"  # Adam, SGD, AdamW\n",
    "        self.momentum = 0.9  # for SGD\n",
    "        \n",
    "        # 学习率调度\n",
    "        self.scheduler_type = \"StepLR\"  # StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "        self.step_size = 7\n",
    "        self.gamma = 0.1\n",
    "        self.patience = 5  # for ReduceLROnPlateau\n",
    "        \n",
    "        # 早停配置\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_patience = 10\n",
    "        self.min_delta = 0.001\n",
    "        \n",
    "        # 设备和路径\n",
    "        self.device = device\n",
    "        self.save_dir = Path(\"checkpoints\")\n",
    "        self.log_dir = Path(\"logs\")\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.use_amp = torch.cuda.is_available()\n",
    "        \n",
    "        # 日志和保存\n",
    "        self.save_best_only = True\n",
    "        self.save_frequency = 5  # 每5个epoch保存一次\n",
    "        self.log_frequency = 100  # 每100个batch记录一次\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "print(f\"训练配置:\")\n",
    "print(f\"  模型: {config.model_name}\")\n",
    "print(f\"  训练轮数: {config.epochs}\")\n",
    "print(f\"  批次大小: {config.batch_size}\")\n",
    "print(f\"  学习率: {config.learning_rate}\")\n",
    "print(f\"  优化器: {config.optimizer_type}\")\n",
    "print(f\"  设备: {config.device}\")\n",
    "print(f\"  混合精度: {config.use_amp}\")\n",
    "\n",
    "# 6.1.2 模型初始化\n",
    "# 使用之前定义的CNN模型\n",
    "model = ConvNet(num_classes=config.num_classes).to(config.device)\n",
    "\n",
    "# 应用权重初始化\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(f\"\\n模型参数数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 6.1.3 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1.4 优化器配置\n",
    "def get_optimizer(model, config):\n",
    "    \"\"\"根据配置获取优化器\"\"\"\n",
    "    if config.optimizer_type == \"Adam\":\n",
    "        return optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "    elif config.optimizer_type == \"SGD\":\n",
    "        return optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            momentum=config.momentum,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "    elif config.optimizer_type == \"AdamW\":\n",
    "        return optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {config.optimizer_type}\")\n",
    "\n",
    "optimizer = get_optimizer(model, config)\n",
    "\n",
    "# 6.1.5 学习率调度器\n",
    "def get_scheduler(optimizer, config):\n",
    "    \"\"\"根据配置获取学习率调度器\"\"\"\n",
    "    if config.scheduler_type == \"StepLR\":\n",
    "        return StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
    "    elif config.scheduler_type == \"ReduceLROnPlateau\":\n",
    "        return ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=config.patience,\n",
    "            factor=config.gamma, verbose=True\n",
    "        )\n",
    "    elif config.scheduler_type == \"CosineAnnealingLR\":\n",
    "        return CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "scheduler = get_scheduler(optimizer, config)\n",
    "\n",
    "# 6.1.6 早停机制\n",
    "class EarlyStopping:\n",
    "    \"\"\"早停机制\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.early_stopping_patience,\n",
    "    min_delta=config.min_delta\n",
    ") if config.early_stopping else None\n",
    "\n",
    "# 6.1.7 混合精度训练设置\n",
    "scaler = torch.cuda.amp.GradScaler() if config.use_amp else None\n",
    "\n",
    "print(f\"\\n训练组件初始化完成:\")\n",
    "print(f\"✓ 模型: {type(model).__name__}\")\n",
    "print(f\"✓ 损失函数: {type(criterion).__name__}\")\n",
    "print(f\"✓ 优化器: {type(optimizer).__name__}\")\n",
    "print(f\"✓ 调度器: {type(scheduler).__name__ if scheduler else None}\")\n",
    "print(f\"✓ 早停: {'启用' if early_stopping else '禁用'}\")\n",
    "print(f\"✓ 混合精度: {'启用' if scaler else '禁用'}\")\n",
    "\n",
    "# 6.2 训练和验证函数\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device, epoch, config):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 创建进度条\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Train]')\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            # 混合精度训练\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # 常规训练\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # 更新进度条\n",
    "        if batch_idx % config.log_frequency == 0:\n",
    "            current_acc = 100. * correct / total\n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device, epoch, config):\n",
    "    \"\"\"验证一个epoch\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config.epochs} [Val]')\n",
    "        \n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "            else:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # 更新进度条\n",
    "            current_acc = 100. * correct / total\n",
    "            current_loss = val_loss / (len(pbar.n) + 1) if hasattr(pbar, 'n') else 0\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = val_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# 6.3 训练历史记录\n",
    "class TrainingHistory:\n",
    "    \"\"\"训练历史记录\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.epochs = []\n",
    "        \n",
    "    def update(self, epoch, train_loss, train_acc, val_loss, val_acc, lr):\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_acc)\n",
    "        self.learning_rates.append(lr)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"保存训练历史\"\"\"\n",
    "        history_dict = {\n",
    "            'epochs': self.epochs,\n",
    "            'train_losses': self.train_losses,\n",
    "            'train_accuracies': self.train_accuracies,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_accuracies': self.val_accuracies,\n",
    "            'learning_rates': self.learning_rates\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"加载训练历史\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            history_dict = json.load(f)\n",
    "        \n",
    "        self.epochs = history_dict['epochs']\n",
    "        self.train_losses = history_dict['train_losses']\n",
    "        self.train_accuracies = history_dict['train_accuracies']\n",
    "        self.val_losses = history_dict['val_losses']\n",
    "        self.val_accuracies = history_dict['val_accuracies']\n",
    "        self.learning_rates = history_dict['learning_rates']\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"绘制训练历史\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 损失曲线\n",
    "        axes[0, 0].plot(self.epochs, self.train_losses, 'b-', label='Train Loss')\n",
    "        axes[0, 0].plot(self.epochs, self.val_losses, 'r-', label='Val Loss')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # 准确率曲线\n",
    "        axes[0, 1].plot(self.epochs, self.train_accuracies, 'b-', label='Train Acc')\n",
    "        axes[0, 1].plot(self.epochs, self.val_accuracies, 'r-', label='Val Acc')\n",
    "        axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # 学习率曲线\n",
    "        axes[1, 0].plot(self.epochs, self.learning_rates, 'g-')\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # 验证损失vs准确率散点图\n",
    "        axes[1, 1].scatter(self.val_losses, self.val_accuracies, c=self.epochs, cmap='viridis')\n",
    "        axes[1, 1].set_title('Validation Loss vs Accuracy')\n",
    "        axes[1, 1].set_xlabel('Validation Loss')\n",
    "        axes[1, 1].set_ylabel('Validation Accuracy (%)')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "history = TrainingHistory()\n",
    "\n",
    "# 6.4 主训练循环\n",
    "print(f\"\\n开始训练...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # 训练阶段\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, epoch, config\n",
    "        )\n",
    "        \n",
    "        # 验证阶段\n",
    "        val_loss, val_acc = validate_epoch(\n",
    "            model, val_loader, criterion, device, epoch, config\n",
    "        )\n",
    "        \n",
    "        # 学习率调度\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # 记录当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 更新训练历史\n",
    "        history.update(epoch, train_loss, train_acc, val_loss, val_acc, current_lr)\n",
    "        \n",
    "        # 计算epoch时间\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # 打印epoch结果\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config.__dict__\n",
    "            }, config.save_dir / 'best_model.pth')\n",
    "            print(f\"  ✓ 新的最佳模型已保存 (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # 定期保存检查点\n",
    "        if (epoch + 1) % config.save_frequency == 0:\n",
    "            checkpoint_path = config.save_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'history': history.__dict__,\n",
    "                'config': config.__dict__\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  ✓ 检查点已保存: {checkpoint_path.name}\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping:\n",
    "            if early_stopping(val_loss, model):\n",
    "                print(f\"  Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n训练被用户中断\")\n",
    "\n",
    "# 训练完成统计\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n训练完成!\")\n",
    "print(f\"总训练时间: {total_time/60:.1f} 分钟\")\n",
    "print(f\"最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "\n",
    "# 保存训练历史\n",
    "history.save(config.log_dir / 'training_history.json')\n",
    "print(f\"训练历史已保存到: {config.log_dir / 'training_history.json'}\")\n",
    "\n",
    "# 可视化训练历史\n",
    "print(f\"\\n绘制训练历史曲线:\")\n",
    "history.plot()\n",
    "\n",
    "print(f\"\\n✓ 完整训练流程演示完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe468d00",
   "metadata": {},
   "source": [
    "## 7. 断点保存与恢复 (Checkpoint & Resume)\n",
    "\n",
    "在深度学习训练中，断点保存与恢复功能至关重要，特别是对于长时间训练的模型。这可以让我们：\n",
    "- 在训练意外中断后恢复训练\n",
    "- 在不同实验之间继续训练\n",
    "- 保存和加载最佳模型\n",
    "- 实现增量训练\n",
    "\n",
    "### 7.1 断点保存策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d029216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    断点管理器 - 统一管理模型的保存与恢复\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir, max_checkpoints=5):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        \n",
    "    def save_checkpoint(self, state, epoch, is_best=False, suffix=\"\"):\n",
    "        \"\"\"保存断点\"\"\"\n",
    "        # 基本检查点\n",
    "        checkpoint_name = f\"checkpoint_epoch_{epoch}{suffix}.pth\"\n",
    "        checkpoint_path = self.save_dir / checkpoint_name\n",
    "        \n",
    "        # 添加时间戳\n",
    "        state['save_time'] = datetime.now().isoformat()\n",
    "        \n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"✓ 检查点已保存: {checkpoint_path}\")\n",
    "        \n",
    "        # 如果是最佳模型，额外保存一份\n",
    "        if is_best:\n",
    "            best_path = self.save_dir / \"best_model.pth\"\n",
    "            shutil.copy2(checkpoint_path, best_path)\n",
    "            print(f\"✓ 最佳模型已保存: {best_path}\")\n",
    "        \n",
    "        # 清理旧的检查点\n",
    "        self._cleanup_checkpoints()\n",
    "        \n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path, model, optimizer=None, scheduler=None):\n",
    "        \"\"\"加载断点\"\"\"\n",
    "        checkpoint_path = Path(checkpoint_path)\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            raise FileNotFoundError(f\"检查点文件不存在: {checkpoint_path}\")\n",
    "        \n",
    "        print(f\"🔄 正在加载检查点: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        # 加载模型状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # 加载优化器状态\n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # 加载调度器状态\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            if checkpoint['scheduler_state_dict'] is not None:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        print(f\"✓ 成功加载检查点 (Epoch {checkpoint.get('epoch', 'Unknown')})\")\n",
    "        return checkpoint\n",
    "    \n",
    "    def find_latest_checkpoint(self):\n",
    "        \"\"\"查找最新的检查点\"\"\"\n",
    "        checkpoints = list(self.save_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        \n",
    "        # 按修改时间排序\n",
    "        latest = max(checkpoints, key=lambda x: x.stat().st_mtime)\n",
    "        return latest\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"列出所有检查点\"\"\"\n",
    "        checkpoints = list(self.save_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
    "        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        print(f\"\\n📁 检查点目录: {self.save_dir}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if not checkpoints:\n",
    "            print(\"暂无检查点文件\")\n",
    "            return []\n",
    "        \n",
    "        for i, ckpt in enumerate(checkpoints):\n",
    "            # 尝试读取检查点信息\n",
    "            try:\n",
    "                state = torch.load(ckpt, map_location='cpu')\n",
    "                epoch = state.get('epoch', 'Unknown')\n",
    "                val_acc = state.get('val_acc', 'Unknown')\n",
    "                save_time = state.get('save_time', 'Unknown')\n",
    "                size = ckpt.stat().st_size / (1024 * 1024)  # MB\n",
    "                \n",
    "                print(f\"{i+1:2d}. {ckpt.name}\")\n",
    "                print(f\"     Epoch: {epoch}, Val Acc: {val_acc}, Size: {size:.1f}MB\")\n",
    "                print(f\"     Time: {save_time}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"{i+1:2d}. {ckpt.name} (无法读取: {e})\")\n",
    "        \n",
    "        return checkpoints\n",
    "    \n",
    "    def _cleanup_checkpoints(self):\n",
    "        \"\"\"清理旧的检查点，只保留最新的几个\"\"\"\n",
    "        checkpoints = list(self.save_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
    "        if len(checkpoints) <= self.max_checkpoints:\n",
    "            return\n",
    "        \n",
    "        # 按修改时间排序，删除最旧的\n",
    "        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        for old_ckpt in checkpoints[self.max_checkpoints:]:\n",
    "            old_ckpt.unlink()\n",
    "            print(f\"🗑️  已删除旧检查点: {old_ckpt.name}\")\n",
    "\n",
    "# 示例：创建检查点管理器\n",
    "checkpoint_manager = CheckpointManager(save_dir=\"./checkpoints\", max_checkpoints=3)\n",
    "\n",
    "print(\"✓ 断点管理器创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de3ae3",
   "metadata": {},
   "source": [
    "### 7.2 恢复训练功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a502ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training(checkpoint_path, model, optimizer, scheduler=None, config=None):\n",
    "    \"\"\"\n",
    "    从断点恢复训练\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: 断点文件路径\n",
    "        model: 模型实例\n",
    "        optimizer: 优化器\n",
    "        scheduler: 学习率调度器\n",
    "        config: 配置对象\n",
    "    \n",
    "    Returns:\n",
    "        start_epoch: 开始的epoch\n",
    "        history: 训练历史\n",
    "        best_val_acc: 最佳验证准确率\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint = checkpoint_manager.load_checkpoint(\n",
    "        checkpoint_path, model, optimizer, scheduler\n",
    "    )\n",
    "    \n",
    "    # 获取恢复信息\n",
    "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "    best_val_acc = checkpoint.get('val_acc', 0.0)\n",
    "    \n",
    "    # 恢复训练历史\n",
    "    history = TrainingHistory()\n",
    "    if 'history' in checkpoint:\n",
    "        history.__dict__.update(checkpoint['history'])\n",
    "    \n",
    "    # 恢复配置\n",
    "    if config is None and 'config' in checkpoint:\n",
    "        config = SimpleNamespace(**checkpoint['config'])\n",
    "    \n",
    "    print(f\"🚀 准备从 Epoch {start_epoch} 恢复训练\")\n",
    "    print(f\"📊 当前最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return start_epoch, history, best_val_acc, config\n",
    "\n",
    "# 演示：模拟恢复训练流程\n",
    "def demo_resume_training():\n",
    "    \"\"\"演示恢复训练的完整流程\"\"\"\n",
    "    \n",
    "    print(\"=== 断点恢复训练演示 ===\\n\")\n",
    "    \n",
    "    # 1. 查看可用的检查点\n",
    "    print(\"1. 查看可用的检查点:\")\n",
    "    available_checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    \n",
    "    if not available_checkpoints:\n",
    "        print(\"❌ 没有找到检查点文件，请先运行一些训练\")\n",
    "        return\n",
    "    \n",
    "    # 2. 选择最新的检查点进行恢复\n",
    "    latest_checkpoint = checkpoint_manager.find_latest_checkpoint()\n",
    "    print(f\"2. 选择最新检查点: {latest_checkpoint.name}\")\n",
    "    \n",
    "    # 3. 重新创建模型和优化器（模拟新的训练会话）\n",
    "    print(\"\\n3. 重新创建模型、优化器和调度器...\")\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # 4. 恢复训练状态\n",
    "    print(\"\\n4. 恢复训练状态...\")\n",
    "    try:\n",
    "        start_epoch, history, best_val_acc, restored_config = resume_training(\n",
    "            latest_checkpoint, model, optimizer, scheduler\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ 成功恢复训练状态!\")\n",
    "        print(f\"  - 下一个训练epoch: {start_epoch}\")\n",
    "        print(f\"  - 历史最佳准确率: {best_val_acc:.2f}%\")\n",
    "        print(f\"  - 已恢复训练历史: {len(history.train_losses)} 个epochs\")\n",
    "        \n",
    "        # 5. 可以继续训练（这里只是演示，不实际运行）\n",
    "        print(f\"\\n5. 现在可以从 epoch {start_epoch} 继续训练...\")\n",
    "        print(\"   (为了演示目的，这里不实际执行训练循环)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 恢复训练失败: {e}\")\n",
    "\n",
    "# 运行演示\n",
    "demo_resume_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a1193",
   "metadata": {},
   "source": [
    "## 8. TensorBoard 可视化\n",
    "\n",
    "TensorBoard 是一个强大的可视化工具，可以帮助我们：\n",
    "- 监控训练过程中的损失和指标\n",
    "- 可视化模型结构\n",
    "- 观察权重和梯度的分布\n",
    "- 比较不同实验的结果\n",
    "\n",
    "### 8.1 TensorBoard 基础使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffc8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class TensorBoardLogger:\n",
    "    \"\"\"\n",
    "    TensorBoard 日志记录器\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir, experiment_name=None):\n",
    "        if experiment_name is None:\n",
    "            experiment_name = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        self.log_dir = Path(log_dir) / experiment_name\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.writer = SummaryWriter(log_dir=str(self.log_dir))\n",
    "        self.step = 0\n",
    "        \n",
    "        print(f\"📊 TensorBoard 日志目录: {self.log_dir}\")\n",
    "        print(f\"💡 启动TensorBoard: tensorboard --logdir {log_dir}\")\n",
    "    \n",
    "    def log_scalar(self, tag, value, step=None):\n",
    "        \"\"\"记录标量值\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        self.writer.add_scalar(tag, value, step)\n",
    "    \n",
    "    def log_scalars(self, tag_dict, step=None):\n",
    "        \"\"\"记录多个标量值\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        for tag, value in tag_dict.items():\n",
    "            self.writer.add_scalar(tag, value, step)\n",
    "    \n",
    "    def log_histogram(self, tag, values, step=None):\n",
    "        \"\"\"记录直方图\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        self.writer.add_histogram(tag, values, step)\n",
    "    \n",
    "    def log_image(self, tag, image, step=None):\n",
    "        \"\"\"记录图像\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        self.writer.add_image(tag, image, step)\n",
    "    \n",
    "    def log_images(self, tag, images, step=None):\n",
    "        \"\"\"记录图像网格\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        grid = vutils.make_grid(images, normalize=True, scale_each=True)\n",
    "        self.writer.add_image(tag, grid, step)\n",
    "    \n",
    "    def log_model_graph(self, model, input_tensor):\n",
    "        \"\"\"记录模型计算图\"\"\"\n",
    "        self.writer.add_graph(model, input_tensor)\n",
    "    \n",
    "    def log_model_weights(self, model, step=None):\n",
    "        \"\"\"记录模型权重分布\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.writer.add_histogram(f\"weights/{name}\", param.data, step)\n",
    "                if param.grad is not None:\n",
    "                    self.writer.add_histogram(f\"gradients/{name}\", param.grad.data, step)\n",
    "    \n",
    "    def log_learning_rate(self, optimizer, step=None):\n",
    "        \"\"\"记录学习率\"\"\"\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "        \n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            lr = param_group['lr']\n",
    "            self.writer.add_scalar(f\"learning_rate/group_{i}\", lr, step)\n",
    "    \n",
    "    def increment_step(self):\n",
    "        \"\"\"增加步数\"\"\"\n",
    "        self.step += 1\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"关闭writer\"\"\"\n",
    "        self.writer.close()\n",
    "\n",
    "# 创建 TensorBoard logger\n",
    "tb_logger = TensorBoardLogger(\n",
    "    log_dir=\"./tensorboard_logs\", \n",
    "    experiment_name=\"mnist_cnn_demo\"\n",
    ")\n",
    "\n",
    "print(\"✓ TensorBoard 记录器创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea78bf3",
   "metadata": {},
   "source": [
    "### 8.2 TensorBoard 可视化演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_tensorboard_logging():\n",
    "    \"\"\"演示 TensorBoard 的各种功能\"\"\"\n",
    "    \n",
    "    print(\"=== TensorBoard 可视化演示 ===\\n\")\n",
    "    \n",
    "    # 1. 创建示例模型和数据\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 获取一个batch的数据用于演示\n",
    "    sample_data, sample_labels = next(iter(train_loader))\n",
    "    sample_data = sample_data.to(device)\n",
    "    \n",
    "    print(\"1. 记录模型计算图...\")\n",
    "    # 记录模型计算图\n",
    "    tb_logger.log_model_graph(model, sample_data[:1])  # 只用一个样本\n",
    "    \n",
    "    print(\"2. 记录样本图像...\")\n",
    "    # 记录样本图像\n",
    "    tb_logger.log_images(\"samples/train_images\", sample_data[:8])  # 前8张图像\n",
    "    \n",
    "    print(\"3. 模拟训练过程记录...\")\n",
    "    # 模拟一个短训练过程来演示日志记录\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for step in range(10):  # 只演示10步\n",
    "        # 前向传播\n",
    "        outputs = model(sample_data)\n",
    "        loss = criterion(outputs, sample_labels.to(device))\n",
    "        \n",
    "        # 计算准确率\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == sample_labels.to(device)).float().mean().item() * 100\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录到 TensorBoard\n",
    "        tb_logger.log_scalar(\"loss/train\", loss.item(), step)\n",
    "        tb_logger.log_scalar(\"accuracy/train\", accuracy, step)\n",
    "        tb_logger.log_learning_rate(optimizer, step)\n",
    "        \n",
    "        # 每5步记录一次权重分布\n",
    "        if step % 5 == 0:\n",
    "            tb_logger.log_model_weights(model, step)\n",
    "        \n",
    "        print(f\"Step {step+1}/10: Loss={loss.item():.4f}, Acc={accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"\\n4. 记录超参数和最终结果...\")\n",
    "    # 记录超参数\n",
    "    hparams = {\n",
    "        'lr': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'model': 'SimpleCNN',\n",
    "        'optimizer': 'Adam'\n",
    "    }\n",
    "    metrics = {\n",
    "        'final_loss': loss.item(),\n",
    "        'final_accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    # TensorBoard 的超参数记录\n",
    "    tb_logger.writer.add_hparams(hparams, metrics)\n",
    "    \n",
    "    print(\"5. 生成混淆矩阵可视化...\")\n",
    "    # 简单的预测结果分析\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "            if len(all_predictions) > 500:  # 只用前500个样本演示\n",
    "                break\n",
    "        \n",
    "        # 创建混淆矩阵\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import seaborn as sns\n",
    "        \n",
    "        cm = confusion_matrix(all_labels[:500], all_predictions[:500])\n",
    "        \n",
    "        # 绘制混淆矩阵\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        # 保存到 TensorBoard\n",
    "        tb_logger.writer.add_figure(\"confusion_matrix\", plt.gcf(), 0)\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"\\n✓ TensorBoard 演示完成!\")\n",
    "    print(f\"📊 日志已保存到: {tb_logger.log_dir}\")\n",
    "    print(f\"🌐 启动 TensorBoard: tensorboard --logdir {tb_logger.log_dir.parent}\")\n",
    "    print(\"   然后在浏览器中打开: http://localhost:6006\")\n",
    "\n",
    "# 运行演示\n",
    "demo_tensorboard_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87374f0d",
   "metadata": {},
   "source": [
    "## 9. 实验管理与日志记录\n",
    "\n",
    "在深度学习研究中，实验管理和详细的日志记录是非常重要的，它们可以帮助我们：\n",
    "- 追踪不同实验的超参数和结果\n",
    "- 复现实验结果\n",
    "- 比较不同方法的性能\n",
    "- 记录训练过程中的详细信息\n",
    "\n",
    "### 9.1 实验管理系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ac89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "import psutil\n",
    "import platform\n",
    "\n",
    "class ExperimentManager:\n",
    "    \"\"\"\n",
    "    实验管理器 - 统一管理实验的配置、日志和结果\n",
    "    \"\"\"\n",
    "    def __init__(self, experiment_name, base_dir=\"./experiments\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.exp_dir = self.base_dir / experiment_name\n",
    "        \n",
    "        # 创建实验目录结构\n",
    "        self.exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (self.exp_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "        (self.exp_dir / \"logs\").mkdir(exist_ok=True)\n",
    "        (self.exp_dir / \"results\").mkdir(exist_ok=True)\n",
    "        (self.exp_dir / \"tensorboard\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # 设置日志\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # 记录实验开始时间和系统信息\n",
    "        self.start_time = datetime.now()\n",
    "        self.log_system_info()\n",
    "        \n",
    "        print(f\"🔬 实验 '{experiment_name}' 已初始化\")\n",
    "        print(f\"📁 实验目录: {self.exp_dir}\")\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"设置日志系统\"\"\"\n",
    "        log_file = self.exp_dir / \"logs\" / \"experiment.log\"\n",
    "        \n",
    "        # 创建logger\n",
    "        self.logger = logging.getLogger(self.experiment_name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # 避免重复添加handler\n",
    "        if not self.logger.handlers:\n",
    "            # 文件handler\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            \n",
    "            # 控制台handler\n",
    "            console_handler = logging.StreamHandler(sys.stdout)\n",
    "            console_handler.setLevel(logging.INFO)\n",
    "            \n",
    "            # 格式化\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "            file_handler.setFormatter(formatter)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            \n",
    "            # 添加handlers\n",
    "            self.logger.addHandler(file_handler)\n",
    "            self.logger.addHandler(console_handler)\n",
    "    \n",
    "    def log_system_info(self):\n",
    "        \"\"\"记录系统信息\"\"\"\n",
    "        info = {\n",
    "            \"experiment_name\": self.experiment_name,\n",
    "            \"start_time\": self.start_time.isoformat(),\n",
    "            \"python_version\": platform.python_version(),\n",
    "            \"platform\": platform.platform(),\n",
    "            \"cpu_count\": psutil.cpu_count(),\n",
    "            \"memory_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "            \"cuda_available\": torch.cuda.is_available(),\n",
    "            \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
    "            \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            info[\"gpu_names\"] = [torch.cuda.get_device_name(i) \n",
    "                               for i in range(torch.cuda.device_count())]\n",
    "        \n",
    "        # 保存系统信息\n",
    "        with open(self.exp_dir / \"system_info.json\", \"w\") as f:\n",
    "            json.dump(info, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"System info: {json.dumps(info, indent=2)}\")\n",
    "    \n",
    "    def log_config(self, config: Dict[str, Any]):\n",
    "        \"\"\"记录实验配置\"\"\"\n",
    "        config_file = self.exp_dir / \"config.json\"\n",
    "        \n",
    "        with open(config_file, \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Configuration: {json.dumps(config, indent=2)}\")\n",
    "    \n",
    "    def log_info(self, message: str):\n",
    "        \"\"\"记录信息\"\"\"\n",
    "        self.logger.info(message)\n",
    "    \n",
    "    def log_results(self, results: Dict[str, Any], epoch: int = None):\n",
    "        \"\"\"记录结果\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        result_entry = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"epoch\": epoch,\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        # 追加到结果文件\n",
    "        results_file = self.exp_dir / \"results\" / \"results.jsonl\"\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(json.dumps(result_entry) + \"\\n\")\n",
    "        \n",
    "        self.logger.info(f\"Results (epoch {epoch}): {json.dumps(results)}\")\n",
    "    \n",
    "    def save_model(self, model, filename, **kwargs):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        save_path = self.exp_dir / \"checkpoints\" / filename\n",
    "        \n",
    "        save_dict = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        torch.save(save_dict, save_path)\n",
    "        self.logger.info(f\"Model saved: {filename}\")\n",
    "        return save_path\n",
    "    \n",
    "    def get_tensorboard_dir(self):\n",
    "        \"\"\"获取TensorBoard目录\"\"\"\n",
    "        return str(self.exp_dir / \"tensorboard\")\n",
    "    \n",
    "    def finalize(self, final_results: Dict[str, Any] = None):\n",
    "        \"\"\"结束实验，记录最终结果\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - self.start_time\n",
    "        \n",
    "        summary = {\n",
    "            \"experiment_name\": self.experiment_name,\n",
    "            \"start_time\": self.start_time.isoformat(),\n",
    "            \"end_time\": end_time.isoformat(),\n",
    "            \"duration_seconds\": duration.total_seconds(),\n",
    "            \"duration_formatted\": str(duration),\n",
    "            \"final_results\": final_results or {}\n",
    "        }\n",
    "        \n",
    "        # 保存实验总结\n",
    "        with open(self.exp_dir / \"experiment_summary.json\", \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Experiment completed. Duration: {duration}\")\n",
    "        if final_results:\n",
    "            self.logger.info(f\"Final results: {json.dumps(final_results)}\")\n",
    "\n",
    "# 示例：创建实验管理器\n",
    "exp_manager = ExperimentManager(\"mnist_cnn_v1\")\n",
    "\n",
    "# 示例配置\n",
    "sample_config = {\n",
    "    \"model\": \"SimpleCNN\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "exp_manager.log_config(sample_config)\n",
    "exp_manager.log_info(\"实验管理器演示完成\")\n",
    "\n",
    "print(\"✓ 实验管理器创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c4f08",
   "metadata": {},
   "source": [
    "## 10. 完整的 MNIST 分类案例\n",
    "\n",
    "现在我们将所有前面学到的技术整合起来，创建一个完整的、生产就绪的 MNIST 分类项目。这个案例将包括：\n",
    "- 完整的项目结构\n",
    "- 配置管理\n",
    "- 数据加载和预处理\n",
    "- 模型定义和训练\n",
    "- 断点保存和恢复\n",
    "- TensorBoard 可视化\n",
    "- 实验管理\n",
    "- 模型评估和测试\n",
    "\n",
    "### 10.1 项目配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5cac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTConfig:\n",
    "    \"\"\"MNIST项目的完整配置\"\"\"\n",
    "    def __init__(self):\n",
    "        # 数据相关\n",
    "        self.data_dir = \"./data\"\n",
    "        self.batch_size = 64\n",
    "        self.num_workers = 4\n",
    "        self.pin_memory = True\n",
    "        \n",
    "        # 模型相关\n",
    "        self.model_name = \"SimpleCNN\"\n",
    "        self.num_classes = 10\n",
    "        self.dropout_rate = 0.5\n",
    "        \n",
    "        # 训练相关\n",
    "        self.epochs = 10\n",
    "        self.learning_rate = 0.001\n",
    "        self.weight_decay = 1e-4\n",
    "        self.optimizer = \"Adam\"\n",
    "        \n",
    "        # 调度器相关\n",
    "        self.scheduler = \"StepLR\"\n",
    "        self.step_size = 5\n",
    "        self.gamma = 0.5\n",
    "        \n",
    "        # 早停相关\n",
    "        self.early_stopping = True\n",
    "        self.patience = 5\n",
    "        self.min_delta = 0.001\n",
    "        \n",
    "        # 保存相关\n",
    "        self.save_frequency = 2\n",
    "        self.max_checkpoints = 3\n",
    "        \n",
    "        # 实验相关\n",
    "        self.experiment_name = f\"mnist_cnn_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.use_amp = torch.cuda.is_available()\n",
    "        \n",
    "        # TensorBoard\n",
    "        self.log_every = 100  # 每多少步记录一次\n",
    "        \n",
    "        # 随机种子\n",
    "        self.seed = 42\n",
    "\n",
    "def complete_mnist_training():\n",
    "    \"\"\"完整的MNIST训练流程\"\"\"\n",
    "    \n",
    "    print(\"🚀 开始完整的 MNIST 分类训练\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 初始化配置\n",
    "    config = MNISTConfig()\n",
    "    \n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(config.seed)\n",
    "    \n",
    "    # 2. 初始化实验管理器\n",
    "    exp_manager = ExperimentManager(config.experiment_name)\n",
    "    exp_manager.log_config(config.__dict__)\n",
    "    \n",
    "    # 3. 初始化 TensorBoard\n",
    "    tb_logger = TensorBoardLogger(\n",
    "        log_dir=exp_manager.get_tensorboard_dir(),\n",
    "        experiment_name=\"training\"\n",
    "    )\n",
    "    \n",
    "    # 4. 创建数据加载器\n",
    "    exp_manager.log_info(\"创建数据加载器...\")\n",
    "    train_loader = create_dataloader(\n",
    "        dataset_type='train',\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    val_loader = create_dataloader(\n",
    "        dataset_type='val',\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    test_loader = create_dataloader(\n",
    "        dataset_type='test',\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    # 5. 创建模型\n",
    "    exp_manager.log_info(\"创建模型...\")\n",
    "    model = SimpleCNN(num_classes=config.num_classes, dropout_rate=config.dropout_rate)\n",
    "    model = model.to(config.device)\n",
    "    \n",
    "    # 记录模型信息\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    exp_manager.log_info(f\"模型总参数数: {total_params:,}\")\n",
    "    exp_manager.log_info(f\"可训练参数数: {trainable_params:,}\")\n",
    "    \n",
    "    # 6. 创建优化器和调度器\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.step_size,\n",
    "        gamma=config.gamma\n",
    "    )\n",
    "    \n",
    "    # 7. 创建损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 8. 创建早停和检查点管理器\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config.patience,\n",
    "        min_delta=config.min_delta\n",
    "    ) if config.early_stopping else None\n",
    "    \n",
    "    checkpoint_manager = CheckpointManager(\n",
    "        save_dir=exp_manager.exp_dir / \"checkpoints\",\n",
    "        max_checkpoints=config.max_checkpoints\n",
    "    )\n",
    "    \n",
    "    # 9. 混合精度训练\n",
    "    scaler = torch.cuda.amp.GradScaler() if config.use_amp else None\n",
    "    \n",
    "    # 10. 记录模型图\n",
    "    sample_input = next(iter(train_loader))[0][:1].to(config.device)\n",
    "    tb_logger.log_model_graph(model, sample_input)\n",
    "    \n",
    "    # 11. 训练循环\n",
    "    exp_manager.log_info(\"开始训练...\")\n",
    "    history = TrainingHistory()\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # 训练\n",
    "        train_loss, train_acc = train_epoch_with_amp(\n",
    "            model, train_loader, criterion, optimizer, config.device,\n",
    "            scaler if config.use_amp else None, epoch, config, tb_logger\n",
    "        )\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_acc = validate_epoch(\n",
    "            model, val_loader, criterion, config.device, epoch, config\n",
    "        )\n",
    "        \n",
    "        # 更新调度器\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 更新历史\n",
    "        history.update(epoch, train_loss, train_acc, val_loss, val_acc, current_lr)\n",
    "        \n",
    "        # TensorBoard 记录\n",
    "        tb_logger.log_scalars({\n",
    "            'Loss/Train': train_loss,\n",
    "            'Loss/Validation': val_loss,\n",
    "            'Accuracy/Train': train_acc,\n",
    "            'Accuracy/Validation': val_acc,\n",
    "            'Learning_Rate': current_lr\n",
    "        }, epoch)\n",
    "        \n",
    "        # 记录权重分布\n",
    "        if epoch % 2 == 0:\n",
    "            tb_logger.log_model_weights(model, epoch)\n",
    "        \n",
    "        # 计算时间\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # 记录结果\n",
    "        epoch_results = {\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'learning_rate': current_lr,\n",
    "            'epoch_time': epoch_time\n",
    "        }\n",
    "        exp_manager.log_results(epoch_results, epoch)\n",
    "        \n",
    "        # 输出结果\n",
    "        exp_manager.log_info(\n",
    "            f\"Epoch {epoch+1}/{config.epochs} - \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% - \"\n",
    "            f\"LR: {current_lr:.6f} - Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        is_best = val_acc > best_val_acc\n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "            exp_manager.log_info(f\"新的最佳模型! 验证准确率: {val_acc:.2f}%\")\n",
    "        \n",
    "        # 保存检查点\n",
    "        if (epoch + 1) % config.save_frequency == 0 or is_best:\n",
    "            checkpoint_state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'history': history.__dict__,\n",
    "                'config': config.__dict__,\n",
    "                'best_val_acc': best_val_acc\n",
    "            }\n",
    "            \n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                checkpoint_state, epoch, is_best=is_best\n",
    "            )\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping:\n",
    "            if early_stopping(val_loss, model):\n",
    "                exp_manager.log_info(f\"早停触发，在第 {epoch+1} 轮停止训练\")\n",
    "                break\n",
    "    \n",
    "    # 12. 测试最佳模型\n",
    "    exp_manager.log_info(\"在测试集上评估最佳模型...\")\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    best_checkpoint = checkpoint_manager.save_dir / \"best_model.pth\"\n",
    "    if best_checkpoint.exists():\n",
    "        checkpoint = torch.load(best_checkpoint, map_location=config.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 测试\n",
    "    test_loss, test_acc = test_model(model, test_loader, criterion, config.device)\n",
    "    \n",
    "    # 13. 最终结果\n",
    "    final_results = {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'total_epochs': epoch + 1,\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params\n",
    "    }\n",
    "    \n",
    "    exp_manager.log_info(f\"训练完成! 最佳验证准确率: {best_val_acc:.2f}%, 测试准确率: {test_acc:.2f}%\")\n",
    "    \n",
    "    # 14. 保存最终结果和关闭资源\n",
    "    history.save(exp_manager.exp_dir / \"training_history.json\")\n",
    "    exp_manager.finalize(final_results)\n",
    "    tb_logger.close()\n",
    "    \n",
    "    print(\"\\n🎉 完整的 MNIST 训练流程结束!\")\n",
    "    print(f\"📊 实验目录: {exp_manager.exp_dir}\")\n",
    "    print(f\"🏆 最终测试准确率: {test_acc:.2f}%\")\n",
    "    \n",
    "    return model, history, final_results\n",
    "\n",
    "# 这里我们创建配置但暂不运行完整训练（训练时间较长）\n",
    "config = MNISTConfig()\n",
    "print(\"✓ MNIST 完整项目配置创建完成\")\n",
    "print(f\"📋 实验名称: {config.experiment_name}\")\n",
    "print(\"💡 运行 complete_mnist_training() 开始完整训练\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_amp(model, dataloader, criterion, optimizer, device, \n",
    "                        scaler, epoch, config, tb_logger):\n",
    "    \"\"\"带有混合精度的训练epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:  # 使用混合精度\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:  # 标准训练\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # TensorBoard 记录\n",
    "        if batch_idx % config.log_every == 0:\n",
    "            step = epoch * len(dataloader) + batch_idx\n",
    "            tb_logger.log_scalar('Batch/Loss', loss.item(), step)\n",
    "            tb_logger.log_scalar('Batch/Accuracy', \n",
    "                               100. * correct / total, step)\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    \"\"\"在测试集上评估模型\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            all_predictions.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "print(\"✓ 辅助函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4516c5",
   "metadata": {},
   "source": [
    "## 11. PyTorch 最佳实践总结\n",
    "\n",
    "通过本教程，我们学习了 PyTorch 的各个方面。以下是一些重要的最佳实践：\n",
    "\n",
    "### 11.1 代码组织\n",
    "\n",
    "1. **模块化设计**：将数据加载、模型定义、训练循环分离\n",
    "2. **配置管理**：使用配置类统一管理超参数\n",
    "3. **实验管理**：为每个实验创建独立的目录和日志\n",
    "4. **版本控制**：使用 Git 管理代码，记录每次实验的代码版本\n",
    "\n",
    "### 11.2 训练优化\n",
    "\n",
    "1. **混合精度训练**：在支持的硬件上使用 AMP 加速训练\n",
    "2. **数据加载优化**：合理设置 `num_workers` 和 `pin_memory`\n",
    "3. **学习率调度**：使用学习率调度器优化训练过程\n",
    "4. **早停机制**：防止过拟合，节省训练时间\n",
    "\n",
    "### 11.3 调试和监控\n",
    "\n",
    "1. **TensorBoard 可视化**：监控训练过程和模型性能\n",
    "2. **详细日志记录**：记录所有重要信息便于复现\n",
    "3. **断点保存**：支持训练中断后的恢复\n",
    "4. **性能分析**：监控 GPU 利用率和内存使用\n",
    "\n",
    "### 11.4 模型部署准备\n",
    "\n",
    "1. **模型保存**：保存完整的模型状态\n",
    "2. **推理优化**：使用 `torch.jit` 或 TorchScript\n",
    "3. **模型压缩**：量化、剪枝等技术\n",
    "4. **端到端测试**：确保模型在生产环境中正常工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75558876",
   "metadata": {},
   "source": [
    "### 11.5 常用代码模板\n",
    "\n",
    "以下是一些常用的 PyTorch 代码模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 基本训练循环模板\n",
    "basic_training_template = \"\"\"\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.6f}')\n",
    "\"\"\"\n",
    "\n",
    "# 2. 模型保存和加载模板\n",
    "save_load_template = \"\"\"\n",
    "# 保存\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "}, checkpoint_path)\n",
    "\n",
    "# 加载\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\"\"\"\n",
    "\n",
    "# 3. 自定义数据集模板\n",
    "custom_dataset_template = \"\"\"\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        data = self._load_data(sample)\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, sample['label']\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        # 实现样本加载逻辑\n",
    "        pass\n",
    "    \n",
    "    def _load_data(self, sample):\n",
    "        # 实现数据加载逻辑\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "print(\"📋 常用代码模板:\")\n",
    "print(\"1. 基本训练循环\")\n",
    "print(\"2. 模型保存和加载\")\n",
    "print(\"3. 自定义数据集\")\n",
    "print(\"\\n💡 这些模板可以作为你项目的起点进行修改\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ccb8d",
   "metadata": {},
   "source": [
    "### 11.6 进阶学习建议\n",
    "\n",
    "完成本教程后，你可以继续学习以下高级主题：\n",
    "\n",
    "1. **分布式训练**：\n",
    "   - 数据并行 (DataParallel, DistributedDataParallel)\n",
    "   - 模型并行\n",
    "   - 混合并行策略\n",
    "\n",
    "2. **模型优化**：\n",
    "   - 量化 (Quantization)\n",
    "   - 剪枝 (Pruning)\n",
    "   - 知识蒸馏 (Knowledge Distillation)\n",
    "   - TorchScript 和 ONNX\n",
    "\n",
    "3. **高级架构**：\n",
    "   - Transformer 模型\n",
    "   - 生成对抗网络 (GANs)\n",
    "   - 变分自编码器 (VAEs)\n",
    "   - 图神经网络 (GNNs)\n",
    "\n",
    "4. **专门领域**：\n",
    "   - 计算机视觉：目标检测、图像分割\n",
    "   - 自然语言处理：BERT、GPT 等\n",
    "   - 强化学习：DQN、PPO 等\n",
    "   - 时间序列分析：LSTM、Transformer\n",
    "\n",
    "### 11.7 推荐资源\n",
    "\n",
    "- **官方文档**：https://pytorch.org/docs/\n",
    "- **官方教程**：https://pytorch.org/tutorials/\n",
    "- **PyTorch 示例**：https://github.com/pytorch/examples\n",
    "- **Papers with Code**：https://paperswithcode.com/\n",
    "- **Awesome PyTorch**：https://github.com/bharathgs/Awesome-pytorch-list\n",
    "\n",
    "## 12. 总结\n",
    "\n",
    "恭喜你完成了 PyTorch 深度学习教程！🎉\n",
    "\n",
    "在本教程中，我们学习了：\n",
    "\n",
    "✅ **PyTorch 基础**：张量操作、自动微分机制\n",
    "✅ **神经网络构建**：从简单的全连接到复杂的CNN\n",
    "✅ **数据处理**：自定义数据集、数据加载优化\n",
    "✅ **训练优化**：优化器、调度器、混合精度训练\n",
    "✅ **实验管理**：断点保存、TensorBoard 可视化、日志记录\n",
    "✅ **完整项目**：端到端的 MNIST 分类案例\n",
    "\n",
    "现在你已经具备了使用 PyTorch 进行深度学习研究和开发的基础技能。记住，深度学习是一个快速发展的领域，持续学习和实践是成功的关键。\n",
    "\n",
    "**下一步建议**：\n",
    "1. 尝试在自己的数据集上应用所学知识\n",
    "2. 参与开源项目，贡献代码\n",
    "3. 阅读最新的研究论文并尝试复现\n",
    "4. 关注 PyTorch 的最新发展和最佳实践\n",
    "\n",
    "祝你在深度学习的道路上取得成功！🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
