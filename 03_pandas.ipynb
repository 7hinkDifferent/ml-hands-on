{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f7ffa3",
   "metadata": {},
   "source": [
    "# Pandas - æ•°æ®å¤„ç†ä¸åˆ†æ\n",
    "\n",
    "Pandasæ˜¯Pythonä¸­æœ€é‡è¦çš„æ•°æ®åˆ†æåº“ï¼Œæä¾›äº†é«˜æ•ˆçš„æ•°æ®ç»“æ„å’Œæ•°æ®æ“ä½œå·¥å…·ã€‚\n",
    "\n",
    "## ä¸»è¦ç‰¹æ€§\n",
    "- ğŸ“Š **DataFrame**: ç±»ä¼¼Excelçš„äºŒç»´æ•°æ®ç»“æ„\n",
    "- ğŸ”„ **æ•°æ®æ¸…æ´—**: å¤„ç†ç¼ºå¤±å€¼ã€é‡å¤å€¼\n",
    "- ğŸ“ˆ **æ•°æ®åˆ†æ**: åˆ†ç»„ã€èšåˆã€ç»Ÿè®¡\n",
    "- ğŸ“ **æ–‡ä»¶I/O**: è¯»å†™CSVã€Excelã€JSONç­‰æ ¼å¼\n",
    "- ğŸ•’ **æ—¶é—´åºåˆ—**: å¼ºå¤§çš„æ—¶é—´æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "data = {\n",
    "    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],\n",
    "    'å¹´é¾„': [25, 30, 35, 28, 32],\n",
    "    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·'],\n",
    "    'è–ªèµ„': [8000, 12000, 15000, 11000, 13000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"åˆ›å»ºçš„DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\næ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"æ•°æ®ç±»å‹:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd96d7",
   "metadata": {},
   "source": [
    "## 1. Pandasæ•°æ®ç»“æ„è¯¦è§£\n",
    "\n",
    "Pandasæä¾›äº†ä¸¤ç§ä¸»è¦çš„æ•°æ®ç»“æ„ï¼šSeriesï¼ˆä¸€ç»´ï¼‰å’ŒDataFrameï¼ˆäºŒç»´ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series - ä¸€ç»´æ•°æ®ç»“æ„\n",
    "print(\"=== Series åˆ›å»ºå’Œæ“ä½œ ===\")\n",
    "\n",
    "# ä»åˆ—è¡¨åˆ›å»ºSeries\n",
    "scores = pd.Series([85, 92, 78, 96, 88], name='è€ƒè¯•æˆç»©')\n",
    "print(f\"æˆç»©Series:\\n{scores}\")\n",
    "print(f\"æ•°æ®ç±»å‹: {scores.dtype}\")\n",
    "print(f\"ç´¢å¼•: {scores.index.tolist()}\")\n",
    "\n",
    "# å¸¦è‡ªå®šä¹‰ç´¢å¼•çš„Series\n",
    "student_scores = pd.Series(\n",
    "    [85, 92, 78, 96, 88], \n",
    "    index=['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],\n",
    "    name='æœŸæœ«æˆç»©'\n",
    ")\n",
    "print(f\"\\nå¸¦ç´¢å¼•çš„Series:\\n{student_scores}\")\n",
    "\n",
    "# ä»å­—å…¸åˆ›å»ºSeries\n",
    "grade_dict = {'è¯­æ–‡': 85, 'æ•°å­¦': 92, 'è‹±è¯­': 78, 'ç‰©ç†': 96, 'åŒ–å­¦': 88}\n",
    "subjects = pd.Series(grade_dict, name='ç§‘ç›®æˆç»©')\n",
    "print(f\"\\nç§‘ç›®æˆç»©:\\n{subjects}\")\n",
    "\n",
    "# SeriesåŸºæœ¬æ“ä½œ\n",
    "print(f\"\\n=== Series åŸºæœ¬æ“ä½œ ===\")\n",
    "print(f\"å¹³å‡åˆ†: {subjects.mean():.2f}\")\n",
    "print(f\"æœ€é«˜åˆ†: {subjects.max()}\")\n",
    "print(f\"æœ€ä½åˆ†ç§‘ç›®: {subjects.idxmin()}\")\n",
    "print(f\"åŠæ ¼ç§‘ç›®æ•°: {(subjects >= 80).sum()}\")\n",
    "print(f\"æˆç»©æ’åº:\\n{subjects.sort_values(ascending=False)}\")\n",
    "\n",
    "# DataFrameåˆ›å»ºçš„å¤šç§æ–¹å¼\n",
    "print(\"\\n=== DataFrame åˆ›å»ºæ–¹å¼ ===\")\n",
    "\n",
    "# æ–¹å¼1ï¼šä»å­—å…¸åˆ›å»º\n",
    "employee_data = {\n",
    "    'å‘˜å·¥ID': ['E001', 'E002', 'E003', 'E004', 'E005'],\n",
    "    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],\n",
    "    'éƒ¨é—¨': ['æŠ€æœ¯éƒ¨', 'é”€å”®éƒ¨', 'æŠ€æœ¯éƒ¨', 'äººäº‹éƒ¨', 'è´¢åŠ¡éƒ¨'],\n",
    "    'å…¥èŒæ—¥æœŸ': ['2020-01-15', '2019-03-20', '2021-07-10', '2018-11-05', '2020-09-30'],\n",
    "    'è–ªèµ„': [12000, 8500, 15000, 9000, 11000],\n",
    "    'å¹´é¾„': [28, 32, 26, 35, 30]\n",
    "}\n",
    "\n",
    "df_employees = pd.DataFrame(employee_data)\n",
    "print(\"å‘˜å·¥ä¿¡æ¯DataFrame:\")\n",
    "print(df_employees)\n",
    "\n",
    "# æ–¹å¼2ï¼šä»Serieså­—å…¸åˆ›å»º\n",
    "name_series = pd.Series(['Alice', 'Bob', 'Charlie', 'Diana'])\n",
    "age_series = pd.Series([25, 30, 35, 28])\n",
    "salary_series = pd.Series([50000, 60000, 70000, 55000])\n",
    "\n",
    "df_from_series = pd.DataFrame({\n",
    "    'Name': name_series,\n",
    "    'Age': age_series,\n",
    "    'Salary': salary_series\n",
    "})\n",
    "print(f\"\\nä»Seriesåˆ›å»ºçš„DataFrame:\\n{df_from_series}\")\n",
    "\n",
    "# æ–¹å¼3ï¼šä»NumPyæ•°ç»„åˆ›å»º\n",
    "np.random.seed(42)\n",
    "array_data = np.random.rand(4, 3)\n",
    "df_from_array = pd.DataFrame(\n",
    "    array_data, \n",
    "    columns=['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3'],\n",
    "    index=['æ ·æœ¬1', 'æ ·æœ¬2', 'æ ·æœ¬3', 'æ ·æœ¬4']\n",
    ")\n",
    "print(f\"\\nä»NumPyæ•°ç»„åˆ›å»ºçš„DataFrame:\\n{df_from_array}\")\n",
    "\n",
    "# DataFrameåŸºæœ¬ä¿¡æ¯\n",
    "print(\"\\n=== DataFrame åŸºæœ¬ä¿¡æ¯ ===\")\n",
    "print(f\"å½¢çŠ¶: {df_employees.shape}\")\n",
    "print(f\"åˆ—å: {df_employees.columns.tolist()}\")\n",
    "print(f\"ç´¢å¼•: {df_employees.index.tolist()}\")\n",
    "print(f\"æ•°æ®ç±»å‹:\\n{df_employees.dtypes}\")\n",
    "print(f\"\\nå†…å­˜ä½¿ç”¨æƒ…å†µ:\")\n",
    "print(df_employees.info())\n",
    "\n",
    "# æ•°æ®é¢„è§ˆ\n",
    "print(\"\\n=== æ•°æ®é¢„è§ˆ ===\")\n",
    "print(\"å‰3è¡Œ:\")\n",
    "print(df_employees.head(3))\n",
    "print(\"\\nå2è¡Œ:\")\n",
    "print(df_employees.tail(2))\n",
    "print(\"\\néšæœºé‡‡æ ·2è¡Œ:\")\n",
    "print(df_employees.sample(2, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389aeab",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®ç´¢å¼•å’Œé€‰æ‹©\n",
    "\n",
    "Pandasæä¾›äº†å¤šç§çµæ´»çš„æ•°æ®é€‰æ‹©å’Œç´¢å¼•æ–¹æ³•ï¼Œè¿™æ˜¯æ•°æ®åˆ†æçš„åŸºç¡€æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101edf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "np.random.seed(42)\n",
    "sales_data = pd.DataFrame({\n",
    "    'æ—¥æœŸ': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "    'äº§å“': np.random.choice(['æ‰‹æœº', 'ç”µè„‘', 'å¹³æ¿', 'è€³æœº'], 100),\n",
    "    'é”€å”®é¢': np.random.randint(1000, 10000, 100),\n",
    "    'é”€é‡': np.random.randint(1, 50, 100),\n",
    "    'åœ°åŒº': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'], 100),\n",
    "    'é”€å”®å‘˜': np.random.choice(['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'], 100)\n",
    "})\n",
    "\n",
    "print(\"é”€å”®æ•°æ®ç¤ºä¾‹:\")\n",
    "print(sales_data.head())\n",
    "\n",
    "print(\"\\n=== åˆ—é€‰æ‹© ===\")\n",
    "# å•åˆ—é€‰æ‹©\n",
    "product_column = sales_data['äº§å“']\n",
    "print(f\"äº§å“åˆ—ç±»å‹: {type(product_column)}\")\n",
    "print(f\"äº§å“å‰5ä¸ª: {product_column.head().tolist()}\")\n",
    "\n",
    "# å¤šåˆ—é€‰æ‹©\n",
    "selected_cols = sales_data[['äº§å“', 'é”€å”®é¢', 'é”€é‡']]\n",
    "print(f\"\\né€‰æ‹©å¤šåˆ—çš„å½¢çŠ¶: {selected_cols.shape}\")\n",
    "print(selected_cols.head(3))\n",
    "\n",
    "# åˆ—é€‰æ‹©çš„ä¸åŒæ–¹å¼\n",
    "sales_amount = sales_data.é”€å”®é¢  # ç‚¹å·è®¿é—®ï¼ˆæ³¨æ„ï¼šåˆ—åä¸èƒ½æœ‰ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦ï¼‰\n",
    "print(f\"\\nç‚¹å·è®¿é—®é”€å”®é¢å¹³å‡å€¼: {sales_amount.mean():.2f}\")\n",
    "\n",
    "print(\"\\n=== è¡Œé€‰æ‹© ===\")\n",
    "# åŸºäºä½ç½®çš„é€‰æ‹© (iloc)\n",
    "first_row = sales_data.iloc[0]  # ç¬¬ä¸€è¡Œ\n",
    "print(f\"ç¬¬ä¸€è¡Œæ•°æ®:\\n{first_row}\")\n",
    "\n",
    "first_5_rows = sales_data.iloc[:5]  # å‰5è¡Œ\n",
    "print(f\"\\nå‰5è¡Œçš„å½¢çŠ¶: {first_5_rows.shape}\")\n",
    "\n",
    "# åŸºäºæ ‡ç­¾çš„é€‰æ‹© (loc)\n",
    "# é¦–å…ˆè®¾ç½®ä¸€ä¸ªæœ‰æ„ä¹‰çš„ç´¢å¼•\n",
    "sales_indexed = sales_data.set_index('æ—¥æœŸ')\n",
    "print(\"\\nåŸºäºæ—¥æœŸç´¢å¼•çš„å‰3è¡Œ:\")\n",
    "print(sales_indexed.head(3))\n",
    "\n",
    "# é€‰æ‹©ç‰¹å®šæ—¥æœŸ\n",
    "specific_date = sales_indexed.loc['2024-01-01']\n",
    "print(f\"\\n2024-01-01çš„æ•°æ®ç±»å‹: {type(specific_date)}\")\n",
    "if isinstance(specific_date, pd.Series):\n",
    "    print(f\"è¯¥æ—¥æœŸåªæœ‰ä¸€æ¡è®°å½•:\\n{specific_date}\")\n",
    "else:\n",
    "    print(f\"è¯¥æ—¥æœŸæœ‰{len(specific_date)}æ¡è®°å½•\")\n",
    "\n",
    "# æ—¥æœŸèŒƒå›´é€‰æ‹©\n",
    "jan_data = sales_indexed.loc['2024-01-01':'2024-01-10']\n",
    "print(f\"\\n1æœˆå‰10å¤©çš„æ•°æ®æ¡æ•°: {len(jan_data)}\")\n",
    "\n",
    "print(\"\\n=== æ¡ä»¶é€‰æ‹© ===\")\n",
    "# å•æ¡ä»¶ç­›é€‰\n",
    "high_sales = sales_data[sales_data['é”€å”®é¢'] > 8000]\n",
    "print(f\"é«˜é”€å”®é¢è®°å½•æ•°: {len(high_sales)}\")\n",
    "print(f\"é«˜é”€å”®é¢å¹³å‡å€¼: {high_sales['é”€å”®é¢'].mean():.2f}\")\n",
    "\n",
    "# å¤šæ¡ä»¶ç­›é€‰\n",
    "beijing_phones = sales_data[\n",
    "    (sales_data['åœ°åŒº'] == 'åŒ—äº¬') & \n",
    "    (sales_data['äº§å“'] == 'æ‰‹æœº')\n",
    "]\n",
    "print(f\"\\nåŒ—äº¬æ‰‹æœºé”€å”®è®°å½•æ•°: {len(beijing_phones)}\")\n",
    "\n",
    "# ä½¿ç”¨ isin() æ–¹æ³•\n",
    "tier1_cities = sales_data[sales_data['åœ°åŒº'].isin(['åŒ—äº¬', 'ä¸Šæµ·'])]\n",
    "print(f\"ä¸€çº¿åŸå¸‚é”€å”®è®°å½•æ•°: {len(tier1_cities)}\")\n",
    "\n",
    "# å­—ç¬¦ä¸²æ¡ä»¶\n",
    "sales_person_zhang = sales_data[sales_data['é”€å”®å‘˜'].str.contains('å¼ ')]\n",
    "print(f\"é”€å”®å‘˜å§“å¼ çš„è®°å½•æ•°: {len(sales_person_zhang)}\")\n",
    "\n",
    "print(\"\\n=== å¤æ‚é€‰æ‹©æ“ä½œ ===\")\n",
    "# ç»„åˆé€‰æ‹©ï¼šç‰¹å®šè¡Œå’Œåˆ—\n",
    "specific_selection = sales_data.loc[\n",
    "    sales_data['é”€å”®é¢'] > 7000, \n",
    "    ['äº§å“', 'é”€å”®é¢', 'åœ°åŒº']\n",
    "]\n",
    "print(f\"é«˜é”€å”®é¢çš„äº§å“å’Œåœ°åŒºä¿¡æ¯:\\n{specific_selection.head()}\")\n",
    "\n",
    "# ä½¿ç”¨ query() æ–¹æ³•\n",
    "query_result = sales_data.query('é”€å”®é¢ > 8000 and åœ°åŒº == \"ä¸Šæµ·\"')\n",
    "print(f\"\\nä½¿ç”¨queryç­›é€‰çš„ç»“æœæ•°: {len(query_result)}\")\n",
    "\n",
    "# éšæœºé‡‡æ ·\n",
    "random_sample = sales_data.sample(n=5, random_state=42)\n",
    "print(f\"\\néšæœºé‡‡æ ·5æ¡è®°å½•:\\n{random_sample[['äº§å“', 'é”€å”®é¢', 'åœ°åŒº']]}\")\n",
    "\n",
    "print(\"\\n=== æ•°æ®æ’åº ===\")\n",
    "# å•åˆ—æ’åº\n",
    "sorted_by_sales = sales_data.sort_values('é”€å”®é¢', ascending=False)\n",
    "print(\"æŒ‰é”€å”®é¢é™åºæ’åˆ—çš„å‰3å:\")\n",
    "print(sorted_by_sales[['äº§å“', 'é”€å”®é¢', 'é”€å”®å‘˜']].head(3))\n",
    "\n",
    "# å¤šåˆ—æ’åº\n",
    "multi_sorted = sales_data.sort_values(['åœ°åŒº', 'é”€å”®é¢'], ascending=[True, False])\n",
    "print(f\"\\næŒ‰åœ°åŒºå‡åºã€é”€å”®é¢é™åºæ’åˆ—çš„å‰5æ¡:\")\n",
    "print(multi_sorted[['åœ°åŒº', 'äº§å“', 'é”€å”®é¢']].head())\n",
    "\n",
    "# ç´¢å¼•æ’åº\n",
    "index_sorted = sales_data.sort_index()\n",
    "print(f\"æŒ‰ç´¢å¼•æ’åºåçš„å‰3è¡Œ:\\n{index_sorted.head(3)}\")\n",
    "\n",
    "print(\"\\n=== å»é‡æ“ä½œ ===\")\n",
    "# æŸ¥çœ‹é‡å¤å€¼\n",
    "duplicate_products = sales_data['äº§å“'].duplicated()\n",
    "print(f\"äº§å“åˆ—é‡å¤å€¼æ•°é‡: {duplicate_products.sum()}\")\n",
    "\n",
    "# å»é™¤é‡å¤çš„äº§å“\n",
    "unique_products = sales_data['äº§å“'].drop_duplicates()\n",
    "print(f\"å”¯ä¸€äº§å“: {unique_products.tolist()}\")\n",
    "\n",
    "# åŸºäºå¤šåˆ—å»é‡\n",
    "unique_combinations = sales_data[['äº§å“', 'åœ°åŒº']].drop_duplicates()\n",
    "print(f\"\\näº§å“-åœ°åŒºå”¯ä¸€ç»„åˆæ•°: {len(unique_combinations)}\")\n",
    "print(unique_combinations)\n",
    "\n",
    "print(\"\\n=== è®¾ç½®å’Œé‡ç½®ç´¢å¼• ===\")\n",
    "# è®¾ç½®å•åˆ—ä¸ºç´¢å¼•\n",
    "product_indexed = sales_data.set_index('äº§å“')\n",
    "print(f\"ä»¥äº§å“ä¸ºç´¢å¼•çš„æ•°æ®å½¢çŠ¶: {product_indexed.shape}\")\n",
    "\n",
    "# è®¾ç½®å¤šçº§ç´¢å¼•\n",
    "multi_indexed = sales_data.set_index(['åœ°åŒº', 'äº§å“'])\n",
    "print(f\"å¤šçº§ç´¢å¼•çš„å‰5è¡Œ:\\n{multi_indexed.head()}\")\n",
    "\n",
    "# é‡ç½®ç´¢å¼•\n",
    "reset_index_df = multi_indexed.reset_index()\n",
    "print(f\"é‡ç½®ç´¢å¼•åçš„åˆ—å: {reset_index_df.columns.tolist()}\")\n",
    "\n",
    "# ç´¢å¼•æ“ä½œç¤ºä¾‹\n",
    "print(\"\\n=== é«˜çº§ç´¢å¼•ç¤ºä¾‹ ===\")\n",
    "# é€è§†è¡¨é¢„è§ˆ\n",
    "pivot_preview = sales_data.groupby(['åœ°åŒº', 'äº§å“'])['é”€å”®é¢'].sum().unstack(fill_value=0)\n",
    "print(\"åœ°åŒº-äº§å“é”€å”®é¢é€è§†è¡¨:\")\n",
    "print(pivot_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73147fe5",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†\n",
    "\n",
    "æ•°æ®æ¸…æ´—æ˜¯æ•°æ®åˆ†æçš„é‡è¦æ­¥éª¤ï¼ŒåŒ…æ‹¬å¤„ç†ç¼ºå¤±å€¼ã€é‡å¤å€¼ã€å¼‚å¸¸å€¼ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŒ…å«é—®é¢˜çš„ç¤ºä¾‹æ•°æ®\n",
    "np.random.seed(42)\n",
    "dirty_data = pd.DataFrame({\n",
    "    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'å¼ ä¸‰', 'èµµå…­', None, 'é’±ä¸ƒ', 'å¼ ä¸‰'],\n",
    "    'å¹´é¾„': [25, 30, np.nan, 25, 28, 35, 32, 25],\n",
    "    'æ”¶å…¥': [5000, 8000, 12000, 5000, np.nan, 15000, 11000, 5000],\n",
    "    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', '  å¹¿å·  ', 'åŒ—äº¬', 'æ·±åœ³', 'æ­å·', '', 'åŒ—äº¬'],\n",
    "    'å­¦å†': ['æœ¬ç§‘', 'ç¡•å£«', 'æœ¬ç§‘', 'æœ¬ç§‘', 'åšå£«', 'ç¡•å£«', 'æœ¬ç§‘', 'æœ¬ç§‘'],\n",
    "    'é‚®ç®±': ['zhang@qq.com', 'li@163.com', 'wang@gmail.com', 'zhang@qq.com', \n",
    "             'zhao@sina.com', 'invalid-email', 'qian@126.com', 'zhang@qq.com']\n",
    "})\n",
    "\n",
    "print(\"åŸå§‹è„æ•°æ®:\")\n",
    "print(dirty_data)\n",
    "print(f\"\\næ•°æ®å½¢çŠ¶: {dirty_data.shape}\")\n",
    "\n",
    "print(\"\\n=== ç¼ºå¤±å€¼å¤„ç† ===\")\n",
    "# æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "print(\"å„åˆ—ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
    "print(dirty_data.isnull().sum())\n",
    "\n",
    "print(\"\\nç¼ºå¤±å€¼ä½ç½®:\")\n",
    "print(dirty_data.isnull())\n",
    "\n",
    "# ç¼ºå¤±å€¼å¯è§†åŒ–ç»Ÿè®¡\n",
    "missing_percentage = (dirty_data.isnull().sum() / len(dirty_data)) * 100\n",
    "print(f\"\\nå„åˆ—ç¼ºå¤±å€¼æ¯”ä¾‹:\")\n",
    "for col, pct in missing_percentage.items():\n",
    "    if pct > 0:\n",
    "        print(f\"{col}: {pct:.1f}%\")\n",
    "\n",
    "# å¤„ç†ç¼ºå¤±å€¼çš„ä¸åŒç­–ç•¥\n",
    "print(\"\\n=== ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ ===\")\n",
    "\n",
    "# ç­–ç•¥1ï¼šåˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ\n",
    "cleaned_drop_rows = dirty_data.dropna()\n",
    "print(f\"åˆ é™¤ç¼ºå¤±è¡Œåæ•°æ®å½¢çŠ¶: {cleaned_drop_rows.shape}\")\n",
    "\n",
    "# ç­–ç•¥2ï¼šåˆ é™¤å«ç¼ºå¤±å€¼çš„åˆ—\n",
    "cleaned_drop_cols = dirty_data.dropna(axis=1)\n",
    "print(f\"åˆ é™¤ç¼ºå¤±åˆ—åæ•°æ®å½¢çŠ¶: {cleaned_drop_cols.shape}\")\n",
    "\n",
    "# ç­–ç•¥3ï¼šå¡«å……ç¼ºå¤±å€¼\n",
    "cleaned_filled = dirty_data.copy()\n",
    "\n",
    "# ç”¨å¹³å‡å€¼å¡«å……æ•°å€¼åˆ—\n",
    "cleaned_filled['å¹´é¾„'].fillna(cleaned_filled['å¹´é¾„'].mean(), inplace=True)\n",
    "cleaned_filled['æ”¶å…¥'].fillna(cleaned_filled['æ”¶å…¥'].median(), inplace=True)\n",
    "\n",
    "# ç”¨ä¼—æ•°å¡«å……åˆ†ç±»åˆ—\n",
    "cleaned_filled['å§“å'].fillna('æœªçŸ¥', inplace=True)\n",
    "\n",
    "print(f\"å¡«å……åçš„ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
    "print(cleaned_filled.isnull().sum())\n",
    "\n",
    "# å‰å‘å¡«å……å’Œåå‘å¡«å……\n",
    "ff_data = dirty_data.copy()\n",
    "ff_data['å¹´é¾„'] = ff_data['å¹´é¾„'].fillna(method='ffill')  # å‰å‘å¡«å……\n",
    "bf_data = dirty_data.copy()\n",
    "bf_data['å¹´é¾„'] = bf_data['å¹´é¾„'].fillna(method='bfill')  # åå‘å¡«å……\n",
    "\n",
    "print(f\"\\nå‰å‘å¡«å……å¹´é¾„åˆ—: {ff_data['å¹´é¾„'].tolist()}\")\n",
    "print(f\"åå‘å¡«å……å¹´é¾„åˆ—: {bf_data['å¹´é¾„'].tolist()}\")\n",
    "\n",
    "print(\"\\n=== é‡å¤å€¼å¤„ç† ===\")\n",
    "# æ£€æŸ¥å®Œå…¨é‡å¤çš„è¡Œ\n",
    "duplicate_rows = dirty_data.duplicated()\n",
    "print(f\"å®Œå…¨é‡å¤çš„è¡Œæ•°: {duplicate_rows.sum()}\")\n",
    "print(f\"é‡å¤è¡Œç´¢å¼•: {dirty_data[duplicate_rows].index.tolist()}\")\n",
    "\n",
    "# åŸºäºç‰¹å®šåˆ—æ£€æŸ¥é‡å¤\n",
    "name_duplicates = dirty_data.duplicated(subset=['å§“å'])\n",
    "print(f\"å§“åé‡å¤çš„è¡Œæ•°: {name_duplicates.sum()}\")\n",
    "\n",
    "# æŸ¥çœ‹é‡å¤çš„å…·ä½“å†…å®¹\n",
    "print(f\"\\né‡å¤çš„å§“åè®°å½•:\")\n",
    "duplicated_names = dirty_data[dirty_data.duplicated(subset=['å§“å'], keep=False)]\n",
    "print(duplicated_names.sort_values('å§“å'))\n",
    "\n",
    "# å»é™¤é‡å¤å€¼\n",
    "no_duplicates = dirty_data.drop_duplicates()\n",
    "print(f\"\\nå»é™¤å®Œå…¨é‡å¤åçš„æ•°æ®å½¢çŠ¶: {no_duplicates.shape}\")\n",
    "\n",
    "# åŸºäºç‰¹å®šåˆ—å»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª\n",
    "no_name_duplicates = dirty_data.drop_duplicates(subset=['å§“å'], keep='first')\n",
    "print(f\"å»é™¤å§“åé‡å¤åçš„æ•°æ®å½¢çŠ¶: {no_name_duplicates.shape}\")\n",
    "\n",
    "print(\"\\n=== æ•°æ®ç±»å‹è½¬æ¢ ===\")\n",
    "# æŸ¥çœ‹å½“å‰æ•°æ®ç±»å‹\n",
    "print(\"å½“å‰æ•°æ®ç±»å‹:\")\n",
    "print(cleaned_filled.dtypes)\n",
    "\n",
    "# ç±»å‹è½¬æ¢\n",
    "type_converted = cleaned_filled.copy()\n",
    "\n",
    "# æ•°å€¼ç±»å‹è½¬æ¢\n",
    "type_converted['å¹´é¾„'] = type_converted['å¹´é¾„'].astype(int)\n",
    "type_converted['æ”¶å…¥'] = type_converted['æ”¶å…¥'].astype(float)\n",
    "\n",
    "# åˆ†ç±»ç±»å‹è½¬æ¢\n",
    "type_converted['å­¦å†'] = type_converted['å­¦å†'].astype('category')\n",
    "type_converted['åŸå¸‚'] = type_converted['åŸå¸‚'].astype('category')\n",
    "\n",
    "print(f\"\\nè½¬æ¢åçš„æ•°æ®ç±»å‹:\")\n",
    "print(type_converted.dtypes)\n",
    "\n",
    "# æŸ¥çœ‹åˆ†ç±»æ•°æ®çš„ç±»åˆ«\n",
    "print(f\"\\nå­¦å†ç±»åˆ«: {type_converted['å­¦å†'].cat.categories.tolist()}\")\n",
    "print(f\"åŸå¸‚ç±»åˆ«: {type_converted['åŸå¸‚'].cat.categories.tolist()}\")\n",
    "\n",
    "print(\"\\n=== å­—ç¬¦ä¸²å¤„ç† ===\")\n",
    "# å­—ç¬¦ä¸²æ¸…æ´—\n",
    "string_cleaned = dirty_data.copy()\n",
    "\n",
    "# å»é™¤å­—ç¬¦ä¸²å‰åç©ºæ ¼\n",
    "string_cleaned['åŸå¸‚'] = string_cleaned['åŸå¸‚'].str.strip()\n",
    "\n",
    "# æ›¿æ¢ç©ºå­—ç¬¦ä¸²ä¸ºNaN\n",
    "string_cleaned['åŸå¸‚'] = string_cleaned['åŸå¸‚'].replace('', np.nan)\n",
    "\n",
    "# å­—ç¬¦ä¸²å¤§å°å†™è½¬æ¢\n",
    "string_cleaned['é‚®ç®±_lower'] = string_cleaned['é‚®ç®±'].str.lower()\n",
    "\n",
    "# æå–é‚®ç®±åŸŸå\n",
    "string_cleaned['é‚®ç®±åŸŸå'] = string_cleaned['é‚®ç®±'].str.extract(r'@(.+)')\n",
    "\n",
    "print(\"å­—ç¬¦ä¸²å¤„ç†åçš„ç»“æœ:\")\n",
    "print(string_cleaned[['åŸå¸‚', 'é‚®ç®±', 'é‚®ç®±_lower', 'é‚®ç®±åŸŸå']].head())\n",
    "\n",
    "# å­—ç¬¦ä¸²åŒ…å«åˆ¤æ–­\n",
    "gmail_users = string_cleaned['é‚®ç®±'].str.contains('gmail', na=False)\n",
    "print(f\"\\nGmailç”¨æˆ·æ•°é‡: {gmail_users.sum()}\")\n",
    "\n",
    "print(\"\\n=== å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç† ===\")\n",
    "# åˆ›å»ºåŒ…å«å¼‚å¸¸å€¼çš„æ•°æ®\n",
    "np.random.seed(42)\n",
    "outlier_data = pd.DataFrame({\n",
    "    'æ”¶å…¥': np.concatenate([\n",
    "        np.random.normal(8000, 2000, 95),  # æ­£å¸¸æ”¶å…¥\n",
    "        [50000, 80000, 100000, 120000, 200000]  # å¼‚å¸¸å€¼\n",
    "    ])\n",
    "})\n",
    "\n",
    "print(f\"æ”¶å…¥æ•°æ®ç»Ÿè®¡:\")\n",
    "print(outlier_data['æ”¶å…¥'].describe())\n",
    "\n",
    "# ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼\n",
    "Q1 = outlier_data['æ”¶å…¥'].quantile(0.25)\n",
    "Q3 = outlier_data['æ”¶å…¥'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nå¼‚å¸¸å€¼è¾¹ç•Œ: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "# è¯†åˆ«å¼‚å¸¸å€¼\n",
    "outliers = (outlier_data['æ”¶å…¥'] < lower_bound) | (outlier_data['æ”¶å…¥'] > upper_bound)\n",
    "print(f\"å¼‚å¸¸å€¼æ•°é‡: {outliers.sum()}\")\n",
    "print(f\"å¼‚å¸¸å€¼: {outlier_data[outliers]['æ”¶å…¥'].tolist()}\")\n",
    "\n",
    "# å¤„ç†å¼‚å¸¸å€¼ï¼šé™åˆ¶åœ¨åˆç†èŒƒå›´å†…\n",
    "outlier_data['æ”¶å…¥_capped'] = outlier_data['æ”¶å…¥'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(f\"\\nå¤„ç†åçš„æ”¶å…¥èŒƒå›´: {outlier_data['æ”¶å…¥_capped'].min():.2f} - {outlier_data['æ”¶å…¥_capped'].max():.2f}\")\n",
    "\n",
    "print(\"\\n=== æ•°æ®éªŒè¯ ===\")\n",
    "# åˆ›å»ºæ•°æ®éªŒè¯è§„åˆ™\n",
    "validation_data = cleaned_filled.copy()\n",
    "\n",
    "# å¹´é¾„éªŒè¯\n",
    "valid_age = (validation_data['å¹´é¾„'] >= 18) & (validation_data['å¹´é¾„'] <= 65)\n",
    "print(f\"æœ‰æ•ˆå¹´é¾„è®°å½•æ•°: {valid_age.sum()}/{len(validation_data)}\")\n",
    "\n",
    "# é‚®ç®±æ ¼å¼éªŒè¯\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "valid_email = validation_data['é‚®ç®±'].str.match(email_pattern, na=False)\n",
    "print(f\"æœ‰æ•ˆé‚®ç®±è®°å½•æ•°: {valid_email.sum()}/{len(validation_data)}\")\n",
    "\n",
    "# æ”¶å…¥åˆç†æ€§éªŒè¯\n",
    "valid_income = (validation_data['æ”¶å…¥'] >= 1000) & (validation_data['æ”¶å…¥'] <= 50000)\n",
    "print(f\"åˆç†æ”¶å…¥è®°å½•æ•°: {valid_income.sum()}/{len(validation_data)}\")\n",
    "\n",
    "# åˆ›å»ºç»¼åˆéªŒè¯ç»“æœ\n",
    "validation_data['æ•°æ®æœ‰æ•ˆ'] = valid_age & valid_email & valid_income\n",
    "print(f\"\\nå®Œå…¨æœ‰æ•ˆçš„è®°å½•æ•°: {validation_data['æ•°æ®æœ‰æ•ˆ'].sum()}/{len(validation_data)}\")\n",
    "\n",
    "print(\"\\n=== æ•°æ®æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ– ===\")\n",
    "# åˆ›å»ºæ•°å€¼æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–\n",
    "numerical_data = pd.DataFrame({\n",
    "    'èº«é«˜': np.random.normal(170, 10, 100),\n",
    "    'ä½“é‡': np.random.normal(65, 15, 100),\n",
    "    'å¹´é¾„': np.random.randint(18, 60, 100)\n",
    "})\n",
    "\n",
    "print(\"åŸå§‹æ•°æ®ç»Ÿè®¡:\")\n",
    "print(numerical_data.describe())\n",
    "\n",
    "# Z-scoreæ ‡å‡†åŒ–\n",
    "from scipy import stats\n",
    "standardized_data = numerical_data.copy()\n",
    "for col in numerical_data.columns:\n",
    "    standardized_data[col + '_æ ‡å‡†åŒ–'] = stats.zscore(numerical_data[col])\n",
    "\n",
    "print(f\"\\næ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡:\")\n",
    "print(standardized_data[['èº«é«˜_æ ‡å‡†åŒ–', 'ä½“é‡_æ ‡å‡†åŒ–', 'å¹´é¾„_æ ‡å‡†åŒ–']].describe())\n",
    "\n",
    "# Min-Maxå½’ä¸€åŒ–\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = pd.DataFrame(\n",
    "    scaler.fit_transform(numerical_data),\n",
    "    columns=[col + '_å½’ä¸€åŒ–' for col in numerical_data.columns]\n",
    ")\n",
    "\n",
    "print(f\"\\nå½’ä¸€åŒ–åçš„æ•°æ®ç»Ÿè®¡:\")\n",
    "print(normalized_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ded170",
   "metadata": {},
   "source": [
    "## 5. åˆ†ç»„ä¸èšåˆæ“ä½œ (GroupBy)\n",
    "\n",
    "åˆ†ç»„æ“ä½œæ˜¯pandasä¸­çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå…è®¸æˆ‘ä»¬å°†æ•°æ®æŒ‰æŸäº›æ¡ä»¶åˆ†ç»„ï¼Œç„¶åå¯¹æ¯ç»„æ•°æ®è¿›è¡Œèšåˆè®¡ç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºé”€å”®æ•°æ®ç¤ºä¾‹\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sales_data = pd.DataFrame({\n",
    "    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'] * 3,\n",
    "    'product': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'] * 3,\n",
    "    'salesperson': ['Alice', 'Bob', 'Charlie', 'David'] * 6,\n",
    "    'month': ['Jan', 'Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Feb', 'Mar', 'Mar', 'Mar', 'Mar'] * 2,\n",
    "    'sales': np.random.randint(100, 1000, 24),\n",
    "    'units': np.random.randint(10, 100, 24)\n",
    "})\n",
    "\n",
    "print(\"é”€å”®æ•°æ®æ ·æœ¬:\")\n",
    "print(sales_data.head(10))\n",
    "print(f\"\\næ•°æ®å½¢çŠ¶: {sales_data.shape}\")\n",
    "\n",
    "# åŸºæœ¬åˆ†ç»„æ“ä½œ\n",
    "print(\"\\n=== åŸºæœ¬åˆ†ç»„æ“ä½œ ===\")\n",
    "\n",
    "# æŒ‰åœ°åŒºåˆ†ç»„ï¼Œè®¡ç®—æ€»é”€å”®é¢\n",
    "region_sales = sales_data.groupby('region')['sales'].sum()\n",
    "print(\"\\næŒ‰åœ°åŒºåˆ†ç»„çš„æ€»é”€å”®é¢:\")\n",
    "print(region_sales)\n",
    "\n",
    "# æŒ‰äº§å“åˆ†ç»„ï¼Œè®¡ç®—å¹³å‡é”€å”®é¢å’Œæ€»é”€é‡\n",
    "product_stats = sales_data.groupby('product').agg({\n",
    "    'sales': ['sum', 'mean', 'count'],\n",
    "    'units': 'sum'\n",
    "})\n",
    "print(\"\\næŒ‰äº§å“åˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "print(product_stats)\n",
    "\n",
    "# å¤šåˆ—åˆ†ç»„\n",
    "region_product_sales = sales_data.groupby(['region', 'product'])['sales'].sum()\n",
    "print(\"\\næŒ‰åœ°åŒºå’Œäº§å“åˆ†ç»„çš„é”€å”®é¢:\")\n",
    "print(region_product_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc747922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜çº§åˆ†ç»„æ“ä½œ\n",
    "print(\"=== é«˜çº§åˆ†ç»„æ“ä½œ ===\")\n",
    "\n",
    "# ä½¿ç”¨transformè¿›è¡Œç»„å†…æ ‡å‡†åŒ–\n",
    "sales_data['sales_zscore'] = sales_data.groupby('region')['sales'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "print(\"\\næ·»åŠ ç»„å†…æ ‡å‡†åŒ–åçš„æ•°æ®:\")\n",
    "print(sales_data[['region', 'sales', 'sales_zscore']].head())\n",
    "\n",
    "# ä½¿ç”¨applyè¿›è¡Œå¤æ‚è®¡ç®—\n",
    "def region_analysis(group):\n",
    "    return pd.Series({\n",
    "        'total_sales': group['sales'].sum(),\n",
    "        'avg_sales': group['sales'].mean(),\n",
    "        'top_performer': group.loc[group['sales'].idxmax(), 'salesperson'],\n",
    "        'sales_range': group['sales'].max() - group['sales'].min(),\n",
    "        'coefficient_of_variation': group['sales'].std() / group['sales'].mean()\n",
    "    })\n",
    "\n",
    "region_detailed = sales_data.groupby('region').apply(region_analysis)\n",
    "print(\"\\nåœ°åŒºè¯¦ç»†åˆ†æ:\")\n",
    "print(region_detailed)\n",
    "\n",
    "# åˆ†ä½æ•°å’Œç™¾åˆ†ä½æ•°\n",
    "print(\"\\nå„åœ°åŒºé”€å”®é¢åˆ†ä½æ•°:\")\n",
    "region_quantiles = sales_data.groupby('region')['sales'].quantile([0.25, 0.5, 0.75]).unstack()\n",
    "print(region_quantiles)\n",
    "\n",
    "# è‡ªå®šä¹‰èšåˆå‡½æ•°\n",
    "def sales_metrics(series):\n",
    "    return pd.Series({\n",
    "        'total': series.sum(),\n",
    "        'average': series.mean(),\n",
    "        'volatility': series.std(),\n",
    "        'peak_to_trough': series.max() - series.min(),\n",
    "        'above_avg_count': (series > series.mean()).sum()\n",
    "    })\n",
    "\n",
    "custom_agg = sales_data.groupby('product')['sales'].apply(sales_metrics)\n",
    "print(\"\\näº§å“é”€å”®æŒ‡æ ‡:\")\n",
    "print(custom_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de47c9",
   "metadata": {},
   "source": [
    "## 6. æ•°æ®åˆå¹¶ä¸è¿æ¥\n",
    "\n",
    "æ•°æ®åˆå¹¶æ˜¯æ•°æ®åˆ†æä¸­çš„é‡è¦æ“ä½œï¼Œpandasæä¾›äº†å¤šç§åˆå¹¶æ•°æ®çš„æ–¹æ³•ï¼šjoinã€mergeã€concatç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b689aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®é›†\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "    'age': [25, 30, 35, 28, 32]\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106],\n",
    "    'customer_id': [1, 2, 2, 3, 6, 7],  # æ³¨æ„ï¼š6å’Œ7ä¸åœ¨customersè¡¨ä¸­\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse'],\n",
    "    'amount': [1200, 800, 500, 300, 50, 25],\n",
    "    'order_date': pd.to_datetime(['2023-01-15', '2023-01-16', '2023-01-17', \n",
    "                                  '2023-01-18', '2023-01-19', '2023-01-20'])\n",
    "})\n",
    "\n",
    "product_info = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Speaker'],\n",
    "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Audio'],\n",
    "    'cost': [800, 500, 300, 200, 100]\n",
    "})\n",
    "\n",
    "print(\"å®¢æˆ·æ•°æ®:\")\n",
    "print(customers)\n",
    "print(\"\\nè®¢å•æ•°æ®:\")\n",
    "print(orders)\n",
    "print(\"\\näº§å“ä¿¡æ¯:\")\n",
    "print(product_info)\n",
    "\n",
    "print(\"\\n=== ä¸åŒç±»å‹çš„åˆå¹¶ ===\")\n",
    "\n",
    "# å†…è¿æ¥ (Inner Join) - åªä¿ç•™ä¸¤ä¸ªè¡¨éƒ½æœ‰çš„è®°å½•\n",
    "inner_join = pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "print(\"\\nå†…è¿æ¥ (Inner Join):\")\n",
    "print(inner_join)\n",
    "\n",
    "# å·¦è¿æ¥ (Left Join) - ä¿ç•™å·¦è¡¨æ‰€æœ‰è®°å½•\n",
    "left_join = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "print(\"\\nå·¦è¿æ¥ (Left Join):\")\n",
    "print(left_join)\n",
    "\n",
    "# å³è¿æ¥ (Right Join) - ä¿ç•™å³è¡¨æ‰€æœ‰è®°å½•\n",
    "right_join = pd.merge(customers, orders, on='customer_id', how='right')\n",
    "print(\"\\nå³è¿æ¥ (Right Join):\")\n",
    "print(right_join)\n",
    "\n",
    "# å¤–è¿æ¥ (Outer Join) - ä¿ç•™ä¸¤ä¸ªè¡¨çš„æ‰€æœ‰è®°å½•\n",
    "outer_join = pd.merge(customers, orders, on='customer_id', how='outer')\n",
    "print(\"\\nå¤–è¿æ¥ (Outer Join):\")\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ace73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè¡¨è¿æ¥\n",
    "print(\"=== å¤šè¡¨è¿æ¥ ===\")\n",
    "\n",
    "# ä¸‰è¡¨è¿æ¥ï¼šå®¢æˆ· -> è®¢å• -> äº§å“ä¿¡æ¯\n",
    "multi_join = (customers\n",
    "              .merge(orders, on='customer_id', how='inner')\n",
    "              .merge(product_info, on='product', how='left'))\n",
    "\n",
    "print(\"\\nä¸‰è¡¨è¿æ¥ç»“æœ:\")\n",
    "print(multi_join)\n",
    "\n",
    "# è®¡ç®—åˆ©æ¶¦\n",
    "multi_join['profit'] = multi_join['amount'] - multi_join['cost']\n",
    "print(\"\\næ·»åŠ åˆ©æ¶¦è®¡ç®—:\")\n",
    "print(multi_join[['name', 'product', 'amount', 'cost', 'profit']])\n",
    "\n",
    "print(\"\\n=== ä¸åŒè¿æ¥é”®çš„åˆå¹¶ ===\")\n",
    "\n",
    "# ä¸åŒåˆ—åçš„è¿æ¥\n",
    "customers_alt = customers.rename(columns={'customer_id': 'cust_id'})\n",
    "diff_key_merge = pd.merge(customers_alt, orders, \n",
    "                         left_on='cust_id', right_on='customer_id', how='inner')\n",
    "print(\"\\nä¸åŒåˆ—åè¿æ¥:\")\n",
    "print(diff_key_merge[['cust_id', 'name', 'order_id', 'product']])\n",
    "\n",
    "# åŸºäºç´¢å¼•çš„è¿æ¥\n",
    "customers_indexed = customers.set_index('customer_id')\n",
    "orders_indexed = orders.set_index('customer_id')\n",
    "index_join = customers_indexed.join(orders_indexed, how='inner', rsuffix='_order')\n",
    "print(\"\\nåŸºäºç´¢å¼•çš„è¿æ¥:\")\n",
    "print(index_join)\n",
    "\n",
    "print(\"\\n=== ä½¿ç”¨concatè¿›è¡Œæ•°æ®è¿æ¥ ===\")\n",
    "\n",
    "# åˆ›å»ºé¢å¤–æ•°æ®ç”¨äºæ¼”ç¤ºconcat\n",
    "customers_2023_q1 = pd.DataFrame({\n",
    "    'customer_id': [6, 7, 8],\n",
    "    'name': ['Frank', 'Grace', 'Henry'],\n",
    "    'city': ['Seattle', 'Boston', 'Miami'],\n",
    "    'age': [29, 31, 27]\n",
    "})\n",
    "\n",
    "customers_2023_q2 = pd.DataFrame({\n",
    "    'customer_id': [9, 10, 11],\n",
    "    'name': ['Ivy', 'Jack', 'Kate'],\n",
    "    'city': ['Denver', 'Portland', 'Atlanta'],\n",
    "    'age': [26, 33, 30]\n",
    "})\n",
    "\n",
    "# å‚ç›´è¿æ¥ (è¡Œæ‹¼æ¥)\n",
    "all_customers = pd.concat([customers, customers_2023_q1, customers_2023_q2], \n",
    "                         ignore_index=True)\n",
    "print(\"\\nå‚ç›´è¿æ¥ (è¡Œæ‹¼æ¥):\")\n",
    "print(all_customers)\n",
    "\n",
    "# æ°´å¹³è¿æ¥ (åˆ—æ‹¼æ¥)\n",
    "customer_details = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', \n",
    "              'david@email.com', 'eve@email.com'],\n",
    "    'phone': ['123-456-7890', '234-567-8901', '345-678-9012', \n",
    "              '456-789-0123', '567-890-1234']\n",
    "})\n",
    "\n",
    "# åŸºäºç´¢å¼•çš„æ°´å¹³è¿æ¥\n",
    "customers_extended = pd.concat([customers.set_index('customer_id'), \n",
    "                               customer_details.set_index('customer_id')], \n",
    "                              axis=1)\n",
    "print(\"\\næ°´å¹³è¿æ¥ (åˆ—æ‹¼æ¥):\")\n",
    "print(customers_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de417f82",
   "metadata": {},
   "source": [
    "## 7. æ—¶é—´åºåˆ—å¤„ç†\n",
    "\n",
    "æ—¶é—´åºåˆ—æ•°æ®åœ¨é‡‘èã€ä¸šåŠ¡åˆ†æç­‰é¢†åŸŸéå¸¸å¸¸è§ã€‚pandasæä¾›äº†å¼ºå¤§çš„æ—¶é—´åºåˆ—å¤„ç†åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ç”Ÿæˆæ—¥æœŸèŒƒå›´\n",
    "date_range = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "print(f\"æ—¥æœŸèŒƒå›´: {date_range[:5]}...{date_range[-3:]}\")\n",
    "print(f\"æ€»å¤©æ•°: {len(date_range)}\")\n",
    "\n",
    "# åˆ›å»ºæ—¶é—´åºåˆ—DataFrame\n",
    "np.random.seed(42)\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': date_range,\n",
    "    'temperature': 20 + 10 * np.sin(2 * np.pi * np.arange(len(date_range)) / 365.25) + np.random.normal(0, 2, len(date_range)),\n",
    "    'humidity': 50 + 20 * np.sin(2 * np.pi * np.arange(len(date_range)) / 365.25 + np.pi/4) + np.random.normal(0, 5, len(date_range)),\n",
    "    'precipitation': np.random.exponential(2, len(date_range))\n",
    "})\n",
    "\n",
    "# è®¾ç½®æ—¥æœŸä¸ºç´¢å¼•\n",
    "ts_data.set_index('date', inplace=True)\n",
    "print(\"\\næ—¶é—´åºåˆ—æ•°æ®æ ·æœ¬:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "print(\"\\n=== åŸºæœ¬æ—¶é—´åºåˆ—æ“ä½œ ===\")\n",
    "\n",
    "# æ—¥æœŸæ—¶é—´å±æ€§æå–\n",
    "ts_data['year'] = ts_data.index.year\n",
    "ts_data['month'] = ts_data.index.month\n",
    "ts_data['day_of_week'] = ts_data.index.dayofweek\n",
    "ts_data['day_name'] = ts_data.index.day_name()\n",
    "ts_data['is_weekend'] = ts_data.index.dayofweek >= 5\n",
    "\n",
    "print(\"\\næ·»åŠ æ—¥æœŸå±æ€§å:\")\n",
    "print(ts_data[['temperature', 'year', 'month', 'day_of_week', 'day_name', 'is_weekend']].head())\n",
    "\n",
    "# æ—¶é—´èŒƒå›´é€‰æ‹©\n",
    "print(\"\\n2023å¹´3æœˆçš„æ•°æ®:\")\n",
    "march_data = ts_data['2023-03']\n",
    "print(march_data.head())\n",
    "\n",
    "print(f\"\\n3æœˆå¹³å‡æ¸©åº¦: {march_data['temperature'].mean():.2f}Â°C\")\n",
    "\n",
    "# å­£åº¦é‡é‡‡æ ·\n",
    "quarterly_avg = ts_data.resample('Q').mean()\n",
    "print(\"\\nå­£åº¦å¹³å‡å€¼:\")\n",
    "print(quarterly_avg)\n",
    "\n",
    "# æœˆåº¦ç»Ÿè®¡\n",
    "monthly_stats = ts_data.resample('M').agg({\n",
    "    'temperature': ['mean', 'min', 'max'],\n",
    "    'humidity': 'mean',\n",
    "    'precipitation': 'sum'\n",
    "})\n",
    "print(\"\\næœˆåº¦ç»Ÿè®¡:\")\n",
    "print(monthly_stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ»‘åŠ¨çª—å£åˆ†æ\n",
    "print(\"=== æ»‘åŠ¨çª—å£åˆ†æ ===\")\n",
    "\n",
    "# 7å¤©ç§»åŠ¨å¹³å‡\n",
    "ts_data['temp_7day_ma'] = ts_data['temperature'].rolling(window=7).mean()\n",
    "ts_data['temp_30day_ma'] = ts_data['temperature'].rolling(window=30).mean()\n",
    "\n",
    "print(\"\\nç§»åŠ¨å¹³å‡:\")\n",
    "print(ts_data[['temperature', 'temp_7day_ma', 'temp_30day_ma']].head(10))\n",
    "\n",
    "# æ»‘åŠ¨çª—å£ç»Ÿè®¡\n",
    "rolling_stats = ts_data['temperature'].rolling(window=30).agg(['mean', 'std', 'min', 'max'])\n",
    "rolling_stats.columns = ['30day_mean', '30day_std', '30day_min', '30day_max']\n",
    "ts_data = pd.concat([ts_data, rolling_stats], axis=1)\n",
    "\n",
    "print(\"\\n30å¤©æ»‘åŠ¨çª—å£ç»Ÿè®¡:\")\n",
    "print(ts_data[['temperature', '30day_mean', '30day_std', '30day_min', '30day_max']].dropna().head())\n",
    "\n",
    "# æŒ‡æ•°ç§»åŠ¨å¹³å‡\n",
    "ts_data['temp_ema'] = ts_data['temperature'].ewm(span=14).mean()\n",
    "print(\"\\næŒ‡æ•°ç§»åŠ¨å¹³å‡ vs ç®€å•ç§»åŠ¨å¹³å‡:\")\n",
    "comparison = ts_data[['temperature', 'temp_7day_ma', 'temp_ema']].dropna()\n",
    "print(comparison.head())\n",
    "\n",
    "print(\"\\n=== æ—¶é—´åºåˆ—åˆ†æ ===\")\n",
    "\n",
    "# å­£èŠ‚æ€§åˆ†è§£ (ç®€å•ç‰ˆæœ¬)\n",
    "ts_data['month_avg'] = ts_data.groupby('month')['temperature'].transform('mean')\n",
    "ts_data['seasonal'] = ts_data['temperature'] - ts_data['month_avg']\n",
    "ts_data['trend'] = ts_data['temperature'].rolling(window=30, center=True).mean()\n",
    "ts_data['residual'] = ts_data['temperature'] - ts_data['trend'] - ts_data['seasonal']\n",
    "\n",
    "print(\"\\nå­£èŠ‚æ€§åˆ†è§£:\")\n",
    "decomposition = ts_data[['temperature', 'trend', 'seasonal', 'residual']].dropna()\n",
    "print(decomposition.head())\n",
    "\n",
    "# æ»åå˜é‡å’Œå·®åˆ†\n",
    "ts_data['temp_lag1'] = ts_data['temperature'].shift(1)\n",
    "ts_data['temp_lag7'] = ts_data['temperature'].shift(7)\n",
    "ts_data['temp_diff1'] = ts_data['temperature'].diff()\n",
    "ts_data['temp_diff7'] = ts_data['temperature'].diff(7)\n",
    "\n",
    "print(\"\\næ»åå’Œå·®åˆ†å˜é‡:\")\n",
    "lag_diff = ts_data[['temperature', 'temp_lag1', 'temp_lag7', 'temp_diff1', 'temp_diff7']].dropna()\n",
    "print(lag_diff.head())\n",
    "\n",
    "# ç›¸å…³æ€§åˆ†æ\n",
    "correlation_matrix = ts_data[['temperature', 'humidity', 'precipitation']].corr()\n",
    "print(\"\\næ°”è±¡å˜é‡ç›¸å…³æ€§:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# å¼‚å¸¸å€¼æ£€æµ‹ (åŸºäºæ ‡å‡†å·®)\n",
    "ts_data['temp_zscore'] = (ts_data['temperature'] - ts_data['temperature'].mean()) / ts_data['temperature'].std()\n",
    "outliers = ts_data[abs(ts_data['temp_zscore']) > 2.5]\n",
    "print(f\"\\nå¼‚å¸¸å€¼æ•°é‡ (|z-score| > 2.5): {len(outliers)}\")\n",
    "if len(outliers) > 0:\n",
    "    print(\"å¼‚å¸¸å€¼ç¤ºä¾‹:\")\n",
    "    print(outliers[['temperature', 'temp_zscore']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460c0d9",
   "metadata": {},
   "source": [
    "## 8. æ•°æ®é€è§†è¡¨ä¸äº¤å‰è¡¨\n",
    "\n",
    "æ•°æ®é€è§†è¡¨æ˜¯æ•°æ®åˆ†æä¸­å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å¿«é€Ÿæ±‡æ€»å’Œé‡ç»„æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ‰©å±•çš„é”€å”®æ•°æ®ç”¨äºé€è§†è¡¨æ¼”ç¤º\n",
    "np.random.seed(42)\n",
    "extended_sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', '2023-12-31', freq='D'),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 365),\n",
    "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard'], 365),\n",
    "    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'David', 'Eve'], 365),\n",
    "    'customer_type': np.random.choice(['Individual', 'Business'], 365),\n",
    "    'sales_amount': np.random.randint(100, 2000, 365),\n",
    "    'units_sold': np.random.randint(1, 20, 365)\n",
    "})\n",
    "\n",
    "extended_sales['month'] = extended_sales['date'].dt.month\n",
    "extended_sales['quarter'] = extended_sales['date'].dt.quarter\n",
    "extended_sales['day_of_week'] = extended_sales['date'].dt.day_name()\n",
    "\n",
    "print(\"æ‰©å±•é”€å”®æ•°æ®æ ·æœ¬:\")\n",
    "print(extended_sales.head())\n",
    "\n",
    "print(\"\\n=== åŸºç¡€é€è§†è¡¨ ===\")\n",
    "\n",
    "# ç®€å•é€è§†è¡¨ï¼šæŒ‰åœ°åŒºå’Œäº§å“æ±‡æ€»é”€å”®é¢\n",
    "pivot_basic = pd.pivot_table(extended_sales, \n",
    "                            values='sales_amount', \n",
    "                            index='region', \n",
    "                            columns='product', \n",
    "                            aggfunc='sum')\n",
    "print(\"\\næŒ‰åœ°åŒºå’Œäº§å“çš„é”€å”®é¢é€è§†è¡¨:\")\n",
    "print(pivot_basic)\n",
    "\n",
    "# æ·»åŠ æ€»è®¡\n",
    "pivot_with_totals = pd.pivot_table(extended_sales, \n",
    "                                  values='sales_amount', \n",
    "                                  index='region', \n",
    "                                  columns='product', \n",
    "                                  aggfunc='sum',\n",
    "                                  margins=True,\n",
    "                                  margins_name='æ€»è®¡')\n",
    "print(\"\\nå¸¦æ€»è®¡çš„é€è§†è¡¨:\")\n",
    "print(pivot_with_totals)\n",
    "\n",
    "print(\"\\n=== å¤šå€¼é€è§†è¡¨ ===\")\n",
    "\n",
    "# å¤šä¸ªèšåˆå€¼\n",
    "multi_value_pivot = pd.pivot_table(extended_sales,\n",
    "                                  values=['sales_amount', 'units_sold'],\n",
    "                                  index='region',\n",
    "                                  columns='customer_type',\n",
    "                                  aggfunc={'sales_amount': 'sum', 'units_sold': 'sum'})\n",
    "print(\"\\nå¤šå€¼é€è§†è¡¨ (é”€å”®é¢å’Œé”€é‡):\")\n",
    "print(multi_value_pivot)\n",
    "\n",
    "print(\"\\n=== å¤šçº§ç´¢å¼•é€è§†è¡¨ ===\")\n",
    "\n",
    "# å¤šçº§è¡Œå’Œåˆ—ç´¢å¼•\n",
    "multi_level_pivot = pd.pivot_table(extended_sales,\n",
    "                                  values='sales_amount',\n",
    "                                  index=['region', 'salesperson'],\n",
    "                                  columns=['quarter', 'customer_type'],\n",
    "                                  aggfunc='mean')\n",
    "print(\"\\nå¤šçº§ç´¢å¼•é€è§†è¡¨:\")\n",
    "print(multi_level_pivot.head(10))\n",
    "\n",
    "print(\"\\n=== ä¸åŒèšåˆå‡½æ•° ===\")\n",
    "\n",
    "# ä½¿ç”¨ä¸åŒçš„èšåˆå‡½æ•°\n",
    "agg_functions_pivot = pd.pivot_table(extended_sales,\n",
    "                                    values='sales_amount',\n",
    "                                    index='product',\n",
    "                                    columns='quarter',\n",
    "                                    aggfunc=['sum', 'mean', 'count', 'std'])\n",
    "print(\"\\nå¤šç§èšåˆå‡½æ•°é€è§†è¡¨:\")\n",
    "print(agg_functions_pivot)\n",
    "\n",
    "# è‡ªå®šä¹‰èšåˆå‡½æ•°\n",
    "def sales_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "custom_agg_pivot = pd.pivot_table(extended_sales,\n",
    "                                 values='sales_amount',\n",
    "                                 index='region',\n",
    "                                 columns='product',\n",
    "                                 aggfunc=[np.mean, sales_range])\n",
    "print(\"\\nè‡ªå®šä¹‰èšåˆå‡½æ•°é€è§†è¡¨:\")\n",
    "print(custom_agg_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93354e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº¤å‰è¡¨åˆ†æ\n",
    "print(\"=== äº¤å‰è¡¨ ===\")\n",
    "\n",
    "# ç®€å•äº¤å‰è¡¨ï¼šåœ°åŒº vs å®¢æˆ·ç±»å‹\n",
    "crosstab_basic = pd.crosstab(extended_sales['region'], \n",
    "                           extended_sales['customer_type'])\n",
    "print(\"\\nåœ°åŒº vs å®¢æˆ·ç±»å‹äº¤å‰è¡¨:\")\n",
    "print(crosstab_basic)\n",
    "\n",
    "# å¸¦æ¯”ä¾‹çš„äº¤å‰è¡¨\n",
    "crosstab_prop = pd.crosstab(extended_sales['region'], \n",
    "                          extended_sales['customer_type'], \n",
    "                          normalize='index')  # æŒ‰è¡Œæ ‡å‡†åŒ–\n",
    "print(\"\\nåœ°åŒº vs å®¢æˆ·ç±»å‹æ¯”ä¾‹äº¤å‰è¡¨:\")\n",
    "print(crosstab_prop)\n",
    "\n",
    "# ä¸‰ç»´äº¤å‰è¡¨\n",
    "crosstab_3d = pd.crosstab([extended_sales['region'], extended_sales['quarter']], \n",
    "                         extended_sales['customer_type'],\n",
    "                         margins=True)\n",
    "print(\"\\nä¸‰ç»´äº¤å‰è¡¨ (åœ°åŒº+å­£åº¦ vs å®¢æˆ·ç±»å‹):\")\n",
    "print(crosstab_3d)\n",
    "\n",
    "# å¸¦å€¼çš„äº¤å‰è¡¨\n",
    "crosstab_values = pd.crosstab(extended_sales['region'], \n",
    "                            extended_sales['product'],\n",
    "                            values=extended_sales['sales_amount'],\n",
    "                            aggfunc='mean')\n",
    "print(\"\\nå¸¦å¹³å‡é”€å”®é¢çš„äº¤å‰è¡¨:\")\n",
    "print(crosstab_values)\n",
    "\n",
    "print(\"\\n=== æ•°æ®é‡å¡‘ ===\")\n",
    "\n",
    "# åˆ›å»ºå®½æ ¼å¼æ•°æ®\n",
    "sales_monthly = extended_sales.groupby(['region', 'month'])['sales_amount'].sum().reset_index()\n",
    "print(\"\\næœˆåº¦é”€å”®æ•°æ®:\")\n",
    "print(sales_monthly.head(10))\n",
    "\n",
    "# é•¿æ ¼å¼è½¬å®½æ ¼å¼ (pivot)\n",
    "wide_format = sales_monthly.pivot(index='region', columns='month', values='sales_amount')\n",
    "print(\"\\nå®½æ ¼å¼æ•°æ® (æ¯åˆ—æ˜¯ä¸€ä¸ªæœˆ):\")\n",
    "print(wide_format.head())\n",
    "\n",
    "# å®½æ ¼å¼è½¬é•¿æ ¼å¼ (melt)\n",
    "long_format = wide_format.reset_index().melt(id_vars='region', \n",
    "                                           var_name='month', \n",
    "                                           value_name='sales_amount')\n",
    "print(\"\\nè½¬å›é•¿æ ¼å¼:\")\n",
    "print(long_format.head(10))\n",
    "\n",
    "# å¤æ‚çš„meltæ“ä½œ\n",
    "complex_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C'],\n",
    "    'math_q1': [85, 90, 78],\n",
    "    'math_q2': [88, 92, 80],\n",
    "    'science_q1': [92, 88, 85],\n",
    "    'science_q2': [90, 90, 87]\n",
    "})\n",
    "\n",
    "print(\"\\nåŸå§‹æˆç»©æ•°æ®:\")\n",
    "print(complex_data)\n",
    "\n",
    "# ä½¿ç”¨melté‡å¡‘æ•°æ®\n",
    "melted_scores = pd.melt(complex_data, \n",
    "                       id_vars=['id', 'name'],\n",
    "                       var_name='subject_quarter', \n",
    "                       value_name='score')\n",
    "\n",
    "# åˆ†ç¦»ç§‘ç›®å’Œå­£åº¦\n",
    "melted_scores[['subject', 'quarter']] = melted_scores['subject_quarter'].str.split('_', expand=True)\n",
    "melted_scores = melted_scores.drop('subject_quarter', axis=1)\n",
    "\n",
    "print(\"\\né‡å¡‘åçš„æˆç»©æ•°æ®:\")\n",
    "print(melted_scores)\n",
    "\n",
    "# stackå’Œunstackæ“ä½œ\n",
    "print(\"\\n=== Stackå’ŒUnstack ===\")\n",
    "\n",
    "# åˆ›å»ºå¤šçº§ç´¢å¼•æ•°æ®\n",
    "multi_index_data = extended_sales.groupby(['region', 'product'])['sales_amount'].sum()\n",
    "print(\"\\nå¤šçº§ç´¢å¼•æ•°æ®:\")\n",
    "print(multi_index_data.head(10))\n",
    "\n",
    "# unstackï¼šå°†å†…å±‚ç´¢å¼•è½¬ä¸ºåˆ—\n",
    "unstacked = multi_index_data.unstack()\n",
    "print(\"\\n Unstackå (äº§å“ä½œä¸ºåˆ—):\")\n",
    "print(unstacked.head())\n",
    "\n",
    "# stackï¼šå°†åˆ—è½¬ä¸ºå†…å±‚ç´¢å¼•\n",
    "restacked = unstacked.stack()\n",
    "print(\"\\né‡æ–°Stack:\")\n",
    "print(restacked.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9dc03",
   "metadata": {},
   "source": [
    "## 9. åŸºç¡€æ•°æ®å¯è§†åŒ–\n",
    "\n",
    "è™½ç„¶ä¸“é—¨çš„å¯è§†åŒ–ä¼šåœ¨matplotlibæ•™ç¨‹ä¸­è¯¦ç»†ä»‹ç»ï¼Œä½†pandasæä¾›äº†å¿«é€Ÿç»˜å›¾åŠŸèƒ½ï¼Œå¯¹æ•°æ®æ¢ç´¢éå¸¸æœ‰ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandaså†…ç½®ç»˜å›¾åŠŸèƒ½\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾å½¢æ ·å¼\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']  # æ”¯æŒä¸­æ–‡\n",
    "plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "\n",
    "print(\"=== åŸºç¡€ç»˜å›¾ ===\")\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "sample_data = extended_sales.groupby('date')['sales_amount'].sum().resample('W').mean()\n",
    "\n",
    "# çº¿å›¾\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sample_data.plot(title='å‘¨å¹³å‡é”€å”®é¢è¶‹åŠ¿', color='blue')\n",
    "plt.ylabel('é”€å”®é¢')\n",
    "\n",
    "# æŸ±çŠ¶å›¾\n",
    "plt.subplot(2, 2, 2)\n",
    "region_sales = extended_sales.groupby('region')['sales_amount'].sum()\n",
    "region_sales.plot(kind='bar', title='å„åœ°åŒºæ€»é”€å”®é¢', color='green')\n",
    "plt.ylabel('é”€å”®é¢')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# é¥¼å›¾\n",
    "plt.subplot(2, 2, 3)\n",
    "product_sales = extended_sales.groupby('product')['sales_amount'].sum()\n",
    "product_sales.plot(kind='pie', title='äº§å“é”€å”®é¢åˆ†å¸ƒ', autopct='%1.1f%%')\n",
    "\n",
    "# ç›´æ–¹å›¾\n",
    "plt.subplot(2, 2, 4)\n",
    "extended_sales['sales_amount'].plot(kind='hist', bins=30, title='é”€å”®é¢åˆ†å¸ƒ', alpha=0.7)\n",
    "plt.xlabel('é”€å”®é¢')\n",
    "plt.ylabel('é¢‘æ¬¡')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== é«˜çº§ç»˜å›¾ ===\")\n",
    "\n",
    "# ç®±çº¿å›¾\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "extended_sales.boxplot(column='sales_amount', by='region', ax=plt.gca())\n",
    "plt.title('å„åœ°åŒºé”€å”®é¢ç®±çº¿å›¾')\n",
    "plt.suptitle('')  # ç§»é™¤é»˜è®¤æ ‡é¢˜\n",
    "\n",
    "# æ•£ç‚¹å›¾çŸ©é˜µ\n",
    "plt.subplot(1, 2, 2)\n",
    "numeric_data = extended_sales[['sales_amount', 'units_sold', 'month', 'quarter']]\n",
    "pd.plotting.scatter_matrix(numeric_data, alpha=0.5, figsize=(6, 6), diagonal='hist')\n",
    "plt.suptitle('æ•°å€¼å˜é‡æ•£ç‚¹å›¾çŸ©é˜µ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ—¶é—´åºåˆ—å¯è§†åŒ–\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# æœˆåº¦è¶‹åŠ¿\n",
    "monthly_sales = extended_sales.groupby([extended_sales['date'].dt.to_period('M'), 'region'])['sales_amount'].sum().unstack()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "monthly_sales.plot(title='å„åœ°åŒºæœˆåº¦é”€å”®è¶‹åŠ¿', marker='o')\n",
    "plt.ylabel('é”€å”®é¢')\n",
    "plt.legend(title='åœ°åŒº')\n",
    "\n",
    "# ç´¯è®¡é”€å”®é¢\n",
    "plt.subplot(2, 1, 2)\n",
    "cumulative_sales = extended_sales.groupby('date')['sales_amount'].sum().cumsum()\n",
    "cumulative_sales.plot(title='ç´¯è®¡é”€å”®é¢', color='red', linewidth=2)\n",
    "plt.ylabel('ç´¯è®¡é”€å”®é¢')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== ç›¸å…³æ€§çƒ­åŠ›å›¾ ===\")\n",
    "\n",
    "# è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ\n",
    "correlation_data = extended_sales[['sales_amount', 'units_sold', 'month', 'quarter']].corr()\n",
    "\n",
    "# ä½¿ç”¨matplotlibç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_data, cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(correlation_data.columns)), correlation_data.columns, rotation=45)\n",
    "plt.yticks(range(len(correlation_data.columns)), correlation_data.columns)\n",
    "plt.title('å˜é‡ç›¸å…³æ€§çƒ­åŠ›å›¾')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i in range(len(correlation_data.columns)):\n",
    "    for j in range(len(correlation_data.columns)):\n",
    "        plt.text(j, i, f'{correlation_data.iloc[i, j]:.2f}', \n",
    "                ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"å¯è§†åŒ–å®Œæˆï¼æ³¨æ„ï¼š\")\n",
    "print(\"1. pandasçš„plot()æ–¹æ³•åŸºäºmatplotlib\")\n",
    "print(\"2. é€‚åˆå¿«é€Ÿæ•°æ®æ¢ç´¢å’ŒåŸå‹åˆ¶ä½œ\")\n",
    "print(\"3. æ›´å¤æ‚çš„å¯è§†åŒ–å»ºè®®ä½¿ç”¨matplotlibæˆ–seaborn\")\n",
    "print(\"4. äº¤äº’å¼å¯è§†åŒ–å¯ä»¥è€ƒè™‘plotlyæˆ–bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935c3b4",
   "metadata": {},
   "source": [
    "## 10. é«˜çº§ç‰¹æ€§ä¸æœ€ä½³å®è·µ\n",
    "\n",
    "è¿™ä¸€èŠ‚æ¶µç›–pandasçš„é«˜çº§åŠŸèƒ½å’Œåœ¨å®é™…é¡¹ç›®ä¸­çš„æœ€ä½³å®è·µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½ä¼˜åŒ–ä¸å†…å­˜ç®¡ç†\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(\"=== æ€§èƒ½ä¼˜åŒ–æŠ€å·§ ===\")\n",
    "\n",
    "# åˆ›å»ºå¤§æ•°æ®é›†ç”¨äºæ€§èƒ½æµ‹è¯•\n",
    "large_data = pd.DataFrame({\n",
    "    'id': range(100000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 100000),\n",
    "    'value': np.random.randn(100000),\n",
    "    'date': pd.date_range('2020-01-01', periods=100000, freq='min')\n",
    "})\n",
    "\n",
    "print(f\"æ•°æ®é›†å¤§å°: {large_data.shape}\")\n",
    "print(f\"å†…å­˜ä½¿ç”¨: {large_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "print(\"\\n=== æ•°æ®ç±»å‹ä¼˜åŒ– ===\")\n",
    "\n",
    "# æŸ¥çœ‹åŸå§‹æ•°æ®ç±»å‹\n",
    "print(\"åŸå§‹æ•°æ®ç±»å‹:\")\n",
    "print(large_data.dtypes)\n",
    "print(f\"åŸå§‹å†…å­˜ä½¿ç”¨: {large_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# ä¼˜åŒ–æ•°æ®ç±»å‹\n",
    "optimized_data = large_data.copy()\n",
    "\n",
    "# å°†categoryåˆ—è½¬ä¸ºcategoryç±»å‹\n",
    "optimized_data['category'] = optimized_data['category'].astype('category')\n",
    "\n",
    "# å°†valueåˆ—è½¬ä¸ºfloat32ï¼ˆå¦‚æœç²¾åº¦å…è®¸ï¼‰\n",
    "optimized_data['value'] = optimized_data['value'].astype('float32')\n",
    "\n",
    "print(\"\\nä¼˜åŒ–åæ•°æ®ç±»å‹:\")\n",
    "print(optimized_data.dtypes)\n",
    "print(f\"ä¼˜åŒ–åå†…å­˜ä½¿ç”¨: {optimized_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "memory_saved = (large_data.memory_usage(deep=True).sum() - optimized_data.memory_usage(deep=True).sum()) / 1024**2\n",
    "print(f\"èŠ‚çœå†…å­˜: {memory_saved:.2f} MB ({memory_saved/large_data.memory_usage(deep=True).sum()*1024**2*100:.1f}%)\")\n",
    "\n",
    "# å‘é‡åŒ–æ“ä½œ vs å¾ªç¯\n",
    "print(\"\\n=== å‘é‡åŒ–æ“ä½œæ€§èƒ½å¯¹æ¯” ===\")\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "test_data = pd.Series(np.random.randn(50000))\n",
    "\n",
    "# æ–¹æ³•1ï¼šä½¿ç”¨å¾ªç¯ï¼ˆä¸æ¨èï¼‰\n",
    "start_time = time.time()\n",
    "result_loop = []\n",
    "for value in test_data:\n",
    "    if value > 0:\n",
    "        result_loop.append(value * 2)\n",
    "    else:\n",
    "        result_loop.append(value / 2)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# æ–¹æ³•2ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆæ¨èï¼‰\n",
    "start_time = time.time()\n",
    "result_vectorized = np.where(test_data > 0, test_data * 2, test_data / 2)\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"å¾ªç¯æ–¹æ³•è€—æ—¶: {loop_time:.4f} ç§’\")\n",
    "print(f\"å‘é‡åŒ–æ–¹æ³•è€—æ—¶: {vectorized_time:.4f} ç§’\")\n",
    "print(f\"å‘é‡åŒ–æé€Ÿ: {loop_time/vectorized_time:.1f}x\")\n",
    "\n",
    "# ä½¿ç”¨eval()å’Œquery()è¿›è¡Œå¿«é€Ÿè®¡ç®—\n",
    "print(\"\\n=== eval()å’Œquery()ä¼˜åŒ– ===\")\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "df_eval = pd.DataFrame({\n",
    "    'A': np.random.randn(10000),\n",
    "    'B': np.random.randn(10000),\n",
    "    'C': np.random.randn(10000)\n",
    "})\n",
    "\n",
    "# ä¼ ç»Ÿæ–¹æ³•\n",
    "start_time = time.time()\n",
    "result_traditional = df_eval[(df_eval['A'] > 0) & (df_eval['B'] < 0.5)]['C'].sum()\n",
    "traditional_time = time.time() - start_time\n",
    "\n",
    "# ä½¿ç”¨query()\n",
    "start_time = time.time()\n",
    "result_query = df_eval.query('A > 0 and B < 0.5')['C'].sum()\n",
    "query_time = time.time() - start_time\n",
    "\n",
    "# ä½¿ç”¨eval()è¿›è¡Œè®¡ç®—\n",
    "start_time = time.time()\n",
    "df_eval['D'] = df_eval.eval('A + B * C')\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "print(f\"ä¼ ç»Ÿç­›é€‰è€—æ—¶: {traditional_time:.4f} ç§’\")\n",
    "print(f\"query()ç­›é€‰è€—æ—¶: {query_time:.4f} ç§’\")\n",
    "print(f\"eval()è®¡ç®—è€—æ—¶: {eval_time:.4f} ç§’\")\n",
    "\n",
    "print(\"\\n=== å¤§æ–‡ä»¶å¤„ç†æŠ€å·§ ===\")\n",
    "\n",
    "# åˆ†å—è¯»å–å¤§æ–‡ä»¶\n",
    "def process_large_file_demo():\n",
    "    \"\"\"æ¼”ç¤ºåˆ†å—å¤„ç†å¤§æ–‡ä»¶çš„æ–¹æ³•\"\"\"\n",
    "    \n",
    "    # æ¨¡æ‹Ÿåˆ›å»ºå¤§æ–‡ä»¶\n",
    "    sample_large_data = pd.DataFrame({\n",
    "        'id': range(50000),\n",
    "        'value': np.random.randn(50000),\n",
    "        'category': np.random.choice(['X', 'Y', 'Z'], 50000)\n",
    "    })\n",
    "    \n",
    "    # ä¿å­˜ä¸ºCSV\n",
    "    sample_large_data.to_csv('/tmp/large_file.csv', index=False)\n",
    "    \n",
    "    # åˆ†å—è¯»å–å’Œå¤„ç†\n",
    "    chunk_size = 10000\n",
    "    total_sum = 0\n",
    "    category_counts = {}\n",
    "    \n",
    "    for chunk in pd.read_csv('/tmp/large_file.csv', chunksize=chunk_size):\n",
    "        # å¤„ç†æ¯ä¸ªchunk\n",
    "        total_sum += chunk['value'].sum()\n",
    "        \n",
    "        # ç´¯è®¡ç±»åˆ«è®¡æ•°\n",
    "        for category, count in chunk['category'].value_counts().items():\n",
    "            category_counts[category] = category_counts.get(category, 0) + count\n",
    "    \n",
    "    print(f\"æ€»å’Œ: {total_sum:.2f}\")\n",
    "    print(f\"ç±»åˆ«è®¡æ•°: {category_counts}\")\n",
    "\n",
    "# è°ƒç”¨æ¼”ç¤ºå‡½æ•°\n",
    "process_large_file_demo()\n",
    "\n",
    "print(\"\\n=== ç´¢å¼•ä¼˜åŒ– ===\")\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "test_df = pd.DataFrame({\n",
    "    'key': np.random.choice(['A', 'B', 'C'], 10000),\n",
    "    'value': np.random.randn(10000)\n",
    "})\n",
    "\n",
    "# æ— ç´¢å¼•æŸ¥è¯¢\n",
    "start_time = time.time()\n",
    "result_no_index = test_df[test_df['key'] == 'A']['value'].mean()\n",
    "no_index_time = time.time() - start_time\n",
    "\n",
    "# è®¾ç½®ç´¢å¼•åæŸ¥è¯¢\n",
    "indexed_df = test_df.set_index('key')\n",
    "start_time = time.time()\n",
    "result_with_index = indexed_df.loc['A']['value'].mean()\n",
    "with_index_time = time.time() - start_time\n",
    "\n",
    "print(f\"æ— ç´¢å¼•æŸ¥è¯¢è€—æ—¶: {no_index_time:.4f} ç§’\")\n",
    "print(f\"æœ‰ç´¢å¼•æŸ¥è¯¢è€—æ—¶: {with_index_time:.4f} ç§’\")\n",
    "print(f\"ç´¢å¼•æé€Ÿ: {no_index_time/with_index_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa97b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ä½³å®è·µå’Œä»£ç é£æ ¼\n",
    "print(\"=== æœ€ä½³å®è·µ ===\")\n",
    "\n",
    "# 1. æ–¹æ³•é“¾å¼è°ƒç”¨\n",
    "print(\"1. æ–¹æ³•é“¾å¼è°ƒç”¨ (Method Chaining):\")\n",
    "\n",
    "# ä¸æ¨èçš„æ–¹å¼\n",
    "df_temp = extended_sales.copy()\n",
    "df_temp = df_temp[df_temp['sales_amount'] > 500]\n",
    "df_temp = df_temp.groupby('region')['sales_amount'].mean()\n",
    "df_temp = df_temp.sort_values(ascending=False)\n",
    "\n",
    "# æ¨èçš„é“¾å¼è°ƒç”¨\n",
    "result_chained = (extended_sales\n",
    "                 .query('sales_amount > 500')\n",
    "                 .groupby('region')['sales_amount']\n",
    "                 .mean()\n",
    "                 .sort_values(ascending=False))\n",
    "\n",
    "print(\"é“¾å¼è°ƒç”¨ç»“æœ:\")\n",
    "print(result_chained)\n",
    "\n",
    "# 2. ä½¿ç”¨assignåˆ›å»ºæ–°åˆ—\n",
    "print(\"\\n2. ä½¿ç”¨assign()åˆ›å»ºæ–°åˆ—:\")\n",
    "\n",
    "# ä¼ ç»Ÿæ–¹å¼\n",
    "df_traditional = extended_sales.copy()\n",
    "df_traditional['profit_margin'] = df_traditional['sales_amount'] * 0.2\n",
    "df_traditional['is_high_value'] = df_traditional['sales_amount'] > 1000\n",
    "\n",
    "# ä½¿ç”¨assignï¼ˆæ›´ä¼˜é›…ï¼‰\n",
    "df_assigned = (extended_sales\n",
    "              .assign(profit_margin=lambda x: x['sales_amount'] * 0.2,\n",
    "                     is_high_value=lambda x: x['sales_amount'] > 1000))\n",
    "\n",
    "print(\"ä½¿ç”¨assignåˆ›å»ºçš„åˆ—:\")\n",
    "print(df_assigned[['sales_amount', 'profit_margin', 'is_high_value']].head())\n",
    "\n",
    "# 3. æ¡ä»¶é€»è¾‘çš„ä¼˜é›…å¤„ç†\n",
    "print(\"\\n3. æ¡ä»¶é€»è¾‘å¤„ç†:\")\n",
    "\n",
    "# ä½¿ç”¨np.selectè¿›è¡Œå¤šæ¡ä»¶åˆ†ç±»\n",
    "conditions = [\n",
    "    extended_sales['sales_amount'] < 300,\n",
    "    extended_sales['sales_amount'] < 700,\n",
    "    extended_sales['sales_amount'] < 1200\n",
    "]\n",
    "choices = ['ä½', 'ä¸­', 'é«˜']\n",
    "extended_sales['é”€å”®ç­‰çº§'] = np.select(conditions, choices, default='å¾ˆé«˜')\n",
    "\n",
    "print(\"é”€å”®ç­‰çº§åˆ†å¸ƒ:\")\n",
    "print(extended_sales['é”€å”®ç­‰çº§'].value_counts())\n",
    "\n",
    "print(\"\\n=== å®é™…æ¡ˆä¾‹ï¼šç”µå•†æ•°æ®åˆ†æ ===\")\n",
    "\n",
    "# åˆ›å»ºæ¨¡æ‹Ÿç”µå•†æ•°æ®\n",
    "np.random.seed(42)\n",
    "ecommerce_data = pd.DataFrame({\n",
    "    'order_id': range(1, 1001),\n",
    "    'customer_id': np.random.randint(1, 201, 1000),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], 1000),\n",
    "    'order_value': np.random.exponential(100, 1000) + 20,\n",
    "    'order_date': pd.date_range('2023-01-01', periods=1000, freq='H'),\n",
    "    'shipping_cost': np.random.uniform(5, 50, 1000),\n",
    "    'customer_rating': np.random.choice([1, 2, 3, 4, 5], 1000, p=[0.05, 0.1, 0.15, 0.35, 0.35])\n",
    "})\n",
    "\n",
    "print(\"ç”µå•†æ•°æ®æ ·æœ¬:\")\n",
    "print(ecommerce_data.head())\n",
    "\n",
    "# ç»¼åˆåˆ†ææµç¨‹\n",
    "analysis_result = (ecommerce_data\n",
    "    # 1. æ•°æ®æ¸…æ´—\n",
    "    .assign(\n",
    "        net_value=lambda x: x['order_value'] - x['shipping_cost'],\n",
    "        order_month=lambda x: x['order_date'].dt.to_period('M'),\n",
    "        is_high_rating=lambda x: x['customer_rating'] >= 4\n",
    "    )\n",
    "    # 2. ç­›é€‰æœ‰æ•ˆè®¢å•\n",
    "    .query('order_value > 0 and shipping_cost > 0')\n",
    "    # 3. åˆ†ç»„åˆ†æ\n",
    "    .groupby(['product_category', 'order_month'])\n",
    "    .agg({\n",
    "        'order_value': ['count', 'mean', 'sum'],\n",
    "        'net_value': 'mean',\n",
    "        'customer_rating': 'mean',\n",
    "        'is_high_rating': 'mean'\n",
    "    })\n",
    "    # 4. é‡æ–°æ•´ç†åˆ—å\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"\\nç”µå•†æ•°æ®ç»¼åˆåˆ†æç»“æœ:\")\n",
    "print(analysis_result.head(10))\n",
    "\n",
    "# å®¢æˆ·ä»·å€¼åˆ†æ\n",
    "customer_analysis = (ecommerce_data\n",
    "    .groupby('customer_id')\n",
    "    .agg({\n",
    "        'order_id': 'count',  # è®¢å•é¢‘æ¬¡\n",
    "        'order_value': ['sum', 'mean'],  # æ€»æ¶ˆè´¹å’Œå¹³å‡æ¶ˆè´¹\n",
    "        'customer_rating': 'mean',  # å¹³å‡è¯„åˆ†\n",
    "        'order_date': ['min', 'max']  # é¦–æ¬¡å’Œæœ€è¿‘è´­ä¹°æ—¶é—´\n",
    "    })\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# å±•å¹³å¤šçº§åˆ—å\n",
    "customer_analysis.columns = ['_'.join(col).strip() for col in customer_analysis.columns]\n",
    "\n",
    "# è®¡ç®—å®¢æˆ·ç”Ÿå‘½å‘¨æœŸ\n",
    "customer_analysis['days_between_orders'] = (\n",
    "    pd.to_datetime(customer_analysis['order_date_max']) - \n",
    "    pd.to_datetime(customer_analysis['order_date_min'])\n",
    ").dt.days\n",
    "\n",
    "# å®¢æˆ·åˆ†å±‚\n",
    "customer_analysis['customer_tier'] = pd.cut(\n",
    "    customer_analysis['order_value_sum'], \n",
    "    bins=[0, 200, 500, 1000, float('inf')],\n",
    "    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    ")\n",
    "\n",
    "print(\"\\nå®¢æˆ·ä»·å€¼åˆ†æ (å‰10å):\")\n",
    "top_customers = customer_analysis.sort_values('order_value_sum', ascending=False).head(10)\n",
    "print(top_customers[['order_id_count', 'order_value_sum', 'order_value_mean', 'customer_tier']])\n",
    "\n",
    "print(\"\\nå®¢æˆ·åˆ†å±‚ç»Ÿè®¡:\")\n",
    "print(customer_analysis['customer_tier'].value_counts())\n",
    "\n",
    "print(\"\\n=== ç»ƒä¹ é¢˜ ===\")\n",
    "print(\"åŸºäºä¸Šé¢çš„ç”µå•†æ•°æ®ï¼Œå°è¯•å®Œæˆä»¥ä¸‹åˆ†æï¼š\")\n",
    "print(\"1. æ‰¾å‡ºæ¯ä¸ªäº§å“ç±»åˆ«ä¸­è¯„åˆ†æœ€é«˜çš„è®¢å•\")\n",
    "print(\"2. è®¡ç®—æ¯æœˆçš„æ”¶å…¥å¢é•¿ç‡\")\n",
    "print(\"3. è¯†åˆ«å¯èƒ½æµå¤±çš„å®¢æˆ·ï¼ˆæœ€è¿‘30å¤©æ— è®¢å•ï¼‰\")\n",
    "print(\"4. åˆ†æä¸åŒè¯„åˆ†ç­‰çº§çš„è®¢å•ä»·å€¼åˆ†å¸ƒ\")\n",
    "print(\"5. è®¡ç®—å„äº§å“ç±»åˆ«çš„åˆ©æ¶¦ç‡ï¼ˆå‡è®¾æˆæœ¬ä¸ºè®¢å•ä»·å€¼çš„60%ï¼‰\")\n",
    "\n",
    "# æä¾›éƒ¨åˆ†è§£ç­”ç¤ºä¾‹\n",
    "print(\"\\nè§£ç­”ç¤ºä¾‹1 - æ¯ä¸ªäº§å“ç±»åˆ«ä¸­è¯„åˆ†æœ€é«˜çš„è®¢å•:\")\n",
    "best_orders = (ecommerce_data\n",
    "              .loc[ecommerce_data.groupby('product_category')['customer_rating'].idxmax()]\n",
    "              [['product_category', 'order_id', 'customer_rating', 'order_value']])\n",
    "print(best_orders)\n",
    "\n",
    "print(\"\\næœ¬èŠ‚æ€»ç»“:\")\n",
    "print(\"- æŒæ¡äº†pandasçš„æ ¸å¿ƒåŠŸèƒ½ï¼šæ•°æ®ç»“æ„ã€æ“ä½œã€åˆ†ç»„ã€åˆå¹¶ã€æ—¶é—´åºåˆ—\")\n",
    "print(\"- å­¦ä¼šäº†æ•°æ®æ¸…æ´—ã€é¢„å¤„ç†å’Œè´¨é‡æ§åˆ¶\")\n",
    "print(\"- äº†è§£äº†é€è§†è¡¨ã€äº¤å‰è¡¨ç­‰æ•°æ®åˆ†æå·¥å…·\")\n",
    "print(\"- æŒæ¡äº†æ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µ\")\n",
    "print(\"- é€šè¿‡å®é™…æ¡ˆä¾‹ç»ƒä¹ äº†ç»¼åˆæ•°æ®åˆ†ææµç¨‹\")\n",
    "print(\"\\nä¸‹ä¸€æ­¥ï¼šå­¦ä¹ matplotlibè¿›è¡Œé«˜çº§æ•°æ®å¯è§†åŒ–ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7908b",
   "metadata": {},
   "source": [
    "## 11. æ€»ç»“ä¸è¿›é˜¶æ–¹å‘\n",
    "\n",
    "### æœ¬èŠ‚å­¦ä¹ å†…å®¹å›é¡¾\n",
    "\n",
    "âœ… **æ•°æ®ç»“æ„åŸºç¡€**: Serieså’ŒDataFrameçš„åˆ›å»ºã€å±æ€§å’ŒåŸºæœ¬æ“ä½œ  \n",
    "âœ… **ç´¢å¼•ä¸é€‰æ‹©**: å„ç§ç´¢å¼•æ–¹æ³•ã€æ¡ä»¶ç­›é€‰ã€å¸ƒå°”ç´¢å¼•  \n",
    "âœ… **æ•°æ®æ¸…æ´—**: å¤„ç†ç¼ºå¤±å€¼ã€é‡å¤å€¼ã€æ•°æ®ç±»å‹è½¬æ¢ã€å¼‚å¸¸å€¼æ£€æµ‹  \n",
    "âœ… **åˆ†ç»„ä¸èšåˆ**: GroupByæ“ä½œã€è‡ªå®šä¹‰èšåˆå‡½æ•°ã€æ•°æ®å˜æ¢  \n",
    "âœ… **æ•°æ®åˆå¹¶**: mergeã€joinã€concatçš„ä¸åŒç”¨æ³•å’Œåœºæ™¯  \n",
    "âœ… **æ—¶é—´åºåˆ—**: æ—¥æœŸæ—¶é—´å¤„ç†ã€é‡é‡‡æ ·ã€æ»‘åŠ¨çª—å£åˆ†æ  \n",
    "âœ… **é€è§†è¡¨**: pivot_tableã€äº¤å‰è¡¨ã€æ•°æ®é‡å¡‘  \n",
    "âœ… **æ•°æ®å¯è§†åŒ–**: pandaså†…ç½®ç»˜å›¾åŠŸèƒ½  \n",
    "âœ… **æ€§èƒ½ä¼˜åŒ–**: å†…å­˜ç®¡ç†ã€å‘é‡åŒ–æ“ä½œã€æœ€ä½³å®è·µ  \n",
    "âœ… **å®é™…åº”ç”¨**: ç”µå•†æ•°æ®åˆ†æç»¼åˆæ¡ˆä¾‹  \n",
    "\n",
    "### æ ¸å¿ƒæŠ€èƒ½æŒæ¡æ£€æŸ¥\n",
    "\n",
    "**åŸºç¡€æ“ä½œ** (å¿…é¡»æŒæ¡)\n",
    "- [ ] åˆ›å»ºå’Œæ“ä½œDataFrameå’ŒSeries\n",
    "- [ ] æ•°æ®é€‰æ‹©å’Œç­›é€‰\n",
    "- [ ] åŸºæœ¬ç»Ÿè®¡åˆ†æ\n",
    "- [ ] æ•°æ®å¯¼å…¥å¯¼å‡º\n",
    "\n",
    "**ä¸­çº§æŠ€èƒ½** (é‡è¦)\n",
    "- [ ] æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†\n",
    "- [ ] åˆ†ç»„èšåˆæ“ä½œ\n",
    "- [ ] æ•°æ®åˆå¹¶å’Œè¿æ¥\n",
    "- [ ] åŸºç¡€å¯è§†åŒ–\n",
    "\n",
    "**é«˜çº§æŠ€èƒ½** (æ¨è)\n",
    "- [ ] æ—¶é—´åºåˆ—åˆ†æ\n",
    "- [ ] é€è§†è¡¨å’Œæ•°æ®é‡å¡‘\n",
    "- [ ] æ€§èƒ½ä¼˜åŒ–\n",
    "- [ ] å¤æ‚æ•°æ®åˆ†ææµç¨‹\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "1. **å¤šåšé¡¹ç›®**: ç”¨çœŸå®æ•°æ®é›†ç»ƒä¹ ï¼Œå¦‚Kaggleæ•°æ®é›†\n",
    "2. **å»ºç«‹ä¹ æƒ¯**: å§‹ç»ˆè¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¢ç´¢æ€§åˆ†æ\n",
    "3. **æ€§èƒ½æ„è¯†**: å¤„ç†å¤§æ•°æ®æ—¶è€ƒè™‘å†…å­˜å’Œè®¡ç®—æ•ˆç‡\n",
    "4. **æ–‡æ¡£åŒ–**: ä¸ºå¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹å†™æ³¨é‡Šå’Œæ–‡æ¡£\n",
    "\n",
    "### å¸¸è§é”™è¯¯å’Œæ³¨æ„äº‹é¡¹\n",
    "\n",
    "âš ï¸ **é¿å…çš„é”™è¯¯**:\n",
    "- å¿˜è®°æ£€æŸ¥æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼\n",
    "- è¿‡åº¦ä½¿ç”¨å¾ªç¯è€Œä¸ç”¨å‘é‡åŒ–æ“ä½œ\n",
    "- ä¸å¤‡ä»½åŸå§‹æ•°æ®å°±è¿›è¡Œä¿®æ”¹\n",
    "- å¿½ç•¥ç´¢å¼•å¯¹é½é—®é¢˜\n",
    "\n",
    "âœ… **æ¨èåšæ³•**:\n",
    "- æ•°æ®åˆ†æå‰å…ˆåš.info()å’Œ.describe()\n",
    "- ä½¿ç”¨æ–¹æ³•é“¾æé«˜ä»£ç å¯è¯»æ€§\n",
    "- é€‚å½“ä½¿ç”¨categoryç±»å‹èŠ‚çœå†…å­˜\n",
    "- ä¸ºé‡è¦çš„ä¸­é—´ç»“æœåˆ›å»ºæ£€æŸ¥ç‚¹\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ è·¯å¾„\n",
    "\n",
    "**ç«‹å³å¯å­¦**:\n",
    "- **Matplotlib** (`04_matplotlib.ipynb`): æ·±å…¥å­¦ä¹ æ•°æ®å¯è§†åŒ–\n",
    "- **NumPyé«˜çº§ç‰¹æ€§**: ç»“åˆnumpyè¿›è¡Œæ›´é«˜æ•ˆçš„æ•°å€¼è®¡ç®—\n",
    "\n",
    "**åç»­æ·±å…¥**:\n",
    "- **æœºå™¨å­¦ä¹ **: ä½¿ç”¨pandasä¸ºscikit-learnå‡†å¤‡æ•°æ®\n",
    "- **å¤§æ•°æ®å·¥å…·**: Daskã€Vaexç­‰å¤„ç†è¶…å¤§æ•°æ®é›†\n",
    "- **æ•°æ®åº“**: å­¦ä¹ SQLä¸pandasçš„ç»“åˆä½¿ç”¨\n",
    "\n",
    "### æ¨èèµ„æº\n",
    "\n",
    "ğŸ“š **å®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹**:\n",
    "- [Pandaså®˜æ–¹æ–‡æ¡£](https://pandas.pydata.org/docs/)\n",
    "- [10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
    "\n",
    "ğŸ“Š **å®è·µå¹³å°**:\n",
    "- [Kaggle Learn - Pandas](https://www.kaggle.com/learn/pandas)\n",
    "- [DataCamp - Pandasè¯¾ç¨‹](https://www.datacamp.com/courses/data-manipulation-with-python)\n",
    "\n",
    "ğŸ¯ **ç»ƒä¹ æ•°æ®é›†**:\n",
    "- Titanicæ•°æ®é›† (Kaggle)\n",
    "- è‚¡ç¥¨ä»·æ ¼æ•°æ®\n",
    "- å¤©æ°”æ•°æ®\n",
    "- ç”µå•†é”€å”®æ•°æ®\n",
    "\n",
    "### è¿›é˜¶ä¸»é¢˜\n",
    "\n",
    "å½“ä½ ç†Ÿç»ƒæŒæ¡åŸºç¡€pandasåï¼Œå¯ä»¥æ¢ç´¢ï¼š\n",
    "\n",
    "1. **Pandasæ‰©å±•**: \n",
    "   - pandas-profiling (è‡ªåŠ¨æ•°æ®åˆ†ææŠ¥å‘Š)\n",
    "   - plotly.express (äº¤äº’å¼å¯è§†åŒ–)\n",
    "   - seaborn (ç»Ÿè®¡å¯è§†åŒ–)\n",
    "\n",
    "2. **å¤§æ•°æ®å¤„ç†**:\n",
    "   - Dask (å¹¶è¡Œè®¡ç®—)\n",
    "   - Vaex (å†…å­˜å¤–è®¡ç®—)\n",
    "   - Apache Arrow (åˆ—å¼å­˜å‚¨)\n",
    "\n",
    "3. **ä¸“ä¸šé¢†åŸŸåº”ç”¨**:\n",
    "   - é‡‘èæ•°æ®åˆ†æ (pandas-ta, yfinance)\n",
    "   - æ—¶é—´åºåˆ—åˆ†æ (statsmodels, prophet)\n",
    "   - åœ°ç†æ•°æ® (geopandas)\n",
    "\n",
    "ç°åœ¨ä½ å·²ç»å…·å¤‡äº†å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›ï¼Œå¯ä»¥å¼€å§‹å­¦ä¹ æ•°æ®å¯è§†åŒ–å’Œæœºå™¨å­¦ä¹ äº†ï¼"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
