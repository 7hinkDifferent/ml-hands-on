{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fe7c15",
   "metadata": {},
   "source": [
    "# Scikit-learnæœºå™¨å­¦ä¹ å®Œå…¨æ•™ç¨‹\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å°†å…¨é¢ä»‹ç»Scikit-learnâ€”â€”Pythonä¸­æœ€é‡è¦çš„æœºå™¨å­¦ä¹ åº“ï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "- **æœºå™¨å­¦ä¹ åŸºç¡€**: ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€æ¨¡å‹è¯„ä¼°\n",
    "- **æ•°æ®é¢„å¤„ç†**: ç‰¹å¾ç¼©æ”¾ã€ç¼–ç ã€ç‰¹å¾é€‰æ‹©ã€æ•°æ®æ¸…æ´—\n",
    "- **åˆ†ç±»ç®—æ³•**: æ”¯æŒå‘é‡æœº(SVM)ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€é€»è¾‘å›å½’ç­‰\n",
    "- **å›å½’ç®—æ³•**: çº¿æ€§å›å½’ã€å¤šé¡¹å¼å›å½’ã€å²­å›å½’ã€LASSOå›å½’\n",
    "- **èšç±»ç®—æ³•**: K-meansã€å±‚æ¬¡èšç±»ã€DBSCAN\n",
    "- **é™ç»´æŠ€æœ¯**: PCAã€t-SNEã€ç‰¹å¾é€‰æ‹©\n",
    "- **æ¨¡å‹é€‰æ‹©**: äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢ã€æ¨¡å‹è¯„ä¼°æŒ‡æ ‡\n",
    "- **å®é™…é¡¹ç›®**: MNISTæ‰‹å†™æ•°å­—åˆ†ç±»ã€å®Œæ•´æœºå™¨å­¦ä¹ æµç¨‹\n",
    "\n",
    "**é‡ç‚¹æ¡ˆä¾‹**: ä½¿ç”¨æ”¯æŒå‘é‡æœº(SVM)å¯¹MNISTæ•°æ®é›†è¿›è¡Œæ‰‹å†™æ•°å­—åˆ†ç±»ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰å’Œæ¨¡å¼è¯†åˆ«é¢†åŸŸçš„ç»å…¸é—®é¢˜ã€‚\n",
    "\n",
    "Scikit-learnä»¥å…¶ç»Ÿä¸€çš„APIã€ä¸°å¯Œçš„ç®—æ³•åº“å’Œä¼˜ç§€çš„æ–‡æ¡£è€Œé—»åï¼Œæ˜¯æœºå™¨å­¦ä¹ å…¥é—¨å’Œå®è·µçš„é¦–é€‰å·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä¿è¯ç»“æœå¯é‡ç°\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"=== Scikit-learnç¯å¢ƒé…ç½® ===\")\n",
    "import sklearn\n",
    "print(f\"Scikit-learnç‰ˆæœ¬: {sklearn.__version__}\")\n",
    "print(f\"NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "\n",
    "# åŠ è½½MNISTæ•°æ®é›† (sklearnå†…ç½®çš„ç®€åŒ–ç‰ˆæœ¬)\n",
    "print(f\"\\n=== åŠ è½½MNISTæ•°æ®é›† ===\")\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# åŠ è½½MNISTæ•°æ®é›†\n",
    "print(\"æ­£åœ¨ä¸‹è½½MNISTæ•°æ®é›†...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "X_mnist, y_mnist = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "print(f\"MNISTæ•°æ®é›†ä¿¡æ¯:\")\n",
    "print(f\"- ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_mnist.shape}\")\n",
    "print(f\"- æ ‡ç­¾å‘é‡å½¢çŠ¶: {y_mnist.shape}\")\n",
    "print(f\"- ç‰¹å¾èŒƒå›´: {X_mnist.min():.1f} - {X_mnist.max():.1f}\")\n",
    "print(f\"- ç±»åˆ«æ•°é‡: {len(np.unique(y_mnist))}\")\n",
    "print(f\"- ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "unique, counts = np.unique(y_mnist, return_counts=True)\n",
    "for digit, count in zip(unique, counts):\n",
    "    print(f\"  æ•°å­— {digit}: {count:,} æ ·æœ¬\")\n",
    "\n",
    "# å¯è§†åŒ–MNISTæ ·æœ¬\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    # æ‰¾åˆ°æ¯ä¸ªæ•°å­—çš„ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "    idx = np.where(y_mnist == i)[0][0]\n",
    "    row, col = divmod(i, 5)\n",
    "    \n",
    "    # å°†784ç»´å‘é‡é‡å¡‘ä¸º28x28å›¾åƒ\n",
    "    image = X_mnist[idx].reshape(28, 28)\n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].set_title(f'æ•°å­— {i}')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('MNISTæ•°æ®é›†æ ·æœ¬å±•ç¤º', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ç¯å¢ƒé…ç½®å’Œæ•°æ®åŠ è½½å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f62a0",
   "metadata": {},
   "source": [
    "## 1. æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ\n",
    "\n",
    "### 1.1 æœºå™¨å­¦ä¹ ç±»å‹\n",
    "\n",
    "- **ç›‘ç£å­¦ä¹ **: ä»æ ‡è®°çš„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ï¼Œé¢„æµ‹æ–°æ•°æ®çš„æ ‡ç­¾\n",
    "  - åˆ†ç±» (Classification): é¢„æµ‹ç¦»æ•£æ ‡ç­¾ (å¦‚æ•°å­—è¯†åˆ«)\n",
    "  - å›å½’ (Regression): é¢„æµ‹è¿ç»­æ•°å€¼ (å¦‚æˆ¿ä»·é¢„æµ‹)\n",
    "\n",
    "- **æ— ç›‘ç£å­¦ä¹ **: ä»æ— æ ‡ç­¾æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼\n",
    "  - èšç±» (Clustering): å°†æ•°æ®åˆ†ç»„\n",
    "  - é™ç»´ (Dimensionality Reduction): ç®€åŒ–æ•°æ®è¡¨ç¤º\n",
    "\n",
    "- **å¼ºåŒ–å­¦ä¹ **: é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥\n",
    "\n",
    "### 1.2 Scikit-learnçš„è®¾è®¡å“²å­¦\n",
    "\n",
    "- **ä¸€è‡´çš„API**: æ‰€æœ‰ä¼°è®¡å™¨éƒ½æœ‰fit()ã€predict()ç­‰æ–¹æ³•\n",
    "- **ç»„åˆæ€§**: ä¸åŒç»„ä»¶å¯ä»¥è½»æ¾ç»„åˆ\n",
    "- **åˆç†çš„é»˜è®¤å€¼**: å¼€ç®±å³ç”¨çš„å‚æ•°è®¾ç½®\n",
    "- **å¯æ£€æŸ¥æ€§**: æ¨¡å‹å†…éƒ¨çŠ¶æ€å¯è®¿é—®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Scikit-learnå…¸å‹å·¥ä½œæµç¨‹æ¼”ç¤º\n",
    "print(\"=== Scikit-learnå…¸å‹å·¥ä½œæµç¨‹ ===\")\n",
    "\n",
    "# ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é¸¢å°¾èŠ±æ•°æ®é›†æ¥æ¼”ç¤ºå®Œæ•´æµç¨‹\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# æ­¥éª¤1: åŠ è½½æ•°æ®\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(\"æ­¥éª¤1: æ•°æ®åŠ è½½\")\n",
    "print(f\"ç‰¹å¾åç§°: {iris.feature_names}\")\n",
    "print(f\"ç±»åˆ«åç§°: {iris.target_names}\")\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {X_iris.shape}\")\n",
    "\n",
    "# æ­¥éª¤2: æ•°æ®åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(f\"\\næ­¥éª¤2: æ•°æ®åˆ†å‰²\")\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\n",
    "\n",
    "# æ­¥éª¤3: ç‰¹å¾ç¼©æ”¾\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\næ­¥éª¤3: ç‰¹å¾ç¼©æ”¾\")\n",
    "print(f\"åŸå§‹ç‰¹å¾èŒƒå›´: {X_train.min():.2f} - {X_train.max():.2f}\")\n",
    "print(f\"ç¼©æ”¾åç‰¹å¾èŒƒå›´: {X_train_scaled.min():.2f} - {X_train_scaled.max():.2f}\")\n",
    "\n",
    "# æ­¥éª¤4: æ¨¡å‹è®­ç»ƒ\n",
    "model = SVC(kernel='rbf', random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\næ­¥éª¤4: æ¨¡å‹è®­ç»ƒ\")\n",
    "print(f\"æ¨¡å‹ç±»å‹: {type(model).__name__}\")\n",
    "print(f\"æ¨¡å‹å‚æ•°: {model.get_params()}\")\n",
    "\n",
    "# æ­¥éª¤5: æ¨¡å‹é¢„æµ‹\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\næ­¥éª¤5: æ¨¡å‹é¢„æµ‹\")\n",
    "print(f\"é¢„æµ‹ç»“æœ: {y_pred}\")\n",
    "print(f\"çœŸå®æ ‡ç­¾: {y_test}\")\n",
    "\n",
    "# æ­¥éª¤6: æ¨¡å‹è¯„ä¼°\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\næ­¥éª¤6: æ¨¡å‹è¯„ä¼°\")\n",
    "print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–\n",
    "feature_names = iris.feature_names\n",
    "for i, feature in enumerate([0, 2]):  # é€‰æ‹©ä¸¤ä¸ªç‰¹å¾\n",
    "    for class_idx, class_name in enumerate(iris.target_names):\n",
    "        mask = y_iris == class_idx\n",
    "        axes[0].scatter(X_iris[mask, feature], X_iris[mask, 1], \n",
    "                       label=class_name, alpha=0.7)\n",
    "    \n",
    "axes[0].set_xlabel(feature_names[0])\n",
    "axes[0].set_ylabel(feature_names[1])\n",
    "axes[0].set_title('é¸¢å°¾èŠ±æ•°æ®é›†ç‰¹å¾åˆ†å¸ƒ')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ··æ·†çŸ©é˜µ\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "im = axes[1].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[1].set_title('æ··æ·†çŸ©é˜µ')\n",
    "axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "axes[1].set_ylabel('çœŸå®æ ‡ç­¾')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        axes[1].text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼\")\n",
    "print(\"è¿™å°±æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®çš„æ ‡å‡†æµç¨‹ï¼š\")\n",
    "print(\"æ•°æ®åŠ è½½ â†’ æ•°æ®åˆ†å‰² â†’ ç‰¹å¾å¤„ç† â†’ æ¨¡å‹è®­ç»ƒ â†’ é¢„æµ‹ â†’ è¯„ä¼°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc66bb",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ã€‚é«˜è´¨é‡çš„æ•°æ®é¢„å¤„ç†é€šå¸¸æ¯”ç®—æ³•é€‰æ‹©æ›´é‡è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b294f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 ç‰¹å¾ç¼©æ”¾\n",
    "print(\"=== ç‰¹å¾ç¼©æ”¾ ===\")\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "from sklearn.datasets import make_classification\n",
    "X_example, y_example = make_classification(n_samples=1000, n_features=4, \n",
    "                                          n_informative=3, n_redundant=1, \n",
    "                                          random_state=42)\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸åŒé‡çº§çš„ç‰¹å¾\n",
    "X_example[:, 0] *= 1000  # ç¬¬ä¸€ä¸ªç‰¹å¾æ”¾å¤§1000å€\n",
    "X_example[:, 1] *= 0.01  # ç¬¬äºŒä¸ªç‰¹å¾ç¼©å°100å€\n",
    "\n",
    "print(\"åŸå§‹æ•°æ®ç‰¹å¾ç»Ÿè®¡:\")\n",
    "feature_stats = pd.DataFrame(X_example, columns=[f'ç‰¹å¾{i+1}' for i in range(4)])\n",
    "print(feature_stats.describe())\n",
    "\n",
    "# ä¸åŒçš„ç¼©æ”¾æ–¹æ³•\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "scalers = {\n",
    "    'æ ‡å‡†åŒ– (StandardScaler)': StandardScaler(),\n",
    "    'æœ€å°-æœ€å¤§ç¼©æ”¾ (MinMaxScaler)': MinMaxScaler(),\n",
    "    'é²æ£’ç¼©æ”¾ (RobustScaler)': RobustScaler()\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# åŸå§‹æ•°æ®åˆ†å¸ƒ\n",
    "axes[0, 0].boxplot(X_example, labels=[f'ç‰¹å¾{i+1}' for i in range(4)])\n",
    "axes[0, 0].set_title('åŸå§‹æ•°æ®åˆ†å¸ƒ')\n",
    "axes[0, 0].set_ylabel('æ•°å€¼')\n",
    "\n",
    "# ä¸åŒç¼©æ”¾æ–¹æ³•çš„æ•ˆæœ\n",
    "for idx, (name, scaler) in enumerate(scalers.items()):\n",
    "    row, col = divmod(idx + 1, 2)\n",
    "    X_scaled = scaler.fit_transform(X_example)\n",
    "    axes[row, col].boxplot(X_scaled, labels=[f'ç‰¹å¾{i+1}' for i in range(4)])\n",
    "    axes[row, col].set_title(name)\n",
    "    axes[row, col].set_ylabel('ç¼©æ”¾åæ•°å€¼')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2.2 åˆ†ç±»å˜é‡ç¼–ç \n",
    "print(\"\\n=== åˆ†ç±»å˜é‡ç¼–ç  ===\")\n",
    "\n",
    "# åˆ›å»ºåŒ…å«åˆ†ç±»å˜é‡çš„ç¤ºä¾‹æ•°æ®\n",
    "data_cat = pd.DataFrame({\n",
    "    'é¢œè‰²': ['çº¢', 'è“', 'ç»¿', 'çº¢', 'è“', 'ç»¿', 'é»„', 'é»„'],\n",
    "    'å°ºå¯¸': ['å°', 'ä¸­', 'å¤§', 'å°', 'å¤§', 'ä¸­', 'å°', 'å¤§'],\n",
    "    'å“ç‰Œ': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B'],\n",
    "    'ä»·æ ¼': [100, 200, 150, 120, 180, 160, 90, 210]\n",
    "})\n",
    "\n",
    "print(\"åŸå§‹åˆ†ç±»æ•°æ®:\")\n",
    "print(data_cat)\n",
    "\n",
    "# æ ‡ç­¾ç¼–ç  (Label Encoding)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_color = LabelEncoder()\n",
    "data_cat['é¢œè‰²_æ ‡ç­¾ç¼–ç '] = le_color.fit_transform(data_cat['é¢œè‰²'])\n",
    "\n",
    "print(f\"\\né¢œè‰²æ ‡ç­¾ç¼–ç æ˜ å°„: {dict(zip(le_color.classes_, range(len(le_color.classes_))))}\")\n",
    "\n",
    "# ç‹¬çƒ­ç¼–ç  (One-Hot Encoding)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "color_onehot = ohe.fit_transform(data_cat[['é¢œè‰²']])\n",
    "color_onehot_df = pd.DataFrame(color_onehot, columns=[f'é¢œè‰²_{cat}' for cat in ohe.categories_[0]])\n",
    "\n",
    "print(\"\\né¢œè‰²ç‹¬çƒ­ç¼–ç ç»“æœ:\")\n",
    "print(color_onehot_df.head())\n",
    "\n",
    "# åºæ•°ç¼–ç  (Ordinal Encoding) - é€‚ç”¨äºæœ‰é¡ºåºå…³ç³»çš„åˆ†ç±»å˜é‡\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# ä¸ºå°ºå¯¸å®šä¹‰é¡ºåº\n",
    "size_mapping = [['å°', 'ä¸­', 'å¤§']]\n",
    "oe = OrdinalEncoder(categories=size_mapping)\n",
    "data_cat['å°ºå¯¸_åºæ•°ç¼–ç '] = oe.fit_transform(data_cat[['å°ºå¯¸']])\n",
    "\n",
    "print(f\"\\nå°ºå¯¸åºæ•°ç¼–ç æ˜ å°„: å°=0, ä¸­=1, å¤§=2\")\n",
    "print(data_cat[['å°ºå¯¸', 'å°ºå¯¸_åºæ•°ç¼–ç ']])\n",
    "\n",
    "# 2.3 å¤„ç†ç¼ºå¤±å€¼\n",
    "print(\"\\n=== å¤„ç†ç¼ºå¤±å€¼ ===\")\n",
    "\n",
    "# åˆ›å»ºå«æœ‰ç¼ºå¤±å€¼çš„æ•°æ®\n",
    "data_missing = pd.DataFrame({\n",
    "    'å¹´é¾„': [25, 30, np.nan, 35, 28, np.nan, 32],\n",
    "    'æ”¶å…¥': [50000, np.nan, 60000, 70000, np.nan, 55000, 65000],\n",
    "    'æ•™è‚²å¹´é™': [16, 18, 12, np.nan, 14, 16, 20]\n",
    "})\n",
    "\n",
    "print(\"å«ç¼ºå¤±å€¼çš„æ•°æ®:\")\n",
    "print(data_missing)\n",
    "print(f\"\\nç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
    "print(data_missing.isnull().sum())\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# ç®€å•å¡«å……ç­–ç•¥\n",
    "imputers = {\n",
    "    'å‡å€¼å¡«å……': SimpleImputer(strategy='mean'),\n",
    "    'ä¸­ä½æ•°å¡«å……': SimpleImputer(strategy='median'),\n",
    "    'ä¼—æ•°å¡«å……': SimpleImputer(strategy='most_frequent'),\n",
    "    'KNNå¡«å……': KNNImputer(n_neighbors=3)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, imputer in imputers.items():\n",
    "    filled_data = imputer.fit_transform(data_missing)\n",
    "    results[name] = pd.DataFrame(filled_data, columns=data_missing.columns)\n",
    "\n",
    "# å±•ç¤ºä¸åŒå¡«å……æ–¹æ³•çš„ç»“æœ\n",
    "print(\"\\nä¸åŒå¡«å……æ–¹æ³•çš„ç»“æœå¯¹æ¯”:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(result.round(0))\n",
    "\n",
    "# 2.4 ç‰¹å¾é€‰æ‹©\n",
    "print(\"\\n=== ç‰¹å¾é€‰æ‹© ===\")\n",
    "\n",
    "# ä½¿ç”¨MNISTæ•°æ®çš„å­é›†è¿›è¡Œç‰¹å¾é€‰æ‹©æ¼”ç¤º\n",
    "# ä¸ºäº†è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬åªä½¿ç”¨å‰1000ä¸ªæ ·æœ¬\n",
    "X_subset = X_mnist[:1000]\n",
    "y_subset = y_mnist[:1000]\n",
    "\n",
    "print(f\"åŸå§‹ç‰¹å¾æ•°é‡: {X_subset.shape[1]}\")\n",
    "\n",
    "# æ–¹å·®é˜ˆå€¼ç‰¹å¾é€‰æ‹©\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_threshold = VarianceThreshold(threshold=100)  # ç§»é™¤æ–¹å·®å°äº100çš„ç‰¹å¾\n",
    "X_var_selected = var_threshold.fit_transform(X_subset)\n",
    "\n",
    "print(f\"æ–¹å·®é˜ˆå€¼é€‰æ‹©åç‰¹å¾æ•°é‡: {X_var_selected.shape[1]}\")\n",
    "\n",
    "# å•å˜é‡ç‰¹å¾é€‰æ‹©\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "k_best = SelectKBest(score_func=chi2, k=100)  # é€‰æ‹©å‰100ä¸ªæœ€ä½³ç‰¹å¾\n",
    "X_k_best = k_best.fit_transform(X_subset, y_subset)\n",
    "\n",
    "print(f\"SelectKBesté€‰æ‹©åç‰¹å¾æ•°é‡: {X_k_best.shape[1]}\")\n",
    "\n",
    "# é€’å½’ç‰¹å¾æ¶ˆé™¤\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºåŸºä¼°è®¡å™¨\n",
    "estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=50)\n",
    "X_rfe = rfe.fit_transform(X_subset, y_subset)\n",
    "\n",
    "print(f\"RFEé€‰æ‹©åç‰¹å¾æ•°é‡: {X_rfe.shape[1]}\")\n",
    "\n",
    "# å¯è§†åŒ–ç‰¹å¾é€‰æ‹©çš„æ•ˆæœ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# æ˜¾ç¤ºä¸€äº›åŸå§‹å›¾åƒ\n",
    "for i in range(3):\n",
    "    axes[i].imshow(X_subset[i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f'åŸå§‹å›¾åƒ (æ ‡ç­¾: {y_subset[i]})')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('MNISTåŸå§‹å›¾åƒç¤ºä¾‹', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\næ•°æ®é¢„å¤„ç†è¦ç‚¹:\")\n",
    "print(\"âœ“ ç‰¹å¾ç¼©æ”¾å¯¹è·ç¦»æ•æ„Ÿçš„ç®—æ³•å¾ˆé‡è¦\")\n",
    "print(\"âœ“ åˆ†ç±»å˜é‡ç¼–ç è¦æ ¹æ®å˜é‡æ€§è´¨é€‰æ‹©åˆé€‚æ–¹æ³•\")\n",
    "print(\"âœ“ ç¼ºå¤±å€¼å¤„ç†è¦è€ƒè™‘æ•°æ®çš„åˆ†å¸ƒå’Œå«ä¹‰\")\n",
    "print(\"âœ“ ç‰¹å¾é€‰æ‹©å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½å¹¶å‡å°‘è®¡ç®—æˆæœ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d00333",
   "metadata": {},
   "source": [
    "## 3. æ”¯æŒå‘é‡æœº (SVM) ä¸MNISTåˆ†ç±»\n",
    "\n",
    "æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§å¼ºå¤§çš„ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºé«˜ç»´æ•°æ®åˆ†ç±»ã€‚æˆ‘ä»¬å°†è¯¦ç»†å­¦ä¹ SVMçš„åŸç†ï¼Œå¹¶åœ¨MNISTæ•°æ®é›†ä¸Šè¿›è¡Œæ‰‹å†™æ•°å­—åˆ†ç±»å®æˆ˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 SVMåŸºç¡€ç†è®ºå’Œæ ¸å‡½æ•°\n",
    "print(\"=== SVMåŸºç¡€ç†è®º ===\")\n",
    "\n",
    "# é¦–å…ˆç”¨ç®€å•çš„äºŒåˆ†ç±»æ•°æ®æ¼”ç¤ºSVMåŸç†\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®\n",
    "X_2d, y_2d = make_blobs(n_samples=100, centers=2, cluster_std=1.2, \n",
    "                       center_box=(-2.0, 2.0), random_state=42)\n",
    "\n",
    "# ä¸åŒæ ¸å‡½æ•°çš„SVM\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    row, col = divmod(idx, 2)\n",
    "    \n",
    "    # è®­ç»ƒSVM\n",
    "    svm = SVC(kernel=kernel, random_state=42)\n",
    "    svm.fit(X_2d, y_2d)\n",
    "    \n",
    "    # åˆ›å»ºç½‘æ ¼ç”¨äºå¯è§†åŒ–å†³ç­–è¾¹ç•Œ\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # é¢„æµ‹ç½‘æ ¼ç‚¹\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = svm.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ\n",
    "    axes[row, col].contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "    \n",
    "    # ç»˜åˆ¶æ•°æ®ç‚¹\n",
    "    scatter = axes[row, col].scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu)\n",
    "    \n",
    "    # ç»˜åˆ¶æ”¯æŒå‘é‡\n",
    "    axes[row, col].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "                          s=100, facecolors='none', edgecolors='black', linewidth=2)\n",
    "    \n",
    "    axes[row, col].set_title(f'{kernel.upper()} æ ¸å‡½æ•°')\n",
    "    axes[row, col].set_xlabel('ç‰¹å¾ 1')\n",
    "    axes[row, col].set_ylabel('ç‰¹å¾ 2')\n",
    "\n",
    "plt.suptitle('ä¸åŒæ ¸å‡½æ•°çš„SVMå†³ç­–è¾¹ç•Œ', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"SVMæ ¸å‡½æ•°è§£é‡Š:\")\n",
    "print(\"â€¢ Linear: çº¿æ€§æ ¸ï¼Œé€‚ç”¨äºçº¿æ€§å¯åˆ†æ•°æ®\")\n",
    "print(\"â€¢ Polynomial: å¤šé¡¹å¼æ ¸ï¼Œå¯ä»¥å¤„ç†éçº¿æ€§å…³ç³»\")\n",
    "print(\"â€¢ RBF (é«˜æ–¯): å¾„å‘åŸºå‡½æ•°æ ¸ï¼Œæœ€å¸¸ç”¨çš„éçº¿æ€§æ ¸\")\n",
    "print(\"â€¢ Sigmoid: Så‹æ ¸ï¼Œç±»ä¼¼ç¥ç»ç½‘ç»œçš„æ¿€æ´»å‡½æ•°\")\n",
    "\n",
    "# 3.2 MNISTæ•°æ®é›†å‡†å¤‡å’Œé¢„å¤„ç†\n",
    "print(\"\\n=== MNISTæ•°æ®é›†å‡†å¤‡ ===\")\n",
    "\n",
    "# ä¸ºäº†è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨MNISTçš„å­é›†\n",
    "# åœ¨å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨å…¨éƒ¨æ•°æ®\n",
    "n_samples = 5000  # ä½¿ç”¨5000ä¸ªæ ·æœ¬è¿›è¡Œæ¼”ç¤º\n",
    "\n",
    "# éšæœºé‡‡æ ·\n",
    "indices = np.random.choice(len(X_mnist), n_samples, replace=False)\n",
    "X_mnist_subset = X_mnist[indices]\n",
    "y_mnist_subset = y_mnist[indices]\n",
    "\n",
    "print(f\"ä½¿ç”¨MNISTå­é›†: {X_mnist_subset.shape}\")\n",
    "print(f\"ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "unique, counts = np.unique(y_mnist_subset, return_counts=True)\n",
    "for digit, count in zip(unique, counts):\n",
    "    print(f\"  æ•°å­— {digit}: {count} æ ·æœ¬\")\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "# 1. ç‰¹å¾ç¼©æ”¾ (åƒç´ å€¼ä»0-255ç¼©æ”¾åˆ°0-1)\n",
    "X_mnist_scaled = X_mnist_subset / 255.0\n",
    "\n",
    "# 2. æ•°æ®åˆ†å‰²\n",
    "X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = train_test_split(\n",
    "    X_mnist_scaled, y_mnist_subset, test_size=0.3, random_state=42, \n",
    "    stratify=y_mnist_subset\n",
    ")\n",
    "\n",
    "print(f\"\\næ•°æ®åˆ†å‰²ç»“æœ:\")\n",
    "print(f\"è®­ç»ƒé›†: {X_train_mnist.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test_mnist.shape}\")\n",
    "\n",
    "# 3.3 SVMæ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–\n",
    "print(\"\\n=== SVMæ¨¡å‹è®­ç»ƒå’Œå‚æ•°ä¼˜åŒ– ===\")\n",
    "\n",
    "# é¦–å…ˆä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒSVM\n",
    "print(\"1. ä½¿ç”¨é»˜è®¤å‚æ•°çš„SVM:\")\n",
    "svm_default = SVC(random_state=42)\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "import time\n",
    "start_time = time.time()\n",
    "svm_default.fit(X_train_mnist, y_train_mnist)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’\")\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "y_pred_default = svm_default.predict(X_test_mnist)\n",
    "accuracy_default = accuracy_score(y_test_mnist, y_pred_default)\n",
    "print(f\"é»˜è®¤å‚æ•°å‡†ç¡®ç‡: {accuracy_default:.4f}\")\n",
    "\n",
    "# ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–å‚æ•°\n",
    "print(\"\\n2. ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–å‚æ•°:\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# æ³¨æ„ï¼šåœ¨å®é™…é¡¹ç›®ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„å‚æ•°ç½‘æ ¼\n",
    "# è¿™é‡Œä¸ºäº†æ¼”ç¤ºå’Œè®¡ç®—æ•ˆç‡ï¼Œä½¿ç”¨è¾ƒå°çš„ç½‘æ ¼\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42), \n",
    "    param_grid, \n",
    "    cv=3,  # 3æŠ˜äº¤å‰éªŒè¯\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...\")\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_mnist, y_train_mnist)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"ç½‘æ ¼æœç´¢æ—¶é—´: {search_time:.2f} ç§’\")\n",
    "print(f\"æœ€ä½³å‚æ•°: {grid_search.best_params_}\")\n",
    "print(f\"æœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ä½¿ç”¨æœ€ä½³å‚æ•°çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test_mnist)\n",
    "accuracy_best = accuracy_score(y_test_mnist, y_pred_best)\n",
    "print(f\"ä¼˜åŒ–åå‡†ç¡®ç‡: {accuracy_best:.4f}\")\n",
    "\n",
    "# 3.4 è¯¦ç»†çš„æ¨¡å‹è¯„ä¼°\n",
    "print(\"\\n=== è¯¦ç»†çš„æ¨¡å‹è¯„ä¼° ===\")\n",
    "\n",
    "# æ··æ·†çŸ©é˜µ\n",
    "cm = confusion_matrix(y_test_mnist, y_pred_best)\n",
    "\n",
    "# åˆ†ç±»æŠ¥å‘Š\n",
    "print(\"åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(classification_report(y_test_mnist, y_pred_best))\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾\n",
    "im = axes[0, 0].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[0, 0].set_title('æ··æ·†çŸ©é˜µ')\n",
    "axes[0, 0].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "axes[0, 0].set_ylabel('çœŸå®æ ‡ç­¾')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        axes[0, 0].text(j, i, format(cm[i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "plt.colorbar(im, ax=axes[0, 0])\n",
    "\n",
    "# 2. æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "axes[0, 1].bar(range(10), class_accuracy)\n",
    "axes[0, 1].set_title('å„æ•°å­—è¯†åˆ«å‡†ç¡®ç‡')\n",
    "axes[0, 1].set_xlabel('æ•°å­—')\n",
    "axes[0, 1].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 1].set_xticks(range(10))\n",
    "\n",
    "# 3. æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬å±•ç¤º\n",
    "correct_mask = y_test_mnist == y_pred_best\n",
    "correct_indices = np.where(correct_mask)[0][:6]\n",
    "\n",
    "for i, idx in enumerate(correct_indices):\n",
    "    row, col = divmod(i, 3)\n",
    "    if row < 2 and col < 3:\n",
    "        axes[row, col + (1 if row == 0 else 0)].imshow(\n",
    "            X_test_mnist[idx].reshape(28, 28), cmap='gray'\n",
    "        )\n",
    "        axes[row, col + (1 if row == 0 else 0)].set_title(\n",
    "            f'æ­£ç¡®: {y_test_mnist[idx]}'\n",
    "        )\n",
    "        axes[row, col + (1 if row == 0 else 0)].axis('off')\n",
    "\n",
    "# é”™è¯¯é¢„æµ‹çš„æ ·æœ¬å±•ç¤º  \n",
    "incorrect_mask = y_test_mnist != y_pred_best\n",
    "incorrect_indices = np.where(incorrect_mask)[0][:3]\n",
    "\n",
    "for i, idx in enumerate(incorrect_indices):\n",
    "    axes[1, i].imshow(X_test_mnist[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title(f'é”™è¯¯: çœŸå®{y_test_mnist[idx]} â†’ é¢„æµ‹{y_pred_best[idx]}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.5 æ¨¡å‹æ€§èƒ½åˆ†æ\n",
    "print(\"\\n=== æ¨¡å‹æ€§èƒ½åˆ†æ ===\")\n",
    "\n",
    "# è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test_mnist, y_pred_best, average='weighted')\n",
    "recall = recall_score(y_test_mnist, y_pred_best, average='weighted')\n",
    "f1 = f1_score(y_test_mnist, y_pred_best, average='weighted')\n",
    "\n",
    "performance_metrics = {\n",
    "    'å‡†ç¡®ç‡ (Accuracy)': accuracy_best,\n",
    "    'ç²¾ç¡®ç‡ (Precision)': precision,\n",
    "    'å¬å›ç‡ (Recall)': recall,\n",
    "    'F1 åˆ†æ•°': f1\n",
    "}\n",
    "\n",
    "print(\"æ•´ä½“æ€§èƒ½æŒ‡æ ‡:\")\n",
    "for metric, value in performance_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# è®­ç»ƒæ—¶é—´å¯¹æ¯”\n",
    "print(f\"\\nè®¡ç®—æ•ˆç‡:\")\n",
    "print(f\"é»˜è®¤å‚æ•°è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’\")\n",
    "print(f\"ç½‘æ ¼æœç´¢æ—¶é—´: {search_time:.2f} ç§’\")\n",
    "print(f\"æ€§èƒ½æå‡: {((accuracy_best - accuracy_default) / accuracy_default * 100):.2f}%\")\n",
    "\n",
    "print(\"\\nSVMåœ¨MNISTä¸Šçš„å…³é”®å‘ç°:\")\n",
    "print(\"âœ“ SVMåœ¨é«˜ç»´å›¾åƒæ•°æ®ä¸Šè¡¨ç°ä¼˜ç§€\")\n",
    "print(\"âœ“ RBFæ ¸å‡½æ•°é€šå¸¸åœ¨å›¾åƒåˆ†ç±»ä¸­æ•ˆæœæœ€å¥½\")\n",
    "print(\"âœ“ å‚æ•°ä¼˜åŒ–å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½\")\n",
    "print(\"âœ“ SVMå¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿï¼Œéœ€è¦è¿›è¡Œé¢„å¤„ç†\")\n",
    "print(\"âœ“ æ”¯æŒå‘é‡çš„æ•°é‡åæ˜ äº†æ•°æ®çš„å¤æ‚åº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529a6a3",
   "metadata": {},
   "source": [
    "## 4. å¤šç§åˆ†ç±»ç®—æ³•å¯¹æ¯”\n",
    "\n",
    "é™¤äº†SVMï¼Œscikit-learnè¿˜æä¾›äº†å¤šç§å¼ºå¤§çš„åˆ†ç±»ç®—æ³•ã€‚æˆ‘ä»¬å°†åœ¨MNISTæ•°æ®é›†ä¸Šæ¯”è¾ƒä¸åŒç®—æ³•çš„æ€§èƒ½ï¼Œå¸®åŠ©ä½ ç†è§£å„ç®—æ³•çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad07354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 å‡†å¤‡å¤šä¸ªåˆ†ç±»å™¨\n",
    "print(\"=== å¤šç§åˆ†ç±»ç®—æ³•å¯¹æ¯” ===\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ä½¿ç”¨ä¹‹å‰çš„MNISTå­é›†æ•°æ®\n",
    "print(f\"ä½¿ç”¨æ•°æ®é›†: è®­ç»ƒé›† {X_train_mnist.shape}, æµ‹è¯•é›† {X_test_mnist.shape}\")\n",
    "\n",
    "# å®šä¹‰å¤šä¸ªåˆ†ç±»å™¨\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=300)\n",
    "}\n",
    "\n",
    "# 4.2 è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰åˆ†ç±»å™¨\n",
    "results = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"\\næ­£åœ¨è®­ç»ƒå’Œè¯„ä¼°å„ç§åˆ†ç±»å™¨...\")\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nè®­ç»ƒ {name}...\")\n",
    "    \n",
    "    # è®­ç»ƒæ—¶é—´\n",
    "    start_time = time.time()\n",
    "    classifier.fit(X_train_mnist, y_train_mnist)\n",
    "    training_time = time.time() - start_time\n",
    "    training_times[name] = training_time\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    y_pred = classifier.predict(X_test_mnist)\n",
    "    \n",
    "    # è¯„ä¼°æŒ‡æ ‡\n",
    "    accuracy = accuracy_score(y_test_mnist, y_pred)\n",
    "    precision = precision_score(y_test_mnist, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_mnist, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_mnist, y_pred, average='weighted')\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "    print(f\"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’\")\n",
    "\n",
    "# 4.3 ç»“æœå¯è§†åŒ–å’Œåˆ†æ\n",
    "print(\"\\n=== ç®—æ³•æ€§èƒ½å¯¹æ¯”åˆ†æ ===\")\n",
    "\n",
    "# åˆ›å»ºç»“æœDataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"æ‰€æœ‰ç®—æ³•æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(results_df)\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. å‡†ç¡®ç‡å¯¹æ¯”\n",
    "algorithms = list(results.keys())\n",
    "accuracies = [results[alg]['accuracy'] for alg in algorithms]\n",
    "\n",
    "axes[0, 0].bar(range(len(algorithms)), accuracies)\n",
    "axes[0, 0].set_title('å‡†ç¡®ç‡å¯¹æ¯”')\n",
    "axes[0, 0].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 0].set_xticks(range(len(algorithms)))\n",
    "axes[0, 0].set_xticklabels(algorithms, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. è®­ç»ƒæ—¶é—´å¯¹æ¯”\n",
    "times = [results[alg]['training_time'] for alg in algorithms]\n",
    "\n",
    "axes[0, 1].bar(range(len(algorithms)), times, color='orange')\n",
    "axes[0, 1].set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”')\n",
    "axes[0, 1].set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')\n",
    "axes[0, 1].set_xticks(range(len(algorithms)))\n",
    "axes[0, 1].set_xticklabels(algorithms, rotation=45, ha='right')\n",
    "axes[0, 1].set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. å‡†ç¡®ç‡vsè®­ç»ƒæ—¶é—´æ•£ç‚¹å›¾\n",
    "axes[0, 2].scatter(times, accuracies, s=100, alpha=0.7)\n",
    "for i, alg in enumerate(algorithms):\n",
    "    axes[0, 2].annotate(alg, (times[i], accuracies[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0, 2].set_xlabel('è®­ç»ƒæ—¶é—´ (ç§’)')\n",
    "axes[0, 2].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 2].set_title('å‡†ç¡®ç‡ vs è®­ç»ƒæ—¶é—´')\n",
    "axes[0, 2].set_xscale('log')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. F1åˆ†æ•°å¯¹æ¯”\n",
    "f1_scores = [results[alg]['f1_score'] for alg in algorithms]\n",
    "\n",
    "axes[1, 0].bar(range(len(algorithms)), f1_scores, color='green')\n",
    "axes[1, 0].set_title('F1åˆ†æ•°å¯¹æ¯”')\n",
    "axes[1, 0].set_ylabel('F1åˆ†æ•°')\n",
    "axes[1, 0].set_xticks(range(len(algorithms)))\n",
    "axes[1, 0].set_xticklabels(algorithms, rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾\n",
    "from math import pi\n",
    "\n",
    "# é€‰æ‹©å‰5ä¸ªç®—æ³•è¿›è¡Œé›·è¾¾å›¾å±•ç¤º\n",
    "top_5_algorithms = sorted(algorithms, key=lambda x: results[x]['accuracy'], reverse=True)[:5]\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "# æ ‡å‡†åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´\n",
    "normalized_data = {}\n",
    "for alg in top_5_algorithms:\n",
    "    normalized_data[alg] = [results[alg][metric] for metric in metrics]\n",
    "\n",
    "# é›·è¾¾å›¾\n",
    "angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
    "angles += angles[:1]  # é—­åˆå›¾å½¢\n",
    "\n",
    "axes[1, 1].set_theta_offset(pi / 2)\n",
    "axes[1, 1].set_theta_direction(-1)\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for i, alg in enumerate(top_5_algorithms):\n",
    "    values = normalized_data[alg]\n",
    "    values += values[:1]  # é—­åˆå›¾å½¢\n",
    "    \n",
    "    axes[1, 1].plot(angles, values, 'o-', linewidth=2, label=alg, color=colors[i])\n",
    "    axes[1, 1].fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "axes[1, 1].set_xticks(angles[:-1])\n",
    "axes[1, 1].set_xticklabels(metrics)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].set_title('å‰5åç®—æ³•æ€§èƒ½é›·è¾¾å›¾')\n",
    "axes[1, 1].legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# 6. æ··æ·†çŸ©é˜µå¯¹æ¯” (é€‰æ‹©æœ€ä½³ç®—æ³•)\n",
    "best_algorithm = max(algorithms, key=lambda x: results[x]['accuracy'])\n",
    "best_classifier = classifiers[best_algorithm]\n",
    "y_pred_best = best_classifier.predict(X_test_mnist)\n",
    "cm_best = confusion_matrix(y_test_mnist, y_pred_best)\n",
    "\n",
    "im = axes[1, 2].imshow(cm_best, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[1, 2].set_title(f'æœ€ä½³ç®—æ³•æ··æ·†çŸ©é˜µ\\n({best_algorithm})')\n",
    "axes[1, 2].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "axes[1, 2].set_ylabel('çœŸå®æ ‡ç­¾')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i in range(cm_best.shape[0]):\n",
    "    for j in range(cm_best.shape[1]):\n",
    "        axes[1, 2].text(j, i, format(cm_best[i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if cm_best[i, j] > cm_best.max() / 2 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.4 ç®—æ³•ç‰¹ç‚¹æ€»ç»“\n",
    "print(\"\\n=== å„ç®—æ³•ç‰¹ç‚¹æ€»ç»“ ===\")\n",
    "\n",
    "algorithm_characteristics = {\n",
    "    'Logistic Regression': {\n",
    "        'advantages': ['å¿«é€Ÿè®­ç»ƒ', 'æ¦‚ç‡è¾“å‡º', 'çº¿æ€§å¯è§£é‡Š'],\n",
    "        'disadvantages': ['å‡è®¾çº¿æ€§å…³ç³»', 'å¯¹ç‰¹å¾å·¥ç¨‹æ•æ„Ÿ'],\n",
    "        'best_for': 'çº¿æ€§å¯åˆ†é—®é¢˜ã€éœ€è¦æ¦‚ç‡è¾“å‡º'\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'advantages': ['é«˜åº¦å¯è§£é‡Š', 'æ— éœ€ç‰¹å¾ç¼©æ”¾', 'å¤„ç†éçº¿æ€§'],\n",
    "        'disadvantages': ['å®¹æ˜“è¿‡æ‹Ÿåˆ', 'å¯¹æ•°æ®å˜åŒ–æ•æ„Ÿ'],\n",
    "        'best_for': 'éœ€è¦å¯è§£é‡Šæ€§çš„é—®é¢˜'\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'advantages': ['å‡å°‘è¿‡æ‹Ÿåˆ', 'ç‰¹å¾é‡è¦æ€§', 'é²æ£’æ€§å¼º'],\n",
    "        'disadvantages': ['å†…å­˜æ¶ˆè€—å¤§', 'å¯è§£é‡Šæ€§é™ä½'],\n",
    "        'best_for': 'é€šç”¨åˆ†ç±»é—®é¢˜ã€ç‰¹å¾é€‰æ‹©'\n",
    "    },\n",
    "    'SVM (RBF)': {\n",
    "        'advantages': ['é«˜ç»´æ•°æ®ä¼˜ç§€', 'å†…å­˜é«˜æ•ˆ', 'çµæ´»æ ¸å‡½æ•°'],\n",
    "        'disadvantages': ['è®­ç»ƒæ—¶é—´é•¿', 'å‚æ•°æ•æ„Ÿ', 'æ— æ¦‚ç‡è¾“å‡º'],\n",
    "        'best_for': 'é«˜ç»´æ•°æ®ã€å°åˆ°ä¸­ç­‰æ ·æœ¬é‡'\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'advantages': ['ç®€å•ç›´è§‚', 'æ— è®­ç»ƒæ—¶é—´', 'é€‚åº”å±€éƒ¨æ¨¡å¼'],\n",
    "        'disadvantages': ['é¢„æµ‹é€Ÿåº¦æ…¢', 'å¯¹ç»´åº¦è¯…å’’æ•æ„Ÿ', 'éœ€è¦ç‰¹å¾ç¼©æ”¾'],\n",
    "        'best_for': 'å°æ•°æ®é›†ã€å±€éƒ¨æ¨¡å¼é‡è¦'\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'advantages': ['é«˜é¢„æµ‹ç²¾åº¦', 'å¤„ç†å¤æ‚æ¨¡å¼', 'ç‰¹å¾é‡è¦æ€§'],\n",
    "        'disadvantages': ['è®­ç»ƒæ—¶é—´é•¿', 'å®¹æ˜“è¿‡æ‹Ÿåˆ', 'å‚æ•°å¤š'],\n",
    "        'best_for': 'ç»“æ„åŒ–æ•°æ®ç«èµ›ã€é«˜ç²¾åº¦è¦æ±‚'\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'advantages': ['è®­ç»ƒæå¿«', 'å°æ•°æ®è¡¨ç°å¥½', 'å¤šåˆ†ç±»è‡ªç„¶'],\n",
    "        'disadvantages': ['ç‰¹å¾ç‹¬ç«‹å‡è®¾', 'æ•°å€¼ç‰¹å¾éœ€è¦å‡è®¾åˆ†å¸ƒ'],\n",
    "        'best_for': 'æ–‡æœ¬åˆ†ç±»ã€å°æ•°æ®é›†'\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'advantages': ['å­¦ä¹ å¤æ‚æ¨¡å¼', 'è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ', 'çµæ´»æ¶æ„'],\n",
    "        'disadvantages': ['éœ€è¦å¤§é‡æ•°æ®', 'é»‘ç›’æ¨¡å‹', 'è®­ç»ƒæ—¶é—´é•¿'],\n",
    "        'best_for': 'å¤§æ•°æ®é›†ã€å¤æ‚éçº¿æ€§æ¨¡å¼'\n",
    "    }\n",
    "}\n",
    "\n",
    "for alg_name in algorithms:\n",
    "    if alg_name in algorithm_characteristics:\n",
    "        char = algorithm_characteristics[alg_name]\n",
    "        print(f\"\\n{alg_name}:\")\n",
    "        print(f\"  å‡†ç¡®ç‡: {results[alg_name]['accuracy']:.4f}\")\n",
    "        print(f\"  ä¼˜ç‚¹: {', '.join(char['advantages'])}\")\n",
    "        print(f\"  ç¼ºç‚¹: {', '.join(char['disadvantages'])}\")\n",
    "        print(f\"  é€‚ç”¨åœºæ™¯: {char['best_for']}\")\n",
    "\n",
    "print(f\"\\nåœ¨MNISTæ‰‹å†™æ•°å­—è¯†åˆ«ä»»åŠ¡ä¸­:\")\n",
    "print(f\"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_algorithm} ({results[best_algorithm]['accuracy']:.4f})\")\n",
    "print(f\"âš¡ æœ€å¿«è®­ç»ƒ: {min(algorithms, key=lambda x: results[x]['training_time'])} ({min(training_times.values()):.2f}ç§’)\")\n",
    "\n",
    "fastest_alg = min(algorithms, key=lambda x: results[x]['training_time'])\n",
    "best_tradeoff = max(algorithms, key=lambda x: results[x]['accuracy'] / results[x]['training_time'])\n",
    "\n",
    "print(f\"âš–ï¸  æœ€ä½³æ€§ä»·æ¯”: {best_tradeoff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d728c",
   "metadata": {},
   "source": [
    "## 5. å›å½’ç®—æ³•è¯¦è§£\n",
    "\n",
    "å›å½’ç”¨äºé¢„æµ‹è¿ç»­æ•°å€¼ï¼Œæ˜¯æœºå™¨å­¦ä¹ çš„å¦ä¸€å¤§ç±»é—®é¢˜ã€‚æˆ‘ä»¬å°†å­¦ä¹ å„ç§å›å½’ç®—æ³•ï¼Œå¹¶ç”¨æˆ¿ä»·é¢„æµ‹ä½œä¸ºå®æˆ˜æ¡ˆä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 åŠ è½½å’Œå‡†å¤‡å›å½’æ•°æ®\n",
    "print(\"=== å›å½’ç®—æ³•è¯¦è§£ ===\")\n",
    "\n",
    "from sklearn.datasets import load_boston, make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ç”±äºBostonæˆ¿ä»·æ•°æ®é›†çš„ä¸€äº›äº‰è®®ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç±»ä¼¼çš„åˆæˆæ•°æ®é›†\n",
    "print(\"ç”Ÿæˆæˆ¿ä»·é¢„æµ‹æ•°æ®é›†...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ç”Ÿæˆæˆ¿ä»·ç›¸å…³ç‰¹å¾\n",
    "n_samples = 1000\n",
    "n_features = 8\n",
    "\n",
    "# åˆ›å»ºç‰¹å¾\n",
    "X_house, y_house = make_regression(\n",
    "    n_samples=n_samples, \n",
    "    n_features=n_features, \n",
    "    noise=10, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ›´æœ‰æ„ä¹‰çš„ç‰¹å¾å\n",
    "feature_names = [\n",
    "    'æˆ¿å±‹é¢ç§¯', 'æˆ¿é—´æ•°é‡', 'æµ´å®¤æ•°é‡', 'æ¥¼å±‚', \n",
    "    'å»ºé€ å¹´ä»½', 'åœ°ç†ä½ç½®è¯„åˆ†', 'å­¦åŒºè¯„åˆ†', 'äº¤é€šä¾¿åˆ©æ€§'\n",
    "]\n",
    "\n",
    "# è½¬æ¢ä¸ºDataFrameä»¥ä¾¿åˆ†æ\n",
    "house_data = pd.DataFrame(X_house, columns=feature_names)\n",
    "house_data['æˆ¿ä»·'] = y_house\n",
    "\n",
    "# ä¸ºäº†æ›´çœŸå®ï¼Œè°ƒæ•´æ•°æ®èŒƒå›´\n",
    "house_data['æˆ¿å±‹é¢ç§¯'] = house_data['æˆ¿å±‹é¢ç§¯'] * 10 + 100  # 100-200 å¹³ç±³\n",
    "house_data['æˆ¿é—´æ•°é‡'] = np.abs(house_data['æˆ¿é—´æ•°é‡']) + 2   # 2-5æˆ¿é—´\n",
    "house_data['æµ´å®¤æ•°é‡'] = np.abs(house_data['æµ´å®¤æ•°é‡']) + 1   # 1-3æµ´å®¤  \n",
    "house_data['æ¥¼å±‚'] = np.abs(house_data['æ¥¼å±‚']) % 20 + 1      # 1-20å±‚\n",
    "house_data['å»ºé€ å¹´ä»½'] = 2020 - (np.abs(house_data['å»ºé€ å¹´ä»½']) % 30)  # 1990-2020\n",
    "house_data['æˆ¿ä»·'] = house_data['æˆ¿ä»·'] * 0.01 + 50  # è°ƒæ•´ä»·æ ¼èŒƒå›´\n",
    "\n",
    "print(f\"æˆ¿ä»·æ•°æ®é›†ä¿¡æ¯:\")\n",
    "print(f\"æ ·æœ¬æ•°é‡: {len(house_data)}\")\n",
    "print(f\"ç‰¹å¾æ•°é‡: {len(feature_names)}\")\n",
    "print(\"\\nå‰5è¡Œæ•°æ®:\")\n",
    "print(house_data.head())\n",
    "\n",
    "print(\"\\næ•°æ®ç»Ÿè®¡:\")\n",
    "print(house_data.describe().round(2))\n",
    "\n",
    "# ç›¸å…³æ€§åˆ†æ\n",
    "correlation_matrix = house_data.corr()\n",
    "\n",
    "# å¯è§†åŒ–æ•°æ®åˆ†å¸ƒå’Œç›¸å…³æ€§\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. æˆ¿ä»·åˆ†å¸ƒ\n",
    "axes[0, 0].hist(house_data['æˆ¿ä»·'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('æˆ¿ä»·åˆ†å¸ƒ')\n",
    "axes[0, 0].set_xlabel('æˆ¿ä»·')\n",
    "axes[0, 0].set_ylabel('é¢‘æ¬¡')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾\n",
    "im = axes[0, 1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "axes[0, 1].set_title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')\n",
    "axes[0, 1].set_xticks(range(len(house_data.columns)))\n",
    "axes[0, 1].set_yticks(range(len(house_data.columns)))\n",
    "axes[0, 1].set_xticklabels(house_data.columns, rotation=45, ha='right')\n",
    "axes[0, 1].set_yticklabels(house_data.columns)\n",
    "\n",
    "# æ·»åŠ ç›¸å…³æ€§æ•°å€¼\n",
    "for i in range(len(house_data.columns)):\n",
    "    for j in range(len(house_data.columns)):\n",
    "        text = axes[0, 1].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\" if abs(correlation_matrix.iloc[i, j]) < 0.5 else \"white\")\n",
    "\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "# 3. æˆ¿å±‹é¢ç§¯vsæˆ¿ä»·æ•£ç‚¹å›¾\n",
    "axes[0, 2].scatter(house_data['æˆ¿å±‹é¢ç§¯'], house_data['æˆ¿ä»·'], alpha=0.6)\n",
    "axes[0, 2].set_xlabel('æˆ¿å±‹é¢ç§¯')\n",
    "axes[0, 2].set_ylabel('æˆ¿ä»·')\n",
    "axes[0, 2].set_title('æˆ¿å±‹é¢ç§¯ vs æˆ¿ä»·')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. æˆ¿é—´æ•°é‡vsæˆ¿ä»·ç®±å‹å›¾\n",
    "room_groups = house_data.groupby(house_data['æˆ¿é—´æ•°é‡'].astype(int))\n",
    "room_prices = [group['æˆ¿ä»·'].values for name, group in room_groups]\n",
    "room_labels = [str(int(name)) for name, group in room_groups]\n",
    "\n",
    "axes[1, 0].boxplot(room_prices, labels=room_labels)\n",
    "axes[1, 0].set_xlabel('æˆ¿é—´æ•°é‡')\n",
    "axes[1, 0].set_ylabel('æˆ¿ä»·')\n",
    "axes[1, 0].set_title('æˆ¿é—´æ•°é‡ vs æˆ¿ä»·åˆ†å¸ƒ')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. å»ºé€ å¹´ä»½vsæˆ¿ä»·\n",
    "axes[1, 1].scatter(house_data['å»ºé€ å¹´ä»½'], house_data['æˆ¿ä»·'], alpha=0.6, color='green')\n",
    "axes[1, 1].set_xlabel('å»ºé€ å¹´ä»½')\n",
    "axes[1, 1].set_ylabel('æˆ¿ä»·')\n",
    "axes[1, 1].set_title('å»ºé€ å¹´ä»½ vs æˆ¿ä»·')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. ç‰¹å¾é‡è¦æ€§åˆæ­¥åˆ†æï¼ˆç›¸å…³ç³»æ•°ï¼‰\n",
    "feature_importance = correlation_matrix['æˆ¿ä»·'].abs().sort_values(ascending=True)[:-1]\n",
    "axes[1, 2].barh(range(len(feature_importance)), feature_importance.values)\n",
    "axes[1, 2].set_yticks(range(len(feature_importance)))\n",
    "axes[1, 2].set_yticklabels(feature_importance.index)\n",
    "axes[1, 2].set_xlabel('ä¸æˆ¿ä»·çš„ç›¸å…³ç³»æ•°(ç»å¯¹å€¼)')\n",
    "axes[1, 2].set_title('ç‰¹å¾é‡è¦æ€§(ç›¸å…³æ€§)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "X_house_final = house_data[feature_names]\n",
    "y_house_final = house_data['æˆ¿ä»·']\n",
    "\n",
    "# æ•°æ®åˆ†å‰²\n",
    "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "    X_house_final, y_house_final, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\næ•°æ®åˆ†å‰²ç»“æœ:\")\n",
    "print(f\"è®­ç»ƒé›†: {X_train_house.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test_house.shape}\")\n",
    "\n",
    "# 5.2 å¤šç§å›å½’ç®—æ³•å®ç°å’Œå¯¹æ¯”\n",
    "print(\"\\n=== å¤šç§å›å½’ç®—æ³•å¯¹æ¯” ===\")\n",
    "\n",
    "# ç‰¹å¾æ ‡å‡†åŒ–\n",
    "scaler_house = StandardScaler()\n",
    "X_train_house_scaled = scaler_house.fit_transform(X_train_house)\n",
    "X_test_house_scaled = scaler_house.transform(X_test_house)\n",
    "\n",
    "# å®šä¹‰å›å½’æ¨¡å‹\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Support Vector Regression': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# å¤šé¡¹å¼å›å½’ï¼ˆä½¿ç”¨Pipelineï¼‰\n",
    "poly_regression = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "regressors['Polynomial Regression'] = poly_regression\n",
    "\n",
    "# è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰å›å½’æ¨¡å‹\n",
    "regression_results = {}\n",
    "regression_times = {}\n",
    "\n",
    "print(\"\\næ­£åœ¨è®­ç»ƒå’Œè¯„ä¼°å„ç§å›å½’æ¨¡å‹...\")\n",
    "for name, regressor in regressors.items():\n",
    "    print(f\"\\nè®­ç»ƒ {name}...\")\n",
    "    \n",
    "    # é€‰æ‹©æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®\n",
    "    if name in ['Ridge Regression', 'Lasso Regression', 'Elastic Net', 'Support Vector Regression']:\n",
    "        X_train_use = X_train_house_scaled\n",
    "        X_test_use = X_test_house_scaled\n",
    "    else:\n",
    "        X_train_use = X_train_house\n",
    "        X_test_use = X_test_house\n",
    "    \n",
    "    # è®­ç»ƒæ—¶é—´\n",
    "    start_time = time.time()\n",
    "    regressor.fit(X_train_use, y_train_house)\n",
    "    training_time = time.time() - start_time\n",
    "    regression_times[name] = training_time\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    y_pred_house = regressor.predict(X_test_use)\n",
    "    \n",
    "    # è¯„ä¼°æŒ‡æ ‡\n",
    "    mae = mean_absolute_error(y_test_house, y_pred_house)\n",
    "    mse = mean_squared_error(y_test_house, y_pred_house)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_house, y_pred_house)\n",
    "    \n",
    "    regression_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'RÂ²': r2,\n",
    "        'Training_Time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  RÂ² åˆ†æ•°: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  è®­ç»ƒæ—¶é—´: {training_time:.4f}ç§’\")\n",
    "\n",
    "# 5.3 å›å½’ç»“æœå¯è§†åŒ–å’Œåˆ†æ\n",
    "print(\"\\n=== å›å½’ç»“æœåˆ†æ ===\")\n",
    "\n",
    "# åˆ›å»ºç»“æœDataFrame\n",
    "regression_df = pd.DataFrame(regression_results).T\n",
    "print(\"æ‰€æœ‰å›å½’ç®—æ³•æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(regression_df.round(4))\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. RÂ²åˆ†æ•°å¯¹æ¯”\n",
    "models = list(regression_results.keys())\n",
    "r2_scores = [regression_results[model]['RÂ²'] for model in models]\n",
    "\n",
    "axes[0, 0].bar(range(len(models)), r2_scores, color='lightblue')\n",
    "axes[0, 0].set_title('RÂ² åˆ†æ•°å¯¹æ¯”')\n",
    "axes[0, 0].set_ylabel('RÂ² åˆ†æ•°')\n",
    "axes[0, 0].set_xticks(range(len(models)))\n",
    "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, v in enumerate(r2_scores):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. RMSEå¯¹æ¯”\n",
    "rmse_scores = [regression_results[model]['RMSE'] for model in models]\n",
    "\n",
    "axes[0, 1].bar(range(len(models)), rmse_scores, color='lightcoral')\n",
    "axes[0, 1].set_title('RMSEå¯¹æ¯” (è¶Šå°è¶Šå¥½)')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_xticks(range(len(models)))\n",
    "axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”\n",
    "training_times_reg = [regression_results[model]['Training_Time'] for model in models]\n",
    "\n",
    "axes[0, 2].bar(range(len(models)), training_times_reg, color='lightgreen')\n",
    "axes[0, 2].set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”')\n",
    "axes[0, 2].set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')\n",
    "axes[0, 2].set_xticks(range(len(models)))\n",
    "axes[0, 2].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0, 2].set_yscale('log')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. æœ€ä½³æ¨¡å‹çš„é¢„æµ‹vsçœŸå®å€¼\n",
    "best_model_name = max(models, key=lambda x: regression_results[x]['RÂ²'])\n",
    "best_regressor = regressors[best_model_name]\n",
    "\n",
    "# é‡æ–°é¢„æµ‹ç”¨äºç»˜å›¾\n",
    "if best_model_name in ['Ridge Regression', 'Lasso Regression', 'Elastic Net', 'Support Vector Regression']:\n",
    "    y_pred_best = best_regressor.predict(X_test_house_scaled)\n",
    "else:\n",
    "    y_pred_best = best_regressor.predict(X_test_house)\n",
    "\n",
    "axes[1, 0].scatter(y_test_house, y_pred_best, alpha=0.6)\n",
    "axes[1, 0].plot([y_test_house.min(), y_test_house.max()], \n",
    "               [y_test_house.min(), y_test_house.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('çœŸå®æˆ¿ä»·')\n",
    "axes[1, 0].set_ylabel('é¢„æµ‹æˆ¿ä»·')\n",
    "axes[1, 0].set_title(f'æœ€ä½³æ¨¡å‹é¢„æµ‹æ•ˆæœ\\n({best_model_name})')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. æ®‹å·®åˆ†æ\n",
    "residuals = y_test_house - y_pred_best\n",
    "axes[1, 1].scatter(y_pred_best, residuals, alpha=0.6)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('é¢„æµ‹å€¼')\n",
    "axes[1, 1].set_ylabel('æ®‹å·®')\n",
    "axes[1, 1].set_title('æ®‹å·®å›¾')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. æ€§èƒ½vså¤æ‚åº¦åˆ†æ\n",
    "complexity_scores = {\n",
    "    'Linear Regression': 1,\n",
    "    'Ridge Regression': 2,\n",
    "    'Lasso Regression': 2,\n",
    "    'Elastic Net': 3,\n",
    "    'Polynomial Regression': 4,\n",
    "    'Decision Tree': 5,\n",
    "    'Random Forest': 7,\n",
    "    'Gradient Boosting': 8,\n",
    "    'Support Vector Regression': 6\n",
    "}\n",
    "\n",
    "complexity_vals = [complexity_scores[model] for model in models]\n",
    "axes[1, 2].scatter(complexity_vals, r2_scores, s=100, alpha=0.7)\n",
    "for i, model in enumerate(models):\n",
    "    axes[1, 2].annotate(model, (complexity_vals[i], r2_scores[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1, 2].set_xlabel('æ¨¡å‹å¤æ‚åº¦')\n",
    "axes[1, 2].set_ylabel('RÂ² åˆ†æ•°')\n",
    "axes[1, 2].set_title('æ¨¡å‹å¤æ‚åº¦ vs æ€§èƒ½')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5.4 å›å½’ç®—æ³•ç‰¹ç‚¹æ€»ç»“\n",
    "print(\"\\n=== å›å½’ç®—æ³•ç‰¹ç‚¹æ€»ç»“ ===\")\n",
    "\n",
    "regression_characteristics = {\n",
    "    'Linear Regression': {\n",
    "        'description': 'æœ€ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹',\n",
    "        'advantages': ['å¯è§£é‡Šæ€§å¼º', 'è®­ç»ƒé€Ÿåº¦å¿«', 'æ— è¶…å‚æ•°'],\n",
    "        'disadvantages': ['å‡è®¾çº¿æ€§å…³ç³»', 'å¯¹å¤šé‡å…±çº¿æ€§æ•æ„Ÿ'],\n",
    "        'when_to_use': 'ç‰¹å¾æ•°é‡è¾ƒå°‘ã€çº¿æ€§å…³ç³»æ˜æ˜¾'\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'description': 'L2æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’',\n",
    "        'advantages': ['é˜²æ­¢è¿‡æ‹Ÿåˆ', 'å¤„ç†å¤šé‡å…±çº¿æ€§', 'ç¨³å®šæ€§å¥½'],\n",
    "        'disadvantages': ['ç³»æ•°ä¸ä¼šä¸ºé›¶', 'ä»å‡è®¾çº¿æ€§å…³ç³»'],\n",
    "        'when_to_use': 'ç‰¹å¾æ•°é‡å¤šã€å­˜åœ¨å¤šé‡å…±çº¿æ€§'\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'description': 'L1æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’',\n",
    "        'advantages': ['è‡ªåŠ¨ç‰¹å¾é€‰æ‹©', 'ç¨€ç–è§£', 'é˜²æ­¢è¿‡æ‹Ÿåˆ'],\n",
    "        'disadvantages': ['å¯èƒ½é€‰æ‹©é”™è¯¯ç‰¹å¾', 'ä¸ç¨³å®š'],\n",
    "        'when_to_use': 'éœ€è¦ç‰¹å¾é€‰æ‹©ã€ç¨€ç–æ¨¡å‹'\n",
    "    },\n",
    "    'Elastic Net': {\n",
    "        'description': 'ç»“åˆL1å’ŒL2æ­£åˆ™åŒ–',\n",
    "        'advantages': ['å¹³è¡¡Ridgeå’ŒLasso', 'ç¨³å®šçš„ç‰¹å¾é€‰æ‹©'],\n",
    "        'disadvantages': ['éœ€è¦è°ƒèŠ‚ä¸¤ä¸ªå‚æ•°', 'è®¡ç®—å¤æ‚'],\n",
    "        'when_to_use': 'ç‰¹å¾æ•°é‡å¾ˆå¤šã€éœ€è¦ç¨³å®šçš„ç‰¹å¾é€‰æ‹©'\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'description': 'åŸºäºå†³ç­–æ ‘çš„å›å½’',\n",
    "        'advantages': ['éçº¿æ€§å»ºæ¨¡', 'å¯è§£é‡Šæ€§', 'æ— éœ€ç‰¹å¾ç¼©æ”¾'],\n",
    "        'disadvantages': ['å®¹æ˜“è¿‡æ‹Ÿåˆ', 'å¯¹æ•°æ®å˜åŒ–æ•æ„Ÿ'],\n",
    "        'when_to_use': 'éçº¿æ€§å…³ç³»ã€éœ€è¦å¯è§£é‡Šæ€§'\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'description': 'å¤šä¸ªå†³ç­–æ ‘çš„é›†æˆ',\n",
    "        'advantages': ['å‡å°‘è¿‡æ‹Ÿåˆ', 'ç‰¹å¾é‡è¦æ€§', 'é²æ£’æ€§'],\n",
    "        'disadvantages': ['å¯è§£é‡Šæ€§é™ä½', 'å†…å­˜æ¶ˆè€—å¤§'],\n",
    "        'when_to_use': 'é€šç”¨å›å½’é—®é¢˜ã€ç‰¹å¾é‡è¦æ€§åˆ†æ'\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name in models[:6]:  # å±•ç¤ºå‰6ä¸ªä¸»è¦ç®—æ³•\n",
    "    if model_name in regression_characteristics:\n",
    "        char = regression_characteristics[model_name]\n",
    "        result = regression_results[model_name]\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  æè¿°: {char['description']}\")\n",
    "        print(f\"  RÂ² åˆ†æ•°: {result['RÂ²']:.4f}\")\n",
    "        print(f\"  RMSE: {result['RMSE']:.2f}\")\n",
    "        print(f\"  ä¼˜ç‚¹: {', '.join(char['advantages'])}\")\n",
    "        print(f\"  ç¼ºç‚¹: {', '.join(char['disadvantages'])}\")\n",
    "        print(f\"  é€‚ç”¨åœºæ™¯: {char['when_to_use']}\")\n",
    "\n",
    "print(f\"\\næˆ¿ä»·é¢„æµ‹ä»»åŠ¡æ€»ç»“:\")\n",
    "print(f\"ğŸ† æœ€ä½³RÂ²åˆ†æ•°: {best_model_name} ({regression_results[best_model_name]['RÂ²']:.4f})\")\n",
    "fastest_reg = min(models, key=lambda x: regression_results[x]['Training_Time'])\n",
    "print(f\"âš¡ æœ€å¿«è®­ç»ƒ: {fastest_reg} ({regression_results[fastest_reg]['Training_Time']:.4f}ç§’)\")\n",
    "\n",
    "print(f\"\\nå›å½’è¯„ä¼°æŒ‡æ ‡è¯´æ˜:\")\n",
    "print(f\"â€¢ MAE (å¹³å‡ç»å¯¹è¯¯å·®): é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®å¼‚çš„å¹³å‡å€¼\")\n",
    "print(f\"â€¢ RMSE (å‡æ–¹æ ¹è¯¯å·®): å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿï¼Œå•ä½ä¸ç›®æ ‡å˜é‡ç›¸åŒ\")\n",
    "print(f\"â€¢ RÂ² (å†³å®šç³»æ•°): æ¨¡å‹è§£é‡Šæ–¹å·®çš„æ¯”ä¾‹ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25d9af",
   "metadata": {},
   "source": [
    "## 6. èšç±»ç®—æ³•è¯¦è§£\n",
    "\n",
    "èšç±»æ˜¯æ— ç›‘ç£å­¦ä¹ çš„é‡è¦åˆ†æ”¯ï¼Œç”¨äºå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼å’Œç»“æ„ã€‚æˆ‘ä»¬å°†å­¦ä¹ å„ç§èšç±»ç®—æ³•ï¼Œå¹¶åœ¨å®¢æˆ·ç»†åˆ†åœºæ™¯ä¸­è¿›è¡Œå®æˆ˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 ç”Ÿæˆå®¢æˆ·æ•°æ®ç”¨äºèšç±»åˆ†æ\n",
    "print(\"=== èšç±»ç®—æ³•è¯¦è§£ ===\")\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# ç”Ÿæˆå®¢æˆ·æ¶ˆè´¹æ•°æ®\n",
    "print(\"ç”Ÿæˆå®¢æˆ·æ¶ˆè´¹è¡Œä¸ºæ•°æ®...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# åˆ›å»ºä¸‰ä¸ªä¸åŒç±»å‹çš„å®¢æˆ·ç¾¤ä½“\n",
    "n_customers = 300\n",
    "\n",
    "# é«˜ä»·å€¼å®¢æˆ· (æ”¶å…¥é«˜ï¼Œæ¶ˆè´¹å¤š)\n",
    "high_value = np.random.multivariate_normal(\n",
    "    mean=[80, 120], cov=[[100, 50], [50, 200]], size=100\n",
    ")\n",
    "\n",
    "# ä¸­ç­‰ä»·å€¼å®¢æˆ· (æ”¶å…¥ä¸­ç­‰ï¼Œæ¶ˆè´¹ä¸­ç­‰)\n",
    "medium_value = np.random.multivariate_normal(\n",
    "    mean=[50, 70], cov=[[80, 30], [30, 120]], size=100\n",
    ")\n",
    "\n",
    "# ä½ä»·å€¼å®¢æˆ· (æ”¶å…¥ä½ï¼Œæ¶ˆè´¹å°‘)\n",
    "low_value = np.random.multivariate_normal(\n",
    "    mean=[25, 35], cov=[[50, 20], [20, 80]], size=100\n",
    ")\n",
    "\n",
    "# åˆå¹¶æ•°æ®\n",
    "X_customers = np.vstack([high_value, medium_value, low_value])\n",
    "true_labels = np.hstack([np.zeros(100), np.ones(100), np.full(100, 2)])\n",
    "\n",
    "# æ·»åŠ æ›´å¤šç‰¹å¾\n",
    "additional_features = np.random.randn(n_customers, 3)  # æ·»åŠ 3ä¸ªé¢å¤–ç‰¹å¾\n",
    "X_customers = np.hstack([X_customers, additional_features])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "feature_names_cluster = ['å¹´æ”¶å…¥(åƒ)', 'å¹´æ¶ˆè´¹(åƒ)', 'è´­ä¹°é¢‘ç‡', 'å®¢æˆ·æ»¡æ„åº¦', 'æ¨èæ„æ„¿']\n",
    "customer_data = pd.DataFrame(X_customers, columns=feature_names_cluster)\n",
    "\n",
    "# ç¡®ä¿æ•°æ®ä¸ºæ­£å€¼å¹¶è°ƒæ•´èŒƒå›´\n",
    "customer_data['å¹´æ”¶å…¥(åƒ)'] = np.abs(customer_data['å¹´æ”¶å…¥(åƒ)']) + 20\n",
    "customer_data['å¹´æ¶ˆè´¹(åƒ)'] = np.abs(customer_data['å¹´æ¶ˆè´¹(åƒ)']) + 10\n",
    "customer_data['è´­ä¹°é¢‘ç‡'] = np.abs(customer_data['è´­ä¹°é¢‘ç‡']) * 5 + 1\n",
    "customer_data['å®¢æˆ·æ»¡æ„åº¦'] = (customer_data['å®¢æˆ·æ»¡æ„åº¦'] + 3) * 1.5  # 1-10åˆ†\n",
    "customer_data['æ¨èæ„æ„¿'] = (customer_data['æ¨èæ„æ„¿'] + 3) * 1.5    # 1-10åˆ†\n",
    "\n",
    "print(f\"å®¢æˆ·æ•°æ®æ¦‚è§ˆ:\")\n",
    "print(f\"æ ·æœ¬æ•°é‡: {len(customer_data)}\")\n",
    "print(f\"ç‰¹å¾æ•°é‡: {len(feature_names_cluster)}\")\n",
    "print(\"\\nå‰5è¡Œæ•°æ®:\")\n",
    "print(customer_data.head())\n",
    "\n",
    "print(\"\\næ•°æ®ç»Ÿè®¡:\")\n",
    "print(customer_data.describe().round(2))\n",
    "\n",
    "# æ•°æ®å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. æ”¶å…¥vsæ¶ˆè´¹æ•£ç‚¹å›¾\n",
    "scatter = axes[0, 0].scatter(customer_data['å¹´æ”¶å…¥(åƒ)'], customer_data['å¹´æ¶ˆè´¹(åƒ)'], \n",
    "                           c=true_labels, cmap='viridis', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('å¹´æ”¶å…¥(åƒ)')\n",
    "axes[0, 0].set_ylabel('å¹´æ¶ˆè´¹(åƒ)')\n",
    "axes[0, 0].set_title('å®¢æˆ·æ”¶å…¥vsæ¶ˆè´¹åˆ†å¸ƒ (çœŸå®åˆ†ç»„)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0, 0])\n",
    "\n",
    "# 2. å„ç‰¹å¾åˆ†å¸ƒ\n",
    "for i, feature in enumerate(feature_names_cluster[:4]):\n",
    "    row, col = divmod(i+1, 3)\n",
    "    if row < 2:\n",
    "        axes[row, col].hist(customer_data[feature], bins=20, alpha=0.7)\n",
    "        axes[row, col].set_title(f'{feature}åˆ†å¸ƒ')\n",
    "        axes[row, col].set_xlabel(feature)\n",
    "        axes[row, col].set_ylabel('é¢‘æ¬¡')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. ç›¸å…³æ€§çŸ©é˜µ\n",
    "correlation_matrix_cluster = customer_data.corr()\n",
    "im = axes[1, 2].imshow(correlation_matrix_cluster, cmap='coolwarm', aspect='auto')\n",
    "axes[1, 2].set_title('å®¢æˆ·ç‰¹å¾ç›¸å…³æ€§')\n",
    "axes[1, 2].set_xticks(range(len(feature_names_cluster)))\n",
    "axes[1, 2].set_yticks(range(len(feature_names_cluster)))\n",
    "axes[1, 2].set_xticklabels(feature_names_cluster, rotation=45, ha='right')\n",
    "axes[1, 2].set_yticklabels(feature_names_cluster)\n",
    "\n",
    "for i in range(len(feature_names_cluster)):\n",
    "    for j in range(len(feature_names_cluster)):\n",
    "        text = axes[1, 2].text(j, i, f'{correlation_matrix_cluster.iloc[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", \n",
    "                              color=\"black\" if abs(correlation_matrix_cluster.iloc[i, j]) < 0.5 else \"white\")\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ•°æ®æ ‡å‡†åŒ–ï¼ˆèšç±»ç®—æ³•å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿï¼‰\n",
    "scaler_cluster = StandardScaler()\n",
    "X_customers_scaled = scaler_cluster.fit_transform(customer_data)\n",
    "\n",
    "print(\"âœ“ å®¢æˆ·æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n",
    "\n",
    "# 6.2 K-meansèšç±»è¯¦è§£\n",
    "print(\"\\n=== K-meansèšç±»è¯¦è§£ ===\")\n",
    "\n",
    "# ç¡®å®šæœ€ä¼˜èšç±»æ•°é‡ - è‚˜éƒ¨æ³•åˆ™\n",
    "print(\"1. ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜èšç±»æ•°é‡...\")\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_customers_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_customers_scaled, kmeans.labels_))\n",
    "\n",
    "# å¯è§†åŒ–è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# è‚˜éƒ¨æ³•åˆ™\n",
    "axes[0].plot(K_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('èšç±»æ•°é‡ (K)')\n",
    "axes[0].set_ylabel('ç°‡å†…è¯¯å·®å¹³æ–¹å’Œ (Inertia)')\n",
    "axes[0].set_title('è‚˜éƒ¨æ³•åˆ™')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# è½®å»“ç³»æ•°\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('èšç±»æ•°é‡ (K)')\n",
    "axes[1].set_ylabel('è½®å»“ç³»æ•°')\n",
    "axes[1].set_title('è½®å»“ç³»æ•°æ³•')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# é€‰æ‹©æœ€ä¼˜Kå€¼\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"åŸºäºè½®å»“ç³»æ•°çš„æœ€ä¼˜èšç±»æ•°é‡: {optimal_k}\")\n",
    "\n",
    "# ä½¿ç”¨æœ€ä¼˜Kå€¼è¿›è¡ŒK-meansèšç±»\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_optimal.fit_predict(X_customers_scaled)\n",
    "\n",
    "print(f\"K-meansèšç±»ç»“æœ:\")\n",
    "print(f\"èšç±»æ•°é‡: {optimal_k}\")\n",
    "print(f\"è½®å»“ç³»æ•°: {silhouette_score(X_customers_scaled, kmeans_labels):.4f}\")\n",
    "print(f\"Calinski-HarabaszæŒ‡æ•°: {calinski_harabasz_score(X_customers_scaled, kmeans_labels):.2f}\")\n",
    "\n",
    "# 6.3 å¤šç§èšç±»ç®—æ³•å¯¹æ¯”\n",
    "print(\"\\n=== å¤šç§èšç±»ç®—æ³•å¯¹æ¯” ===\")\n",
    "\n",
    "# å®šä¹‰å¤šç§èšç±»ç®—æ³•\n",
    "clustering_algorithms = {\n",
    "    'K-Means': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
    "    'Hierarchical': AgglomerativeClustering(n_clusters=3),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'Gaussian Mixture': GaussianMixture(n_components=3, random_state=42),\n",
    "    'Spectral Clustering': SpectralClustering(n_clusters=3, random_state=42)\n",
    "}\n",
    "\n",
    "# åº”ç”¨æ‰€æœ‰èšç±»ç®—æ³•\n",
    "clustering_results = {}\n",
    "cluster_labels = {}\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"åº”ç”¨ {name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if name == 'Gaussian Mixture':\n",
    "        labels = algorithm.fit_predict(X_customers_scaled)\n",
    "    else:\n",
    "        labels = algorithm.fit_predict(X_customers_scaled)\n",
    "    \n",
    "    clustering_time = time.time() - start_time\n",
    "    cluster_labels[name] = labels\n",
    "    \n",
    "    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "    if len(np.unique(labels)) > 1:  # ç¡®ä¿æœ‰å¤šä¸ªèšç±»\n",
    "        silhouette = silhouette_score(X_customers_scaled, labels)\n",
    "        calinski = calinski_harabasz_score(X_customers_scaled, labels)\n",
    "        \n",
    "        # ä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        ari = adjusted_rand_score(true_labels, labels)\n",
    "        \n",
    "        clustering_results[name] = {\n",
    "            'silhouette_score': silhouette,\n",
    "            'calinski_harabasz': calinski,\n",
    "            'adjusted_rand_index': ari,\n",
    "            'n_clusters': len(np.unique(labels)),\n",
    "            'clustering_time': clustering_time\n",
    "        }\n",
    "    else:\n",
    "        clustering_results[name] = {\n",
    "            'silhouette_score': -1,  # æ— æ•ˆèšç±»\n",
    "            'calinski_harabasz': -1,\n",
    "            'adjusted_rand_index': -1,\n",
    "            'n_clusters': len(np.unique(labels)),\n",
    "            'clustering_time': clustering_time\n",
    "        }\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "clustering_df = pd.DataFrame(clustering_results).T\n",
    "print(\"\\nèšç±»ç®—æ³•æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(clustering_df.round(4))\n",
    "\n",
    "# 6.4 èšç±»ç»“æœå¯è§†åŒ–\n",
    "print(\"\\n=== èšç±»ç»“æœå¯è§†åŒ– ===\")\n",
    "\n",
    "# ä¸ºäº†å¯è§†åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨PCAé™ç»´åˆ°2D\n",
    "pca_viz = PCA(n_components=2, random_state=42)\n",
    "X_customers_2d = pca_viz.fit_transform(X_customers_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# ç»˜åˆ¶ä¸åŒç®—æ³•çš„èšç±»ç»“æœ\n",
    "plot_configs = [\n",
    "    ('çœŸå®åˆ†ç»„', true_labels),\n",
    "    ('K-Means', cluster_labels['K-Means']),\n",
    "    ('Hierarchical', cluster_labels['Hierarchical']),\n",
    "    ('DBSCAN', cluster_labels['DBSCAN']),\n",
    "    ('Gaussian Mixture', cluster_labels['Gaussian Mixture']),\n",
    "    ('Spectral Clustering', cluster_labels['Spectral Clustering'])\n",
    "]\n",
    "\n",
    "for idx, (title, labels) in enumerate(plot_configs):\n",
    "    row, col = divmod(idx, 3)\n",
    "    \n",
    "    # å¤„ç†å™ªå£°ç‚¹ï¼ˆDBSCANä¸­çš„-1æ ‡ç­¾ï¼‰\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        if label == -1:\n",
    "            # å™ªå£°ç‚¹ç”¨é»‘è‰²xæ ‡è®°\n",
    "            mask = labels == label\n",
    "            axes[row, col].scatter(X_customers_2d[mask, 0], X_customers_2d[mask, 1], \n",
    "                                 c='black', marker='x', s=50, alpha=0.7, label='å™ªå£°')\n",
    "        else:\n",
    "            mask = labels == label\n",
    "            axes[row, col].scatter(X_customers_2d[mask, 0], X_customers_2d[mask, 1], \n",
    "                                 c=color, alpha=0.7, s=50, label=f'ç°‡ {label}')\n",
    "    \n",
    "    axes[row, col].set_title(f'{title}')\n",
    "    axes[row, col].set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    axes[row, col].set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6.5 å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾\n",
    "print(\"\\n=== å±‚æ¬¡èšç±»åˆ†æ ===\")\n",
    "\n",
    "# è®¡ç®—è·ç¦»çŸ©é˜µå’Œé“¾æ¥çŸ©é˜µ\n",
    "# ä¸ºäº†æ•ˆç‡ï¼Œåªä½¿ç”¨éƒ¨åˆ†æ•°æ®ç»˜åˆ¶æ ‘çŠ¶å›¾\n",
    "sample_size = 50\n",
    "sample_indices = np.random.choice(len(X_customers_scaled), sample_size, replace=False)\n",
    "X_sample = X_customers_scaled[sample_indices]\n",
    "\n",
    "# è®¡ç®—é“¾æ¥çŸ©é˜µ\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "# ç»˜åˆ¶æ ‘çŠ¶å›¾\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(linkage_matrix, orientation='top', distance_sort='descending')\n",
    "plt.title('å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ (Wardé“¾æ¥)')\n",
    "plt.xlabel('æ ·æœ¬ç´¢å¼•')\n",
    "plt.ylabel('è·ç¦»')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 6.6 èšç±»ç»“æœè§£é‡Šå’Œä¸šåŠ¡åº”ç”¨\n",
    "print(\"\\n=== èšç±»ç»“æœè§£é‡Šå’Œä¸šåŠ¡åº”ç”¨ ===\")\n",
    "\n",
    "# ä½¿ç”¨K-meansç»“æœè¿›è¡Œå®¢æˆ·ç»†åˆ†åˆ†æ\n",
    "customer_data['èšç±»æ ‡ç­¾'] = kmeans_labels\n",
    "\n",
    "# å„èšç±»çš„ç‰¹å¾åˆ†æ\n",
    "cluster_analysis = customer_data.groupby('èšç±»æ ‡ç­¾').agg({\n",
    "    'å¹´æ”¶å…¥(åƒ)': ['mean', 'std'],\n",
    "    'å¹´æ¶ˆè´¹(åƒ)': ['mean', 'std'],\n",
    "    'è´­ä¹°é¢‘ç‡': ['mean', 'std'],\n",
    "    'å®¢æˆ·æ»¡æ„åº¦': ['mean', 'std'],\n",
    "    'æ¨èæ„æ„¿': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"å„å®¢æˆ·ç¾¤ä½“ç‰¹å¾åˆ†æ:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# å„èšç±»çš„å¤§å°\n",
    "cluster_sizes = customer_data['èšç±»æ ‡ç­¾'].value_counts().sort_index()\n",
    "print(f\"\\nå„èšç±»å¤§å°:\")\n",
    "for cluster_id, size in cluster_sizes.items():\n",
    "    percentage = size / len(customer_data) * 100\n",
    "    print(f\"èšç±» {cluster_id}: {size} å®¢æˆ· ({percentage:.1f}%)\")\n",
    "\n",
    "# å®¢æˆ·ç¾¤ä½“å‘½åå’Œç‰¹å¾æè¿°\n",
    "cluster_descriptions = {}\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = customer_data[customer_data['èšç±»æ ‡ç­¾'] == cluster_id]\n",
    "    avg_income = cluster_data['å¹´æ”¶å…¥(åƒ)'].mean()\n",
    "    avg_spending = cluster_data['å¹´æ¶ˆè´¹(åƒ)'].mean()\n",
    "    avg_frequency = cluster_data['è´­ä¹°é¢‘ç‡'].mean()\n",
    "    \n",
    "    if avg_income > 60 and avg_spending > 80:\n",
    "        cluster_descriptions[cluster_id] = {\n",
    "            'name': 'é«˜ä»·å€¼å®¢æˆ·',\n",
    "            'description': 'é«˜æ”¶å…¥ã€é«˜æ¶ˆè´¹ã€é«˜é¢‘ç‡è´­ä¹°',\n",
    "            'strategy': 'ç»´æŠ¤å…³ç³»ã€æä¾›VIPæœåŠ¡ã€æ¨èé«˜ç«¯äº§å“'\n",
    "        }\n",
    "    elif avg_income > 40 and avg_spending > 50:\n",
    "        cluster_descriptions[cluster_id] = {\n",
    "            'name': 'æ½œåŠ›å®¢æˆ·',\n",
    "            'description': 'ä¸­ç­‰æ”¶å…¥å’Œæ¶ˆè´¹ï¼Œæœ‰æå‡ç©ºé—´',\n",
    "            'strategy': 'æ¨èä¿ƒé”€æ´»åŠ¨ã€æå‡è´­ä¹°é¢‘ç‡ã€äº¤å‰é”€å”®'\n",
    "        }\n",
    "    else:\n",
    "        cluster_descriptions[cluster_id] = {\n",
    "            'name': 'ä»·æ ¼æ•æ„Ÿå®¢æˆ·',\n",
    "            'description': 'æ”¶å…¥å’Œæ¶ˆè´¹è¾ƒä½ï¼Œä»·æ ¼æ•æ„Ÿ',\n",
    "            'strategy': 'æä¾›ä¼˜æƒ æ´»åŠ¨ã€åŸºç¡€äº§å“æ¨èã€æå‡æ»¡æ„åº¦'\n",
    "        }\n",
    "\n",
    "print(f\"\\nå®¢æˆ·ç¾¤ä½“æè¿°å’Œè¥é”€ç­–ç•¥:\")\n",
    "for cluster_id, desc in cluster_descriptions.items():\n",
    "    print(f\"\\nèšç±» {cluster_id} - {desc['name']}:\")\n",
    "    print(f\"  ç‰¹å¾: {desc['description']}\")\n",
    "    print(f\"  è¥é”€ç­–ç•¥: {desc['strategy']}\")\n",
    "    print(f\"  å®¢æˆ·æ•°é‡: {cluster_sizes[cluster_id]}\")\n",
    "\n",
    "# 6.7 èšç±»ç®—æ³•ç‰¹ç‚¹æ€»ç»“\n",
    "print(f\"\\n=== èšç±»ç®—æ³•ç‰¹ç‚¹æ€»ç»“ ===\")\n",
    "\n",
    "algorithm_comparison = {\n",
    "    'K-Means': {\n",
    "        'advantages': ['ç®€å•å¿«é€Ÿ', 'é€‚åˆçƒå½¢èšç±»', 'å¯æ‰©å±•æ€§å¥½'],\n",
    "        'disadvantages': ['éœ€è¦é¢„è®¾Kå€¼', 'å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ', 'å‡è®¾çƒå½¢èšç±»'],\n",
    "        'best_for': 'å¤§æ•°æ®é›†ã€çƒå½¢åˆ†å¸ƒã€å·²çŸ¥èšç±»æ•°'\n",
    "    },\n",
    "    'Hierarchical': {\n",
    "        'advantages': ['ä¸éœ€è¦é¢„è®¾èšç±»æ•°', 'äº§ç”Ÿèšç±»å±‚æ¬¡', 'ç¡®å®šæ€§ç»“æœ'],\n",
    "        'disadvantages': ['è®¡ç®—å¤æ‚åº¦é«˜', 'å¯¹å™ªå£°æ•æ„Ÿ', 'éš¾ä»¥å¤„ç†å¤§æ•°æ®'],\n",
    "        'best_for': 'å°æ•°æ®é›†ã€éœ€è¦èšç±»å±‚æ¬¡ã€æ¢ç´¢æ€§åˆ†æ'\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'advantages': ['å‘ç°ä»»æ„å½¢çŠ¶èšç±»', 'è‡ªåŠ¨æ£€æµ‹å™ªå£°', 'ä¸éœ€è¦é¢„è®¾èšç±»æ•°'],\n",
    "        'disadvantages': ['å‚æ•°æ•æ„Ÿ', 'å¯†åº¦å·®å¼‚å¤§æ—¶æ•ˆæœå·®', 'é«˜ç»´æ•°æ®å›°éš¾'],\n",
    "        'best_for': 'å™ªå£°æ•°æ®ã€ä»»æ„å½¢çŠ¶èšç±»ã€å¼‚å¸¸æ£€æµ‹'\n",
    "    },\n",
    "    'Gaussian Mixture': {\n",
    "        'advantages': ['è½¯èšç±»(æ¦‚ç‡)', 'å¤„ç†æ¤­åœ†å½¢èšç±»', 'æ¨¡å‹é€‰æ‹©çµæ´»'],\n",
    "        'disadvantages': ['éœ€è¦é¢„è®¾ç»„ä»¶æ•°', 'å¯¹åˆå§‹åŒ–æ•æ„Ÿ', 'è®¡ç®—å¤æ‚'],\n",
    "        'best_for': 'æ¤­åœ†å½¢èšç±»ã€éœ€è¦æ¦‚ç‡è¾“å‡ºã€æ··åˆåˆ†å¸ƒ'\n",
    "    }\n",
    "}\n",
    "\n",
    "for alg_name, characteristics in algorithm_comparison.items():\n",
    "    if alg_name in clustering_results:\n",
    "        result = clustering_results[alg_name]\n",
    "        print(f\"\\n{alg_name}:\")\n",
    "        print(f\"  è½®å»“ç³»æ•°: {result['silhouette_score']:.4f}\")\n",
    "        print(f\"  ä¼˜ç‚¹: {', '.join(characteristics['advantages'])}\")\n",
    "        print(f\"  ç¼ºç‚¹: {', '.join(characteristics['disadvantages'])}\")\n",
    "        print(f\"  é€‚ç”¨åœºæ™¯: {characteristics['best_for']}\")\n",
    "\n",
    "# æœ€ä½³èšç±»ç®—æ³•æ¨è\n",
    "best_clustering = max(clustering_results.keys(), \n",
    "                     key=lambda x: clustering_results[x]['silhouette_score'])\n",
    "print(f\"\\nğŸ† æœ¬æ¡ˆä¾‹æœ€ä½³èšç±»ç®—æ³•: {best_clustering}\")\n",
    "print(f\"   è½®å»“ç³»æ•°: {clustering_results[best_clustering]['silhouette_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nèšç±»åˆ†æå…³é”®è¦ç‚¹:\")\n",
    "print(f\"âœ“ é€‰æ‹©åˆé€‚çš„èšç±»æ•°é‡å¾ˆé‡è¦ï¼ˆè‚˜éƒ¨æ³•åˆ™ã€è½®å»“ç³»æ•°ï¼‰\")\n",
    "print(f\"âœ“ æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ï¼‰å¯¹èšç±»ç»“æœæœ‰é‡å¤§å½±å“\")\n",
    "print(f\"âœ“ ä¸åŒç®—æ³•é€‚ç”¨äºä¸åŒçš„æ•°æ®åˆ†å¸ƒå’Œä¸šåŠ¡åœºæ™¯\")\n",
    "print(f\"âœ“ èšç±»ç»“æœéœ€è¦ç»“åˆä¸šåŠ¡çŸ¥è¯†è¿›è¡Œè§£é‡Šå’Œåº”ç”¨\")\n",
    "print(f\"âœ“ è¯„ä¼°æŒ‡æ ‡è¦ç»“åˆå¤šä¸ªç»´åº¦è¿›è¡Œç»¼åˆåˆ¤æ–­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15765ba8",
   "metadata": {},
   "source": [
    "## 7. é™ç»´æŠ€æœ¯è¯¦è§£\n",
    "\n",
    "é™ç»´æ˜¯å¤„ç†é«˜ç»´æ•°æ®çš„é‡è¦æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘è®¡ç®—å¤æ‚åº¦ã€é¿å…ç»´åº¦è¯…å’’ã€ä¾¿äºå¯è§†åŒ–ã€‚æˆ‘ä»¬å°†å­¦ä¹ PCAã€t-SNEç­‰ç»å…¸é™ç»´æŠ€æœ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adfe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 ä¸»æˆåˆ†åˆ†æ (PCA) è¯¦è§£\n",
    "print(\"=== ä¸»æˆåˆ†åˆ†æ (PCA) è¯¦è§£ ===\")\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "# ä½¿ç”¨MNISTæ•°æ®é›†è¿›è¡Œé™ç»´æ¼”ç¤º\n",
    "# ä¸ºäº†è®¡ç®—æ•ˆç‡ï¼Œä½¿ç”¨å­é›†\n",
    "n_samples_dim = 2000\n",
    "indices_dim = np.random.choice(len(X_mnist), n_samples_dim, replace=False)\n",
    "X_mnist_dim = X_mnist[indices_dim] / 255.0  # æ ‡å‡†åŒ–\n",
    "y_mnist_dim = y_mnist[indices_dim]\n",
    "\n",
    "print(f\"é™ç»´æ•°æ®é›†ä¿¡æ¯:\")\n",
    "print(f\"åŸå§‹ç»´åº¦: {X_mnist_dim.shape}\")\n",
    "print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y_mnist_dim))}\")\n",
    "\n",
    "# PCAåˆ†æ\n",
    "print(\"\\n1. PCAä¸»æˆåˆ†åˆ†æ:\")\n",
    "\n",
    "# è®¡ç®—æ‰€æœ‰ä¸»æˆåˆ†\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_mnist_dim)\n",
    "\n",
    "# ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# å¯è§†åŒ–PCAç»“æœ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. è§£é‡Šæ–¹å·®æ¯”\n",
    "axes[0, 0].plot(range(1, 51), pca_full.explained_variance_ratio_[:50], 'bo-', markersize=4)\n",
    "axes[0, 0].set_xlabel('ä¸»æˆåˆ†')\n",
    "axes[0, 0].set_ylabel('è§£é‡Šæ–¹å·®æ¯”')\n",
    "axes[0, 0].set_title('å‰50ä¸ªä¸»æˆåˆ†çš„è§£é‡Šæ–¹å·®æ¯”')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”\n",
    "axes[0, 1].plot(range(1, 101), cumulative_variance[:100], 'ro-', markersize=3)\n",
    "axes[0, 1].axhline(y=0.95, color='green', linestyle='--', label='95%æ–¹å·®')\n",
    "axes[0, 1].axhline(y=0.99, color='orange', linestyle='--', label='99%æ–¹å·®')\n",
    "axes[0, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')\n",
    "axes[0, 1].set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”')\n",
    "axes[0, 1].set_title('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# æ‰¾åˆ°ä¿ç•™95%å’Œ99%æ–¹å·®çš„ä¸»æˆåˆ†æ•°é‡\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "n_components_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "\n",
    "print(f\"ä¿ç•™95%æ–¹å·®éœ€è¦çš„ä¸»æˆåˆ†æ•°é‡: {n_components_95}\")\n",
    "print(f\"ä¿ç•™99%æ–¹å·®éœ€è¦çš„ä¸»æˆåˆ†æ•°é‡: {n_components_99}\")\n",
    "\n",
    "# 3. ä½¿ç”¨ä¸åŒæ•°é‡çš„ä¸»æˆåˆ†è¿›è¡Œé™ç»´\n",
    "pca_components = [2, 10, 50, 100, 200]\n",
    "reconstruction_errors = []\n",
    "\n",
    "for n_comp in pca_components:\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_transformed = pca.fit_transform(X_mnist_dim)\n",
    "    X_reconstructed = pca.inverse_transform(X_transformed)\n",
    "    \n",
    "    # è®¡ç®—é‡æ„è¯¯å·®\n",
    "    error = np.mean((X_mnist_dim - X_reconstructed) ** 2)\n",
    "    reconstruction_errors.append(error)\n",
    "\n",
    "# é‡æ„è¯¯å·®å›¾\n",
    "axes[0, 2].plot(pca_components, reconstruction_errors, 'go-')\n",
    "axes[0, 2].set_xlabel('ä¸»æˆåˆ†æ•°é‡')\n",
    "axes[0, 2].set_ylabel('é‡æ„è¯¯å·® (MSE)')\n",
    "axes[0, 2].set_title('ä¸»æˆåˆ†æ•°é‡ vs é‡æ„è¯¯å·®')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 2D PCAå¯è§†åŒ–\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_mnist_dim)\n",
    "\n",
    "scatter = axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_mnist_dim, cmap='tab10', alpha=0.7)\n",
    "axes[1, 0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "axes[1, 0].set_title('PCA 2Då¯è§†åŒ–')\n",
    "plt.colorbar(scatter, ax=axes[1, 0])\n",
    "\n",
    "# 5. ä¸»æˆåˆ†å¯è§†åŒ–ï¼ˆå‰å‡ ä¸ªä¸»æˆåˆ†çœ‹èµ·æ¥åƒä»€ä¹ˆï¼‰\n",
    "pca_viz = PCA(n_components=9)\n",
    "pca_viz.fit(X_mnist_dim)\n",
    "\n",
    "for i in range(9):\n",
    "    row, col = divmod(i, 3)\n",
    "    if row == 1 and col < 2:\n",
    "        component = pca_viz.components_[i].reshape(28, 28)\n",
    "        axes[1, col + 1].imshow(component, cmap='coolwarm')\n",
    "        axes[1, col + 1].set_title(f'ä¸»æˆåˆ† {i+1}')\n",
    "        axes[1, col + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# é‡æ„æ•ˆæœå±•ç¤º\n",
    "print(\"\\n2. PCAé‡æ„æ•ˆæœå±•ç¤º:\")\n",
    "\n",
    "# é€‰æ‹©å‡ ä¸ªä¸åŒçš„ä¸»æˆåˆ†æ•°é‡è¿›è¡Œé‡æ„\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "\n",
    "# åŸå§‹å›¾åƒ\n",
    "sample_idx = 0\n",
    "original_image = X_mnist_dim[sample_idx].reshape(28, 28)\n",
    "axes[0, 0].imshow(original_image, cmap='gray')\n",
    "axes[0, 0].set_title('åŸå§‹å›¾åƒ')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# ä¸åŒä¸»æˆåˆ†æ•°é‡çš„é‡æ„\n",
    "components_to_test = [5, 10, 25, 50, 100]\n",
    "for i, n_comp in enumerate(components_to_test):\n",
    "    pca_recon = PCA(n_components=n_comp)\n",
    "    X_transform = pca_recon.fit_transform(X_mnist_dim)\n",
    "    X_recon = pca_recon.inverse_transform(X_transform)\n",
    "    \n",
    "    reconstructed_image = X_recon[sample_idx].reshape(28, 28)\n",
    "    axes[0, i+1].imshow(reconstructed_image, cmap='gray')\n",
    "    axes[0, i+1].set_title(f'{n_comp} ä¸»æˆåˆ†')\n",
    "    axes[0, i+1].axis('off')\n",
    "\n",
    "# å±•ç¤ºæ›´å¤šæ ·æœ¬\n",
    "for row in range(1, 3):\n",
    "    sample_idx = row * 100\n",
    "    original = X_mnist_dim[sample_idx].reshape(28, 28)\n",
    "    axes[row, 0].imshow(original, cmap='gray')\n",
    "    axes[row, 0].set_title(f'åŸå§‹ (æ•°å­—{y_mnist_dim[sample_idx]})')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    for i, n_comp in enumerate(components_to_test):\n",
    "        pca_recon = PCA(n_components=n_comp)\n",
    "        X_transform = pca_recon.fit_transform(X_mnist_dim)\n",
    "        X_recon = pca_recon.inverse_transform(X_transform)\n",
    "        \n",
    "        reconstructed = X_recon[sample_idx].reshape(28, 28)\n",
    "        axes[row, i+1].imshow(reconstructed, cmap='gray')\n",
    "        axes[row, i+1].axis('off')\n",
    "\n",
    "plt.suptitle('PCAé‡æ„æ•ˆæœå¯¹æ¯”', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7.2 t-SNEéçº¿æ€§é™ç»´\n",
    "print(\"\\n=== t-SNEéçº¿æ€§é™ç»´ ===\")\n",
    "\n",
    "# ä¸ºäº†è®¡ç®—æ•ˆç‡ï¼Œä½¿ç”¨æ›´å°çš„å­é›†\n",
    "n_samples_tsne = 1000\n",
    "indices_tsne = np.random.choice(len(X_mnist_dim), n_samples_tsne, replace=False)\n",
    "X_tsne_input = X_mnist_dim[indices_tsne]\n",
    "y_tsne_input = y_mnist_dim[indices_tsne]\n",
    "\n",
    "print(\"æ­£åœ¨è®¡ç®—t-SNEé™ç»´...\")\n",
    "print(\"æ³¨æ„ï¼št-SNEè®¡ç®—æ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "\n",
    "# ä¸åŒperplexityå‚æ•°çš„t-SNE\n",
    "perplexities = [5, 30, 50]\n",
    "tsne_results = {}\n",
    "\n",
    "for perp in perplexities:\n",
    "    print(f\"è®¡ç®— perplexity={perp} çš„t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000)\n",
    "    X_tsne = tsne.fit_transform(X_tsne_input)\n",
    "    tsne_results[perp] = X_tsne\n",
    "\n",
    "# æ¯”è¾ƒPCAå’Œt-SNEç»“æœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# PCA 2Dç»“æœï¼ˆä»ä¹‹å‰è®¡ç®—çš„ç»“æœä¸­å–å­é›†ï¼‰\n",
    "pca_subset = X_pca_2d[indices_tsne]\n",
    "scatter = axes[0, 0].scatter(pca_subset[:, 0], pca_subset[:, 1], c=y_tsne_input, cmap='tab10', alpha=0.7)\n",
    "axes[0, 0].set_title('PCA 2D')\n",
    "axes[0, 0].set_xlabel('PC1')\n",
    "axes[0, 0].set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=axes[0, 0])\n",
    "\n",
    "# ä¸åŒperplexityçš„t-SNEç»“æœ\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    row, col = divmod(idx + 1, 2)\n",
    "    X_tsne = tsne_results[perp]\n",
    "    \n",
    "    scatter = axes[row, col].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_tsne_input, cmap='tab10', alpha=0.7)\n",
    "    axes[row, col].set_title(f't-SNE (perplexity={perp})')\n",
    "    axes[row, col].set_xlabel('t-SNE 1')\n",
    "    axes[row, col].set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter, ax=axes[row, col])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7.3 ç‰¹å¾é€‰æ‹©æŠ€æœ¯\n",
    "print(\"\\n=== ç‰¹å¾é€‰æ‹©æŠ€æœ¯ ===\")\n",
    "\n",
    "# ä½¿ç”¨å®Œæ•´çš„MNISTå­é›†è¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "print(\"åº”ç”¨å„ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•...\")\n",
    "\n",
    "# 1. æ–¹å·®é˜ˆå€¼é€‰æ‹©\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_selector = VarianceThreshold(threshold=0.1)\n",
    "X_var_selected = var_selector.fit_transform(X_mnist_dim)\n",
    "\n",
    "print(f\"æ–¹å·®é˜ˆå€¼é€‰æ‹©:\")\n",
    "print(f\"åŸå§‹ç‰¹å¾æ•°: {X_mnist_dim.shape[1]}\")\n",
    "print(f\"é€‰æ‹©åç‰¹å¾æ•°: {X_var_selected.shape[1]}\")\n",
    "print(f\"ä¿ç•™ç‰¹å¾æ¯”ä¾‹: {X_var_selected.shape[1]/X_mnist_dim.shape[1]:.2%}\")\n",
    "\n",
    "# 2. å•å˜é‡ç‰¹å¾é€‰æ‹©\n",
    "univariate_selectors = {\n",
    "    'chi2': SelectKBest(score_func=chi2, k=100),\n",
    "    'f_classif': SelectKBest(score_func=f_classif, k=100),\n",
    "    'mutual_info': SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "}\n",
    "\n",
    "feature_selection_results = {}\n",
    "\n",
    "for name, selector in univariate_selectors.items():\n",
    "    print(f\"\\n{name} ç‰¹å¾é€‰æ‹©:\")\n",
    "    X_selected = selector.fit_transform(X_mnist_dim, y_mnist_dim)\n",
    "    \n",
    "    # è·å–ç‰¹å¾å¾—åˆ†\n",
    "    scores = selector.scores_\n",
    "    selected_features = selector.get_support()\n",
    "    \n",
    "    feature_selection_results[name] = {\n",
    "        'X_selected': X_selected,\n",
    "        'scores': scores,\n",
    "        'selected_features': selected_features,\n",
    "        'n_features': X_selected.shape[1]\n",
    "    }\n",
    "    \n",
    "    print(f\"é€‰æ‹©çš„ç‰¹å¾æ•°: {X_selected.shape[1]}\")\n",
    "    print(f\"å¹³å‡ç‰¹å¾å¾—åˆ†: {np.mean(scores):.2f}\")\n",
    "\n",
    "# 3. é€’å½’ç‰¹å¾æ¶ˆé™¤ (RFE)\n",
    "print(f\"\\né€’å½’ç‰¹å¾æ¶ˆé™¤ (RFE):\")\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºåŸºä¼°è®¡å™¨\n",
    "estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
    "rfe_selector = RFE(estimator=estimator, n_features_to_select=100, step=50)\n",
    "\n",
    "X_rfe_selected = rfe_selector.fit_transform(X_mnist_dim, y_mnist_dim)\n",
    "print(f\"RFEé€‰æ‹©çš„ç‰¹å¾æ•°: {X_rfe_selected.shape[1]}\")\n",
    "\n",
    "# 7.4 ç‰¹å¾é€‰æ‹©æ•ˆæœè¯„ä¼°\n",
    "print(\"\\n=== ç‰¹å¾é€‰æ‹©æ•ˆæœè¯„ä¼° ===\")\n",
    "\n",
    "# æ¯”è¾ƒä¸åŒç‰¹å¾é€‰æ‹©æ–¹æ³•å¯¹åˆ†ç±»æ€§èƒ½çš„å½±å“\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "feature_sets = {\n",
    "    'åŸå§‹ç‰¹å¾': X_mnist_dim,\n",
    "    'æ–¹å·®é˜ˆå€¼': X_var_selected,\n",
    "    'Chi2é€‰æ‹©': feature_selection_results['chi2']['X_selected'],\n",
    "    'Fåˆ†ç±»é€‰æ‹©': feature_selection_results['f_classif']['X_selected'],\n",
    "    'RFEé€‰æ‹©': X_rfe_selected,\n",
    "    'PCA (100ç»´)': PCA(n_components=100).fit_transform(X_mnist_dim)\n",
    "}\n",
    "\n",
    "# ä½¿ç”¨é€»è¾‘å›å½’è¿›è¡Œäº¤å‰éªŒè¯\n",
    "classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "cv_results = {}\n",
    "\n",
    "print(\"è¯„ä¼°ä¸åŒç‰¹å¾é€‰æ‹©æ–¹æ³•çš„åˆ†ç±»æ€§èƒ½...\")\n",
    "for name, X_features in feature_sets.items():\n",
    "    print(f\"è¯„ä¼° {name}...\")\n",
    "    scores = cross_val_score(classifier, X_features, y_mnist_dim, cv=3, scoring='accuracy')\n",
    "    cv_results[name] = {\n",
    "        'mean_accuracy': scores.mean(),\n",
    "        'std_accuracy': scores.std(),\n",
    "        'n_features': X_features.shape[1]\n",
    "    }\n",
    "\n",
    "# å¯è§†åŒ–ç‰¹å¾é€‰æ‹©æ•ˆæœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. å‡†ç¡®ç‡å¯¹æ¯”\n",
    "methods = list(cv_results.keys())\n",
    "accuracies = [cv_results[method]['mean_accuracy'] for method in methods]\n",
    "errors = [cv_results[method]['std_accuracy'] for method in methods]\n",
    "\n",
    "axes[0, 0].bar(range(len(methods)), accuracies, yerr=errors, capsize=5)\n",
    "axes[0, 0].set_title('ä¸åŒç‰¹å¾é€‰æ‹©æ–¹æ³•çš„åˆ†ç±»å‡†ç¡®ç‡')\n",
    "axes[0, 0].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 0].set_xticks(range(len(methods)))\n",
    "axes[0, 0].set_xticklabels(methods, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, (acc, err) in enumerate(zip(accuracies, errors)):\n",
    "    axes[0, 0].text(i, acc + err + 0.005, f'{acc:.3f}Â±{err:.3f}', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. ç‰¹å¾æ•°é‡å¯¹æ¯”\n",
    "n_features = [cv_results[method]['n_features'] for method in methods]\n",
    "axes[0, 1].bar(range(len(methods)), n_features, color='orange')\n",
    "axes[0, 1].set_title('ç‰¹å¾æ•°é‡å¯¹æ¯”')\n",
    "axes[0, 1].set_ylabel('ç‰¹å¾æ•°é‡')\n",
    "axes[0, 1].set_xticks(range(len(methods)))\n",
    "axes[0, 1].set_xticklabels(methods, rotation=45, ha='right')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. å‡†ç¡®ç‡vsç‰¹å¾æ•°é‡\n",
    "axes[1, 0].scatter(n_features, accuracies, s=100, alpha=0.7)\n",
    "for i, method in enumerate(methods):\n",
    "    axes[1, 0].annotate(method, (n_features[i], accuracies[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1, 0].set_xlabel('ç‰¹å¾æ•°é‡')\n",
    "axes[1, 0].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[1, 0].set_title('å‡†ç¡®ç‡ vs ç‰¹å¾æ•°é‡')\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–ï¼ˆä»¥Chi2ä¸ºä¾‹ï¼‰\n",
    "chi2_scores = feature_selection_results['chi2']['scores']\n",
    "feature_importance_image = chi2_scores.reshape(28, 28)\n",
    "im = axes[1, 1].imshow(feature_importance_image, cmap='hot', interpolation='nearest')\n",
    "axes[1, 1].set_title('Chi2ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾')\n",
    "axes[1, 1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7.5 é™ç»´æŠ€æœ¯æ€»ç»“\n",
    "print(\"\\n=== é™ç»´æŠ€æœ¯æ€»ç»“ ===\")\n",
    "\n",
    "dimensionality_reduction_summary = {\n",
    "    'PCA': {\n",
    "        'type': 'çº¿æ€§é™ç»´',\n",
    "        'advantages': ['å¿«é€Ÿè®¡ç®—', 'å¯è§£é‡Šæ€§', 'ä¿ç•™æœ€å¤§æ–¹å·®', 'å¯é€†å˜æ¢'],\n",
    "        'disadvantages': ['çº¿æ€§å‡è®¾', 'å¯èƒ½ä¸¢å¤±éçº¿æ€§ç»“æ„'],\n",
    "        'best_for': 'æ•°æ®é¢„å¤„ç†ã€å™ªå£°å‡å°‘ã€å‹ç¼©',\n",
    "        'parameters': 'ä¸»æˆåˆ†æ•°é‡'\n",
    "    },\n",
    "    't-SNE': {\n",
    "        'type': 'éçº¿æ€§é™ç»´',\n",
    "        'advantages': ['ä¿ç•™å±€éƒ¨ç»“æ„', 'å¯è§†åŒ–æ•ˆæœå¥½', 'å‘ç°éçº¿æ€§æ¨¡å¼'],\n",
    "        'disadvantages': ['è®¡ç®—å¤æ‚', 'ä¸å¯é€†', 'å¯¹å‚æ•°æ•æ„Ÿ'],\n",
    "        'best_for': 'æ•°æ®å¯è§†åŒ–ã€èšç±»åˆ†æ',\n",
    "        'parameters': 'perplexity, learning_rate'\n",
    "    },\n",
    "    'æ–¹å·®é˜ˆå€¼': {\n",
    "        'type': 'ç‰¹å¾é€‰æ‹©',\n",
    "        'advantages': ['ç®€å•å¿«é€Ÿ', 'ç§»é™¤æ— ç”¨ç‰¹å¾', 'å‡å°‘ç»´åº¦'],\n",
    "        'disadvantages': ['å¯èƒ½ç§»é™¤æœ‰ç”¨ç‰¹å¾', 'ä¸è€ƒè™‘ç›®æ ‡å˜é‡'],\n",
    "        'best_for': 'é¢„å¤„ç†æ­¥éª¤ã€ç§»é™¤å¸¸æ•°ç‰¹å¾',\n",
    "        'parameters': 'æ–¹å·®é˜ˆå€¼'\n",
    "    },\n",
    "    'å•å˜é‡é€‰æ‹©': {\n",
    "        'type': 'ç‰¹å¾é€‰æ‹©',\n",
    "        'advantages': ['è€ƒè™‘ç›®æ ‡å˜é‡', 'ç»Ÿè®¡æ˜¾è‘—æ€§', 'å¯è§£é‡Š'],\n",
    "        'disadvantages': ['å¿½ç•¥ç‰¹å¾äº¤äº’', 'å¯èƒ½é€‰æ‹©å†—ä½™ç‰¹å¾'],\n",
    "        'best_for': 'åˆæ­¥ç‰¹å¾ç­›é€‰ã€ç†è§£ç‰¹å¾é‡è¦æ€§',\n",
    "        'parameters': 'è¯„åˆ†å‡½æ•°ã€ç‰¹å¾æ•°é‡'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"é™ç»´æŠ€æœ¯å¯¹æ¯”:\")\n",
    "for technique, info in dimensionality_reduction_summary.items():\n",
    "    print(f\"\\n{technique} ({info['type']}):\")\n",
    "    print(f\"  ä¼˜ç‚¹: {', '.join(info['advantages'])}\")\n",
    "    print(f\"  ç¼ºç‚¹: {', '.join(info['disadvantages'])}\")\n",
    "    print(f\"  é€‚ç”¨åœºæ™¯: {info['best_for']}\")\n",
    "    print(f\"  å…³é”®å‚æ•°: {info['parameters']}\")\n",
    "\n",
    "# æ€§èƒ½æ€»ç»“\n",
    "print(f\"\\næœ¬æ¡ˆä¾‹æ€§èƒ½æ€»ç»“:\")\n",
    "best_method = max(cv_results.keys(), key=lambda x: cv_results[x]['mean_accuracy'])\n",
    "most_efficient = min(cv_results.keys(), key=lambda x: cv_results[x]['n_features'])\n",
    "\n",
    "print(f\"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_method} ({cv_results[best_method]['mean_accuracy']:.4f})\")\n",
    "print(f\"âš¡ æœ€å°‘ç‰¹å¾: {most_efficient} ({cv_results[most_efficient]['n_features']} ç‰¹å¾)\")\n",
    "\n",
    "print(f\"\\né™ç»´æŠ€æœ¯é€‰æ‹©å»ºè®®:\")\n",
    "print(f\"â€¢ æ•°æ®é¢„å¤„ç†: ä½¿ç”¨PCAæˆ–æ–¹å·®é˜ˆå€¼\")\n",
    "print(f\"â€¢ å¯è§†åŒ–åˆ†æ: ä½¿ç”¨t-SNEæˆ–PCA\")\n",
    "print(f\"â€¢ ç‰¹å¾é€‰æ‹©: ç»“åˆå•å˜é‡é€‰æ‹©å’ŒRFE\")\n",
    "print(f\"â€¢ è®¡ç®—æ•ˆç‡: ä¼˜å…ˆè€ƒè™‘PCAå’Œæ–¹å·®é˜ˆå€¼\")\n",
    "print(f\"â€¢ å¯è§£é‡Šæ€§: é€‰æ‹©PCAæˆ–å•å˜é‡ç‰¹å¾é€‰æ‹©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12903612",
   "metadata": {},
   "source": [
    "## 8. æ¨¡å‹é€‰æ‹©ä¸è¯„ä¼°\n",
    "\n",
    "æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°æ˜¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒç¯èŠ‚ã€‚æˆ‘ä»¬å°†å­¦ä¹ äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢ã€æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ç­‰å…³é”®æŠ€æœ¯ï¼Œç¡®ä¿æ¨¡å‹çš„å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 äº¤å‰éªŒè¯è¯¦è§£\n",
    "print(\"=== äº¤å‰éªŒè¯è¯¦è§£ ===\")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit,\n",
    "    cross_val_score, cross_validate, validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨ä¹‹å‰çš„MNISTå­é›†æ•°æ®\n",
    "print(f\"ä½¿ç”¨æ•°æ®é›†: {X_mnist_dim.shape}\")\n",
    "\n",
    "# å‡†å¤‡ä¸€ä¸ªåŸºç¡€åˆ†ç±»å™¨\n",
    "base_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# 8.1.1 ä¸åŒäº¤å‰éªŒè¯ç­–ç•¥\n",
    "print(\"\\n1. ä¸åŒäº¤å‰éªŒè¯ç­–ç•¥å¯¹æ¯”:\")\n",
    "\n",
    "cv_strategies = {\n",
    "    'KFold(5)': KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    'StratifiedKFold(5)': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    'KFold(10)': KFold(n_splits=10, shuffle=True, random_state=42),\n",
    "    'ShuffleSplit': ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, cv_strategy in cv_strategies.items():\n",
    "    print(f\"  è¯„ä¼° {name}...\")\n",
    "    scores = cross_val_score(base_classifier, X_mnist_dim, y_mnist_dim, \n",
    "                           cv=cv_strategy, scoring='accuracy')\n",
    "    cv_results[name] = {\n",
    "        'scores': scores,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std()\n",
    "    }\n",
    "\n",
    "# å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. äº¤å‰éªŒè¯å¾—åˆ†å¯¹æ¯”\n",
    "strategies = list(cv_results.keys())\n",
    "means = [cv_results[s]['mean'] for s in strategies]\n",
    "stds = [cv_results[s]['std'] for s in strategies]\n",
    "\n",
    "axes[0, 0].bar(range(len(strategies)), means, yerr=stds, capsize=5)\n",
    "axes[0, 0].set_title('ä¸åŒäº¤å‰éªŒè¯ç­–ç•¥çš„å‡†ç¡®ç‡å¯¹æ¯”')\n",
    "axes[0, 0].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 0].set_xticks(range(len(strategies)))\n",
    "axes[0, 0].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    axes[0, 0].text(i, mean + std + 0.005, f'{mean:.3f}Â±{std:.3f}', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. å¾—åˆ†åˆ†å¸ƒç®±å‹å›¾\n",
    "score_data = [cv_results[s]['scores'] for s in strategies]\n",
    "axes[0, 1].boxplot(score_data, labels=strategies)\n",
    "axes[0, 1].set_title('äº¤å‰éªŒè¯å¾—åˆ†åˆ†å¸ƒ')\n",
    "axes[0, 1].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 8.1.2 å¤šæŒ‡æ ‡äº¤å‰éªŒè¯\n",
    "print(\"\\n2. å¤šæŒ‡æ ‡äº¤å‰éªŒè¯:\")\n",
    "\n",
    "# å®šä¹‰å¤šä¸ªè¯„ä¼°æŒ‡æ ‡\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'f1_macro': 'f1_macro'\n",
    "}\n",
    "\n",
    "# ä½¿ç”¨StratifiedKFoldè¿›è¡Œå¤šæŒ‡æ ‡è¯„ä¼°\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "multi_metric_results = cross_validate(\n",
    "    base_classifier, X_mnist_dim, y_mnist_dim,\n",
    "    cv=skf, scoring=scoring_metrics, return_train_score=True\n",
    ")\n",
    "\n",
    "# 3. è®­ç»ƒé›†vséªŒè¯é›†æ€§èƒ½å¯¹æ¯”\n",
    "train_test_comparison = {}\n",
    "for metric in scoring_metrics.keys():\n",
    "    train_scores = multi_metric_results[f'train_{metric}']\n",
    "    test_scores = multi_metric_results[f'test_{metric}']\n",
    "    \n",
    "    train_test_comparison[metric] = {\n",
    "        'train_mean': train_scores.mean(),\n",
    "        'train_std': train_scores.std(),\n",
    "        'test_mean': test_scores.mean(),\n",
    "        'test_std': test_scores.std(),\n",
    "        'overfitting': train_scores.mean() - test_scores.mean()\n",
    "    }\n",
    "\n",
    "# å¯è§†åŒ–è®­ç»ƒé›†vsæµ‹è¯•é›†æ€§èƒ½\n",
    "metrics = list(train_test_comparison.keys())\n",
    "train_means = [train_test_comparison[m]['train_mean'] for m in metrics]\n",
    "test_means = [train_test_comparison[m]['test_mean'] for m in metrics]\n",
    "train_stds = [train_test_comparison[m]['train_std'] for m in metrics]\n",
    "test_stds = [train_test_comparison[m]['test_std'] for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x - width/2, train_means, width, yerr=train_stds, label='è®­ç»ƒé›†', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, test_means, width, yerr=test_stds, label='éªŒè¯é›†', alpha=0.8)\n",
    "axes[1, 0].set_title('è®­ç»ƒé›† vs éªŒè¯é›†æ€§èƒ½å¯¹æ¯”')\n",
    "axes[1, 0].set_ylabel('å¾—åˆ†')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(metrics)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. è¿‡æ‹Ÿåˆç¨‹åº¦åˆ†æ\n",
    "overfitting_scores = [train_test_comparison[m]['overfitting'] for m in metrics]\n",
    "bars = axes[1, 1].bar(range(len(metrics)), overfitting_scores)\n",
    "axes[1, 1].set_title('è¿‡æ‹Ÿåˆç¨‹åº¦åˆ†æ (è®­ç»ƒ-éªŒè¯)')\n",
    "axes[1, 1].set_ylabel('æ€§èƒ½å·®å¼‚')\n",
    "axes[1, 1].set_xticks(range(len(metrics)))\n",
    "axes[1, 1].set_xticklabels(metrics)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# æ ‡è®°è¿‡æ‹Ÿåˆç¨‹åº¦\n",
    "for i, score in enumerate(overfitting_scores):\n",
    "    color = 'red' if score > 0.05 else 'green'\n",
    "    axes[1, 1].text(i, score + 0.001, f'{score:.3f}', \n",
    "                    ha='center', va='bottom', color=color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"äº¤å‰éªŒè¯ç»“æœåˆ†æ:\")\n",
    "for strategy, result in cv_results.items():\n",
    "    print(f\"{strategy}: {result['mean']:.4f} Â± {result['std']:.4f}\")\n",
    "\n",
    "print(\"\\nå¤šæŒ‡æ ‡è¯„ä¼°ç»“æœ:\")\n",
    "for metric, result in train_test_comparison.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {result['train_mean']:.4f} Â± {result['train_std']:.4f}\")\n",
    "    print(f\"  éªŒè¯é›†: {result['test_mean']:.4f} Â± {result['test_std']:.4f}\")\n",
    "    print(f\"  è¿‡æ‹Ÿåˆç¨‹åº¦: {result['overfitting']:.4f}\")\n",
    "\n",
    "# 8.2 ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢\n",
    "print(\"\\n=== ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢ ===\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨\n",
    "print(\"1. ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–:\")\n",
    "\n",
    "# å®šä¹‰å‚æ•°ç½‘æ ¼\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºéšæœºæ£®æ—åˆ†ç±»å™¨\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# ç½‘æ ¼æœç´¢\n",
    "print(\"æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...\")\n",
    "grid_search = GridSearchCV(\n",
    "    rf_classifier, param_grid_rf, \n",
    "    cv=3, scoring='accuracy', \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†ä»¥èŠ‚çœæ—¶é—´\n",
    "indices_grid = np.random.choice(len(X_mnist_dim), 1000, replace=False)\n",
    "X_grid = X_mnist_dim[indices_grid]\n",
    "y_grid = y_mnist_dim[indices_grid]\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_grid, y_grid)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"ç½‘æ ¼æœç´¢å®Œæˆï¼Œè€—æ—¶: {grid_time:.2f}ç§’\")\n",
    "print(f\"æœ€ä½³å‚æ•°: {grid_search.best_params_}\")\n",
    "print(f\"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 2. éšæœºæœç´¢\n",
    "print(\"\\n2. éšæœºæœç´¢è¶…å‚æ•°ä¼˜åŒ–:\")\n",
    "\n",
    "# å®šä¹‰å‚æ•°åˆ†å¸ƒ\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# éšæœºæœç´¢\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_classifier, param_dist_rf,\n",
    "    n_iter=20, cv=3, scoring='accuracy',\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"æ­£åœ¨è¿›è¡Œéšæœºæœç´¢...\")\n",
    "start_time = time.time()\n",
    "random_search.fit(X_grid, y_grid)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"éšæœºæœç´¢å®Œæˆï¼Œè€—æ—¶: {random_time:.2f}ç§’\")\n",
    "print(f\"æœ€ä½³å‚æ•°: {random_search.best_params_}\")\n",
    "print(f\"æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# 8.3 å­¦ä¹ æ›²çº¿åˆ†æ\n",
    "print(\"\\n=== å­¦ä¹ æ›²çº¿åˆ†æ ===\")\n",
    "\n",
    "# è®¡ç®—å­¦ä¹ æ›²çº¿\n",
    "print(\"è®¡ç®—å­¦ä¹ æ›²çº¿...\")\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# ä½¿ç”¨æœ€ä¼˜çš„éšæœºæ£®æ—æ¨¡å‹\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "train_sizes_abs, train_scores_lc, val_scores_lc = learning_curve(\n",
    "    best_rf, X_grid, y_grid,\n",
    "    train_sizes=train_sizes, cv=3,\n",
    "    scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "# è®¡ç®—éªŒè¯æ›²çº¿\n",
    "print(\"è®¡ç®—éªŒè¯æ›²çº¿...\")\n",
    "param_name = 'n_estimators'\n",
    "param_range = [10, 50, 100, 150, 200, 250, 300]\n",
    "\n",
    "train_scores_vc, val_scores_vc = validation_curve(\n",
    "    RandomForestClassifier(random_state=42), X_grid, y_grid,\n",
    "    param_name=param_name, param_range=param_range,\n",
    "    cv=3, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "# å¯è§†åŒ–å­¦ä¹ æ›²çº¿å’ŒéªŒè¯æ›²çº¿\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. å­¦ä¹ æ›²çº¿\n",
    "train_mean_lc = np.mean(train_scores_lc, axis=1)\n",
    "train_std_lc = np.std(train_scores_lc, axis=1)\n",
    "val_mean_lc = np.mean(val_scores_lc, axis=1)\n",
    "val_std_lc = np.std(val_scores_lc, axis=1)\n",
    "\n",
    "axes[0, 0].plot(train_sizes_abs, train_mean_lc, 'o-', color='blue', label='è®­ç»ƒé›†')\n",
    "axes[0, 0].fill_between(train_sizes_abs, train_mean_lc - train_std_lc, \n",
    "                       train_mean_lc + train_std_lc, alpha=0.1, color='blue')\n",
    "\n",
    "axes[0, 0].plot(train_sizes_abs, val_mean_lc, 'o-', color='red', label='éªŒè¯é›†')\n",
    "axes[0, 0].fill_between(train_sizes_abs, val_mean_lc - val_std_lc, \n",
    "                       val_mean_lc + val_std_lc, alpha=0.1, color='red')\n",
    "\n",
    "axes[0, 0].set_xlabel('è®­ç»ƒé›†å¤§å°')\n",
    "axes[0, 0].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 0].set_title('å­¦ä¹ æ›²çº¿')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. éªŒè¯æ›²çº¿\n",
    "train_mean_vc = np.mean(train_scores_vc, axis=1)\n",
    "train_std_vc = np.std(train_scores_vc, axis=1)\n",
    "val_mean_vc = np.mean(val_scores_vc, axis=1)\n",
    "val_std_vc = np.std(val_scores_vc, axis=1)\n",
    "\n",
    "axes[0, 1].plot(param_range, train_mean_vc, 'o-', color='blue', label='è®­ç»ƒé›†')\n",
    "axes[0, 1].fill_between(param_range, train_mean_vc - train_std_vc, \n",
    "                       train_mean_vc + train_std_vc, alpha=0.1, color='blue')\n",
    "\n",
    "axes[0, 1].plot(param_range, val_mean_vc, 'o-', color='red', label='éªŒè¯é›†')\n",
    "axes[0, 1].fill_between(param_range, val_mean_vc - val_std_vc, \n",
    "                       val_mean_vc + val_std_vc, alpha=0.1, color='red')\n",
    "\n",
    "axes[0, 1].set_xlabel(f'{param_name}')\n",
    "axes[0, 1].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 1].set_title('éªŒè¯æ›²çº¿')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ç½‘æ ¼æœç´¢vséšæœºæœç´¢æ•ˆç‡å¯¹æ¯”\n",
    "search_comparison = {\n",
    "    'ç½‘æ ¼æœç´¢': {\n",
    "        'time': grid_time,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'n_combinations': len(grid_search.cv_results_['params'])\n",
    "    },\n",
    "    'éšæœºæœç´¢': {\n",
    "        'time': random_time,\n",
    "        'best_score': random_search.best_score_,\n",
    "        'n_combinations': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "methods = list(search_comparison.keys())\n",
    "times = [search_comparison[m]['time'] for m in methods]\n",
    "scores = [search_comparison[m]['best_score'] for m in methods]\n",
    "\n",
    "# æ—¶é—´å¯¹æ¯”\n",
    "axes[1, 0].bar(methods, times, color=['blue', 'orange'])\n",
    "axes[1, 0].set_title('æœç´¢æ—¶é—´å¯¹æ¯”')\n",
    "axes[1, 0].set_ylabel('æ—¶é—´ (ç§’)')\n",
    "for i, time_val in enumerate(times):\n",
    "    axes[1, 0].text(i, time_val + 0.1, f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# å¾—åˆ†å¯¹æ¯”\n",
    "axes[1, 1].bar(methods, scores, color=['blue', 'orange'])\n",
    "axes[1, 1].set_title('æœ€ä½³å¾—åˆ†å¯¹æ¯”')\n",
    "axes[1, 1].set_ylabel('å‡†ç¡®ç‡')\n",
    "for i, score in enumerate(scores):\n",
    "    axes[1, 1].text(i, score + 0.001, f'{score:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.4 è¯¦ç»†çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡\n",
    "print(\"\\n=== è¯¦ç»†çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ ===\")\n",
    "\n",
    "# ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œè¯¦ç»†è¯„ä¼°\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_detailed = best_model.predict(X_test_mnist)\n",
    "y_pred_proba = best_model.predict_proba(X_test_mnist)\n",
    "\n",
    "# è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡\n",
    "evaluation_metrics = {\n",
    "    'accuracy': accuracy_score(y_test_mnist, y_pred_detailed),\n",
    "    'precision_macro': precision_score(y_test_mnist, y_pred_detailed, average='macro'),\n",
    "    'precision_micro': precision_score(y_test_mnist, y_pred_detailed, average='micro'),\n",
    "    'precision_weighted': precision_score(y_test_mnist, y_pred_detailed, average='weighted'),\n",
    "    'recall_macro': recall_score(y_test_mnist, y_pred_detailed, average='macro'),\n",
    "    'recall_micro': recall_score(y_test_mnist, y_pred_detailed, average='micro'),\n",
    "    'recall_weighted': recall_score(y_test_mnist, y_pred_detailed, average='weighted'),\n",
    "    'f1_macro': f1_score(y_test_mnist, y_pred_detailed, average='macro'),\n",
    "    'f1_micro': f1_score(y_test_mnist, y_pred_detailed, average='micro'),\n",
    "    'f1_weighted': f1_score(y_test_mnist, y_pred_detailed, average='weighted')\n",
    "}\n",
    "\n",
    "print(\"è¯¦ç»†è¯„ä¼°æŒ‡æ ‡:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# åˆ†ç±»æŠ¥å‘Š\n",
    "print(f\"\\nåˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(classification_report(y_test_mnist, y_pred_detailed))\n",
    "\n",
    "# 8.5 æ¨¡å‹é€‰æ‹©æœ€ä½³å®è·µæ€»ç»“\n",
    "print(\"\\n=== æ¨¡å‹é€‰æ‹©æœ€ä½³å®è·µæ€»ç»“ ===\")\n",
    "\n",
    "best_practices = {\n",
    "    'æ•°æ®åˆ†å‰²': [\n",
    "        'ä½¿ç”¨åˆ†å±‚æŠ½æ ·ä¿è¯ç±»åˆ«å¹³è¡¡',\n",
    "        'ç•™å‡ºç‹¬ç«‹çš„æµ‹è¯•é›†',\n",
    "        'è®­ç»ƒé›†ç”¨äºæ¨¡å‹è®­ç»ƒï¼ŒéªŒè¯é›†ç”¨äºæ¨¡å‹é€‰æ‹©'\n",
    "    ],\n",
    "    'äº¤å‰éªŒè¯': [\n",
    "        'åˆ†ç±»é—®é¢˜ä½¿ç”¨StratifiedKFold',\n",
    "        'å°æ•°æ®é›†ä½¿ç”¨LeaveOneOut',\n",
    "        'å¤§æ•°æ®é›†ä½¿ç”¨ShuffleSplitæé«˜æ•ˆç‡'\n",
    "    ],\n",
    "    'è¶…å‚æ•°ä¼˜åŒ–': [\n",
    "        'å…ˆä½¿ç”¨éšæœºæœç´¢å¿«é€Ÿå®šä½åŒºåŸŸ',\n",
    "        'å†ä½¿ç”¨ç½‘æ ¼æœç´¢ç²¾ç»†ä¼˜åŒ–',\n",
    "        'è€ƒè™‘ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼ˆhyperoptç­‰ï¼‰'\n",
    "    ],\n",
    "    'æ¨¡å‹è¯„ä¼°': [\n",
    "        'ä½¿ç”¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ç»¼åˆåˆ¤æ–­',\n",
    "        'å…³æ³¨è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ€§èƒ½å·®å¼‚',\n",
    "        'ç»˜åˆ¶å­¦ä¹ æ›²çº¿è¯Šæ–­è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ'\n",
    "    ],\n",
    "    'æ¨¡å‹é€‰æ‹©': [\n",
    "        'ç®€å•æ¨¡å‹ä¼˜äºå¤æ‚æ¨¡å‹ï¼ˆå¥¥å¡å§†å‰ƒåˆ€ï¼‰',\n",
    "        'è€ƒè™‘æ¨¡å‹çš„å¯è§£é‡Šæ€§éœ€æ±‚',\n",
    "        'å¹³è¡¡æ€§èƒ½ã€è®­ç»ƒæ—¶é—´å’Œé¢„æµ‹æ—¶é—´'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"æ¨¡å‹é€‰æ‹©æœ€ä½³å®è·µ:\")\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  â€¢ {practice}\")\n",
    "\n",
    "# å¸¸è§è¯„ä¼°æŒ‡æ ‡è§£é‡Š\n",
    "print(f\"\\nè¯„ä¼°æŒ‡æ ‡è§£é‡Š:\")\n",
    "metric_explanations = {\n",
    "    'Accuracy': 'æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹ï¼Œé€‚ç”¨äºç±»åˆ«å¹³è¡¡çš„æ•°æ®',\n",
    "    'Precision': 'é¢„æµ‹ä¸ºæ­£ç±»ä¸­çœŸæ­£æ­£ç±»çš„æ¯”ä¾‹ï¼Œå…³æ³¨è¯¯æŠ¥',\n",
    "    'Recall': 'çœŸæ­£æ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼Œå…³æ³¨æ¼æŠ¥',\n",
    "    'F1-Score': 'Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡æ•°',\n",
    "    'Macroå¹³å‡': 'å„ç±»åˆ«æŒ‡æ ‡çš„ç®—æœ¯å¹³å‡ï¼Œç»™æ¯ä¸ªç±»åˆ«ç›¸åŒæƒé‡',\n",
    "    'Microå¹³å‡': 'æ‰€æœ‰æ ·æœ¬çš„å…¨å±€æŒ‡æ ‡ï¼Œç»™æ¯ä¸ªæ ·æœ¬ç›¸åŒæƒé‡',\n",
    "    'Weightedå¹³å‡': 'æŒ‰ç±»åˆ«æ ·æœ¬æ•°åŠ æƒçš„å¹³å‡ï¼Œé€‚ç”¨äºä¸å¹³è¡¡æ•°æ®'\n",
    "}\n",
    "\n",
    "for metric, explanation in metric_explanations.items():\n",
    "    print(f\"â€¢ {metric}: {explanation}\")\n",
    "\n",
    "print(f\"\\næœ¬æ¡ˆä¾‹æ€»ç»“:\")\n",
    "print(f\"âœ“ ä½¿ç”¨äº†{len(cv_strategies)}ç§äº¤å‰éªŒè¯ç­–ç•¥\")\n",
    "print(f\"âœ“ ç½‘æ ¼æœç´¢è¯„ä¼°äº†{search_comparison['ç½‘æ ¼æœç´¢']['n_combinations']}ç§å‚æ•°ç»„åˆ\")\n",
    "print(f\"âœ“ éšæœºæœç´¢åœ¨{random_time:.1f}ç§’å†…æ‰¾åˆ°äº†æ¥è¿‘æœ€ä¼˜çš„è§£\")\n",
    "print(f\"âœ“ æœ€ç»ˆæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡: {evaluation_metrics['accuracy']:.4f}\")\n",
    "print(f\"âœ“ å­¦ä¹ æ›²çº¿æ˜¾ç¤ºæ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad19d7e",
   "metadata": {},
   "source": [
    "## 9. ç»¼åˆé¡¹ç›®æ¡ˆä¾‹ï¼šå®Œæ•´æœºå™¨å­¦ä¹ æµç¨‹\n",
    "\n",
    "å°†å‰é¢å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯æ•´åˆï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚ä»æ•°æ®æ¢ç´¢åˆ°æ¨¡å‹éƒ¨ç½²ï¼Œä½“ç°çœŸå®é¡¹ç›®çš„å®Œæ•´æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 é¡¹ç›®åˆå§‹åŒ–å’Œæ•°æ®æ¢ç´¢\n",
    "print(\"=== ç»¼åˆé¡¹ç›®æ¡ˆä¾‹ï¼šMNISTæ‰‹å†™æ•°å­—è¯†åˆ«å®Œæ•´æµç¨‹ ===\")\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# é¡¹ç›®é…ç½®\n",
    "PROJECT_CONFIG = {\n",
    "    'project_name': 'MNIST_Digit_Recognition',\n",
    "    'version': '1.0',\n",
    "    'author': 'ML_Tutorial',\n",
    "    'created_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'target_accuracy': 0.95,\n",
    "    'max_training_time': 300  # ç§’\n",
    "}\n",
    "\n",
    "print(f\"é¡¹ç›®: {PROJECT_CONFIG['project_name']} v{PROJECT_CONFIG['version']}\")\n",
    "print(f\"ç›®æ ‡å‡†ç¡®ç‡: {PROJECT_CONFIG['target_accuracy']}\")\n",
    "print(f\"æœ€å¤§è®­ç»ƒæ—¶é—´: {PROJECT_CONFIG['max_training_time']}ç§’\")\n",
    "\n",
    "# ä½¿ç”¨å®Œæ•´çš„MNISTæ•°æ®é›†ï¼ˆé™åˆ¶å¤§å°ä»¥èŠ‚çœè®¡ç®—æ—¶é—´ï¼‰\n",
    "n_samples_final = 10000\n",
    "indices_final = np.random.choice(len(X_mnist), n_samples_final, replace=False)\n",
    "X_project = X_mnist[indices_final]\n",
    "y_project = y_mnist[indices_final]\n",
    "\n",
    "print(f\"\\næ•°æ®é›†ä¿¡æ¯:\")\n",
    "print(f\"æ ·æœ¬æ•°é‡: {X_project.shape[0]}\")\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_project.shape[1]}\")\n",
    "print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y_project))}\")\n",
    "\n",
    "# 9.1.1 æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)\n",
    "print(\"\\næ­¥éª¤1: æ¢ç´¢æ€§æ•°æ®åˆ†æ\")\n",
    "\n",
    "# ç±»åˆ«åˆ†å¸ƒ\n",
    "class_distribution = pd.Series(y_project).value_counts().sort_index()\n",
    "print(\"ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# æ•°æ®è´¨é‡æ£€æŸ¥\n",
    "print(f\"\\næ•°æ®è´¨é‡æ£€æŸ¥:\")\n",
    "print(f\"ç¼ºå¤±å€¼: {np.isnan(X_project).sum()}\")\n",
    "print(f\"ç‰¹å¾å€¼èŒƒå›´: {X_project.min()} - {X_project.max()}\")\n",
    "print(f\"é›¶å€¼æ¯”ä¾‹: {(X_project == 0).mean():.2%}\")\n",
    "\n",
    "# å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. ç±»åˆ«åˆ†å¸ƒ\n",
    "axes[0, 0].bar(class_distribution.index, class_distribution.values)\n",
    "axes[0, 0].set_title('ç±»åˆ«åˆ†å¸ƒ')\n",
    "axes[0, 0].set_xlabel('æ•°å­—')\n",
    "axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. åƒç´ å€¼åˆ†å¸ƒ\n",
    "axes[0, 1].hist(X_project.flatten(), bins=50, alpha=0.7)\n",
    "axes[0, 1].set_title('åƒç´ å€¼åˆ†å¸ƒ')\n",
    "axes[0, 1].set_xlabel('åƒç´ å€¼')\n",
    "axes[0, 1].set_ylabel('é¢‘æ¬¡')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. æ¯ä¸ªæ ·æœ¬çš„éé›¶åƒç´ æ•°é‡\n",
    "non_zero_pixels = np.sum(X_project > 0, axis=1)\n",
    "axes[0, 2].hist(non_zero_pixels, bins=30, alpha=0.7, color='green')\n",
    "axes[0, 2].set_title('æ¯ä¸ªæ ·æœ¬çš„éé›¶åƒç´ æ•°é‡')\n",
    "axes[0, 2].set_xlabel('éé›¶åƒç´ æ•°')\n",
    "axes[0, 2].set_ylabel('é¢‘æ¬¡')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4-6. å±•ç¤ºæ¯ä¸ªæ•°å­—çš„å…¸å‹æ ·æœ¬\n",
    "for i in range(3):\n",
    "    digit = i * 3\n",
    "    if digit < 10:\n",
    "        # æ‰¾åˆ°è¯¥æ•°å­—çš„æ ·æœ¬\n",
    "        digit_indices = np.where(y_project == digit)[0]\n",
    "        if len(digit_indices) > 0:\n",
    "            sample_idx = digit_indices[0]\n",
    "            image = X_project[sample_idx].reshape(28, 28)\n",
    "            axes[1, i].imshow(image, cmap='gray')\n",
    "            axes[1, i].set_title(f'æ•°å­— {digit} æ ·æœ¬')\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9.2 æ•°æ®é¢„å¤„ç†æµæ°´çº¿\n",
    "print(\"\\næ­¥éª¤2: æ„å»ºæ•°æ®é¢„å¤„ç†æµæ°´çº¿\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# 2.1 æ•°æ®åˆ†å‰²\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_project, y_project, test_size=0.2, random_state=42, stratify=y_project\n",
    ")\n",
    "\n",
    "print(f\"æ•°æ®åˆ†å‰²ç»“æœ:\")\n",
    "print(f\"è®­ç»ƒé›†: {X_train_final.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test_final.shape}\")\n",
    "\n",
    "# 2.2 åˆ›å»ºé¢„å¤„ç†æµæ°´çº¿\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=chi2, k=400))\n",
    "])\n",
    "\n",
    "print(\"é¢„å¤„ç†æµæ°´çº¿ç»„ä»¶:\")\n",
    "print(\"1. æ ‡å‡†åŒ– (StandardScaler)\")\n",
    "print(\"2. ç‰¹å¾é€‰æ‹© (SelectKBest, k=400)\")\n",
    "\n",
    "# åº”ç”¨é¢„å¤„ç†\n",
    "print(\"\\nåº”ç”¨é¢„å¤„ç†æµæ°´çº¿...\")\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train_final, y_train_final)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test_final)\n",
    "\n",
    "print(f\"é¢„å¤„ç†åç‰¹å¾æ•°é‡: {X_train_processed.shape[1]}\")\n",
    "\n",
    "# 9.3 æ¨¡å‹å€™é€‰æ± å’ŒåŸºå‡†æµ‹è¯•\n",
    "print(\"\\næ­¥éª¤3: æ¨¡å‹å€™é€‰æ± å’ŒåŸºå‡†æµ‹è¯•\")\n",
    "\n",
    "# å®šä¹‰å€™é€‰æ¨¡å‹\n",
    "candidate_models = {\n",
    "    'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random_Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM_RBF': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'Gradient_Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# åŸºå‡†æµ‹è¯•\n",
    "baseline_results = {}\n",
    "print(\"è¿›è¡ŒåŸºå‡†æµ‹è¯•...\")\n",
    "\n",
    "for name, model in candidate_models.items():\n",
    "    print(f\"æµ‹è¯• {name}...\")\n",
    "    \n",
    "    # ä½¿ç”¨3æŠ˜äº¤å‰éªŒè¯å¿«é€Ÿè¯„ä¼°\n",
    "    start_time = time.time()\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train_final, \n",
    "                               cv=3, scoring='accuracy')\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    baseline_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  å‡†ç¡®ç‡: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "    print(f\"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’\")\n",
    "\n",
    "# é€‰æ‹©æœ€æœ‰å‰æ™¯çš„æ¨¡å‹\n",
    "best_baseline_model = max(baseline_results.keys(), \n",
    "                         key=lambda x: baseline_results[x]['cv_mean'])\n",
    "print(f\"\\næœ€ä½³åŸºå‡†æ¨¡å‹: {best_baseline_model}\")\n",
    "print(f\"åŸºå‡†å‡†ç¡®ç‡: {baseline_results[best_baseline_model]['cv_mean']:.4f}\")\n",
    "\n",
    "# 9.4 è¶…å‚æ•°ä¼˜åŒ–\n",
    "print(\"\\næ­¥éª¤4: è¶…å‚æ•°ä¼˜åŒ–\")\n",
    "\n",
    "# æ ¹æ®åŸºå‡†æµ‹è¯•ç»“æœï¼Œé€‰æ‹©æœ€ä½³æ¨¡å‹è¿›è¡Œä¼˜åŒ–\n",
    "if best_baseline_model == 'Random_Forest':\n",
    "    param_grid_final = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    best_model_class = RandomForestClassifier(random_state=42)\n",
    "elif best_baseline_model == 'SVM_RBF':\n",
    "    param_grid_final = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    best_model_class = SVC(kernel='rbf', random_state=42, probability=True)\n",
    "else:\n",
    "    # é»˜è®¤ä½¿ç”¨éšæœºæ£®æ—\n",
    "    param_grid_final = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    best_model_class = RandomForestClassifier(random_state=42)\n",
    "\n",
    "print(f\"ä¼˜åŒ–æ¨¡å‹: {best_baseline_model}\")\n",
    "print(f\"å‚æ•°ç½‘æ ¼: {param_grid_final}\")\n",
    "\n",
    "# ç½‘æ ¼æœç´¢ä¼˜åŒ–\n",
    "grid_search_final = GridSearchCV(\n",
    "    best_model_class, param_grid_final,\n",
    "    cv=5, scoring='accuracy',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–...\")\n",
    "start_time = time.time()\n",
    "grid_search_final.fit(X_train_processed, y_train_final)\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "print(f\"ä¼˜åŒ–å®Œæˆï¼Œè€—æ—¶: {optimization_time:.2f}ç§’\")\n",
    "print(f\"æœ€ä½³å‚æ•°: {grid_search_final.best_params_}\")\n",
    "print(f\"æœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†: {grid_search_final.best_score_:.4f}\")\n",
    "\n",
    "# 9.5 æœ€ç»ˆæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°\n",
    "print(\"\\næ­¥éª¤5: æœ€ç»ˆæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°\")\n",
    "\n",
    "# è·å–æœ€ä½³æ¨¡å‹\n",
    "final_model = grid_search_final.best_estimator_\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "y_pred_final = final_model.predict(X_test_processed)\n",
    "y_pred_proba_final = final_model.predict_proba(X_test_processed)\n",
    "\n",
    "# è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡\n",
    "final_metrics = {\n",
    "    'accuracy': accuracy_score(y_test_final, y_pred_final),\n",
    "    'precision_macro': precision_score(y_test_final, y_pred_final, average='macro'),\n",
    "    'recall_macro': recall_score(y_test_final, y_pred_final, average='macro'),\n",
    "    'f1_macro': f1_score(y_test_final, y_pred_final, average='macro')\n",
    "}\n",
    "\n",
    "print(\"æœ€ç»ˆæ¨¡å‹æ€§èƒ½:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡\n",
    "target_met = final_metrics['accuracy'] >= PROJECT_CONFIG['target_accuracy']\n",
    "print(f\"\\nç›®æ ‡å‡†ç¡®ç‡ ({PROJECT_CONFIG['target_accuracy']}) è¾¾æˆ: {'âœ“' if target_met else 'âœ—'}\")\n",
    "\n",
    "# 9.6 ç»“æœå¯è§†åŒ–å’Œåˆ†æ\n",
    "print(\"\\næ­¥éª¤6: ç»“æœå¯è§†åŒ–å’Œåˆ†æ\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n",
    "models = list(baseline_results.keys()) + ['Final_Optimized']\n",
    "accuracies = [baseline_results[m]['cv_mean'] for m in baseline_results.keys()] + [final_metrics['accuracy']]\n",
    "colors = ['lightblue'] * len(baseline_results) + ['red']\n",
    "\n",
    "bars = axes[0, 0].bar(range(len(models)), accuracies, color=colors)\n",
    "axes[0, 0].set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')\n",
    "axes[0, 0].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[0, 0].set_xticks(range(len(models)))\n",
    "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ·»åŠ ç›®æ ‡çº¿\n",
    "axes[0, 0].axhline(y=PROJECT_CONFIG['target_accuracy'], color='green', \n",
    "                   linestyle='--', label=f\"ç›®æ ‡å‡†ç¡®ç‡ ({PROJECT_CONFIG['target_accuracy']})\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. æ··æ·†çŸ©é˜µ\n",
    "cm_final = confusion_matrix(y_test_final, y_pred_final)\n",
    "im = axes[0, 1].imshow(cm_final, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[0, 1].set_title('æœ€ç»ˆæ¨¡å‹æ··æ·†çŸ©é˜µ')\n",
    "axes[0, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "axes[0, 1].set_ylabel('çœŸå®æ ‡ç­¾')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i in range(cm_final.shape[0]):\n",
    "    for j in range(cm_final.shape[1]):\n",
    "        axes[0, 1].text(j, i, format(cm_final[i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if cm_final[i, j] > cm_final.max() / 2 else \"black\")\n",
    "\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "# 3. ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    # è·å–åŸå§‹ç‰¹å¾çš„é‡è¦æ€§\n",
    "    feature_importance = final_model.feature_importances_\n",
    "    # åå‘æ˜ å°„åˆ°åŸå§‹åƒç´ ä½ç½®\n",
    "    selected_features = preprocessing_pipeline.named_steps['feature_selection'].get_support()\n",
    "    full_importance = np.zeros(784)\n",
    "    full_importance[selected_features] = feature_importance\n",
    "    \n",
    "    importance_image = full_importance.reshape(28, 28)\n",
    "    im2 = axes[0, 2].imshow(importance_image, cmap='hot', interpolation='nearest')\n",
    "    axes[0, 2].set_title('ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾')\n",
    "    axes[0, 2].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[0, 2])\n",
    "else:\n",
    "    axes[0, 2].text(0.5, 0.5, 'è¯¥æ¨¡å‹ä¸æ”¯æŒ\\nç‰¹å¾é‡è¦æ€§åˆ†æ', \n",
    "                   ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "    axes[0, 2].set_title('ç‰¹å¾é‡è¦æ€§')\n",
    "\n",
    "# 4. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n",
    "if y_pred_proba_final is not None:\n",
    "    max_proba = np.max(y_pred_proba_final, axis=1)\n",
    "    axes[1, 0].hist(max_proba, bins=30, alpha=0.7)\n",
    "    axes[1, 0].set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')\n",
    "    axes[1, 0].set_xlabel('æœ€å¤§é¢„æµ‹æ¦‚ç‡')\n",
    "    axes[1, 0].set_ylabel('é¢‘æ¬¡')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. é”™è¯¯åˆ†æ\n",
    "incorrect_indices = np.where(y_test_final != y_pred_final)[0]\n",
    "if len(incorrect_indices) > 0:\n",
    "    # é€‰æ‹©ä¸€äº›é”™è¯¯æ ·æœ¬å±•ç¤º\n",
    "    sample_errors = incorrect_indices[:6]\n",
    "    for i, idx in enumerate(sample_errors):\n",
    "        if i < 6:\n",
    "            row, col = divmod(i + 6, 3)  # ä»ç¬¬äºŒè¡Œå¼€å§‹\n",
    "            if row < 2:\n",
    "                true_label = y_test_final.iloc[idx] if hasattr(y_test_final, 'iloc') else y_test_final[idx]\n",
    "                pred_label = y_pred_final[idx]\n",
    "                image = X_test_final[idx].reshape(28, 28)\n",
    "                \n",
    "                axes[row, col].imshow(image, cmap='gray')\n",
    "                axes[row, col].set_title(f'é”™è¯¯: {true_label}â†’{pred_label}')\n",
    "                axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9.7 æ¨¡å‹ä¿å­˜å’Œéƒ¨ç½²å‡†å¤‡\n",
    "print(\"\\næ­¥éª¤7: æ¨¡å‹ä¿å­˜å’Œéƒ¨ç½²å‡†å¤‡\")\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹åŒ…\n",
    "model_package = {\n",
    "    'model': final_model,\n",
    "    'preprocessing_pipeline': preprocessing_pipeline,\n",
    "    'feature_names': [f'pixel_{i}' for i in range(784)],\n",
    "    'target_names': [str(i) for i in range(10)],\n",
    "    'model_metadata': {\n",
    "        'model_type': type(final_model).__name__,\n",
    "        'best_params': grid_search_final.best_params_,\n",
    "        'cv_score': grid_search_final.best_score_,\n",
    "        'test_accuracy': final_metrics['accuracy'],\n",
    "        'training_samples': len(X_train_final),\n",
    "        'features_selected': X_train_processed.shape[1],\n",
    "        'created_date': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹ï¼ˆæ³¨é‡Šæ‰å®é™…ä¿å­˜ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼‰\n",
    "# model_filename = f\"mnist_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "# joblib.dump(model_package, model_filename)\n",
    "# print(f\"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_filename}\")\n",
    "\n",
    "print(\"æ¨¡å‹åŒ…ç»„ä»¶:\")\n",
    "for key, value in model_package.items():\n",
    "    if key != 'model' and key != 'preprocessing_pipeline':\n",
    "        print(f\"â€¢ {key}: {type(value)}\")\n",
    "\n",
    "# æ¨¡å‹é¢„æµ‹å‡½æ•°ç¤ºä¾‹\n",
    "def predict_digit(image_array, model_package):\n",
    "    \\\"\\\"\\\"\n",
    "    é¢„æµ‹å•ä¸ªæ‰‹å†™æ•°å­—å›¾åƒ\n",
    "    \n",
    "    å‚æ•°:\n",
    "    image_array: 28x28çš„numpyæ•°ç»„æˆ–784ç»´å‘é‡\n",
    "    model_package: åŒ…å«æ¨¡å‹å’Œé¢„å¤„ç†å™¨çš„åŒ…\n",
    "    \n",
    "    è¿”å›:\n",
    "    prediction: é¢„æµ‹çš„æ•°å­—\n",
    "    probability: é¢„æµ‹æ¦‚ç‡\n",
    "    \\\"\\\"\\\"\n",
    "    # ç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„å½¢çŠ¶\n",
    "    if image_array.shape == (28, 28):\n",
    "        image_vector = image_array.flatten()\n",
    "    else:\n",
    "        image_vector = image_array\n",
    "    \n",
    "    # é¢„å¤„ç†\n",
    "    image_processed = model_package['preprocessing_pipeline'].transform([image_vector])\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    prediction = model_package['model'].predict(image_processed)[0]\n",
    "    probability = model_package['model'].predict_proba(image_processed)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# ç¤ºä¾‹é¢„æµ‹\n",
    "sample_image = X_test_final[0]\n",
    "pred, prob = predict_digit(sample_image, model_package)\n",
    "print(f\"\\né¢„æµ‹ç¤ºä¾‹:\")\n",
    "print(f\"çœŸå®æ ‡ç­¾: {y_test_final.iloc[0] if hasattr(y_test_final, 'iloc') else y_test_final[0]}\")\n",
    "print(f\"é¢„æµ‹æ ‡ç­¾: {pred}\")\n",
    "print(f\"é¢„æµ‹æ¦‚ç‡: {prob[pred]:.4f}\")\n",
    "\n",
    "# 9.8 é¡¹ç›®æ€»ç»“æŠ¥å‘Š\n",
    "print(\"\\næ­¥éª¤8: é¡¹ç›®æ€»ç»“æŠ¥å‘Š\")\n",
    "\n",
    "project_summary = f\\\"\\\"\\\"\n",
    "=== {PROJECT_CONFIG['project_name']} é¡¹ç›®æ€»ç»“æŠ¥å‘Š ===\n",
    "\n",
    "é¡¹ç›®ä¿¡æ¯:\n",
    "â€¢ é¡¹ç›®ç‰ˆæœ¬: {PROJECT_CONFIG['version']}\n",
    "â€¢ åˆ›å»ºæ—¥æœŸ: {PROJECT_CONFIG['created_date']}\n",
    "â€¢ ç›®æ ‡å‡†ç¡®ç‡: {PROJECT_CONFIG['target_accuracy']}\n",
    "\n",
    "æ•°æ®é›†ä¿¡æ¯:\n",
    "â€¢ æ€»æ ·æœ¬æ•°: {len(X_project):,}\n",
    "â€¢ è®­ç»ƒæ ·æœ¬: {len(X_train_final):,}\n",
    "â€¢ æµ‹è¯•æ ·æœ¬: {len(X_test_final):,}\n",
    "â€¢ åŸå§‹ç‰¹å¾æ•°: {X_project.shape[1]}\n",
    "â€¢ é€‰æ‹©ç‰¹å¾æ•°: {X_train_processed.shape[1]}\n",
    "\n",
    "æ¨¡å‹å¼€å‘è¿‡ç¨‹:\n",
    "1. âœ“ æ¢ç´¢æ€§æ•°æ®åˆ†æ\n",
    "2. âœ“ æ•°æ®é¢„å¤„ç†æµæ°´çº¿æ„å»º\n",
    "3. âœ“ å¤šæ¨¡å‹åŸºå‡†æµ‹è¯• ({len(candidate_models)} ä¸ªæ¨¡å‹)\n",
    "4. âœ“ è¶…å‚æ•°ä¼˜åŒ– (ç½‘æ ¼æœç´¢)\n",
    "5. âœ“ æœ€ç»ˆæ¨¡å‹è¯„ä¼°\n",
    "6. âœ“ æ¨¡å‹ä¿å­˜å’Œéƒ¨ç½²å‡†å¤‡\n",
    "\n",
    "æœ€ç»ˆæ¨¡å‹æ€§èƒ½:\n",
    "â€¢ æ¨¡å‹ç±»å‹: {type(final_model).__name__}\n",
    "â€¢ æµ‹è¯•å‡†ç¡®ç‡: {final_metrics['accuracy']:.4f}\n",
    "â€¢ ç²¾ç¡®ç‡ (å®å¹³å‡): {final_metrics['precision_macro']:.4f}\n",
    "â€¢ å¬å›ç‡ (å®å¹³å‡): {final_metrics['recall_macro']:.4f}\n",
    "â€¢ F1åˆ†æ•° (å®å¹³å‡): {final_metrics['f1_macro']:.4f}\n",
    "\n",
    "ç›®æ ‡è¾¾æˆæƒ…å†µ:\n",
    "â€¢ å‡†ç¡®ç‡ç›®æ ‡: {'è¾¾æˆ âœ“' if target_met else 'æœªè¾¾æˆ âœ—'}\n",
    "\n",
    "è®¡ç®—æ•ˆç‡:\n",
    "â€¢ åŸºå‡†æµ‹è¯•æ—¶é—´: {sum(baseline_results[m]['training_time'] for m in baseline_results):.1f} ç§’\n",
    "â€¢ è¶…å‚æ•°ä¼˜åŒ–æ—¶é—´: {optimization_time:.1f} ç§’\n",
    "â€¢ æ€»è®¡ç®—æ—¶é—´: {sum(baseline_results[m]['training_time'] for m in baseline_results) + optimization_time:.1f} ç§’\n",
    "\n",
    "å»ºè®®å’Œåç»­å·¥ä½œ:\n",
    "1. è€ƒè™‘ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›ä¸€æ­¥æå‡æ€§èƒ½\n",
    "2. å®æ–½æ¨¡å‹ç›‘æ§å’Œè‡ªåŠ¨é‡è®­ç»ƒæœºåˆ¶\n",
    "3. ä¼˜åŒ–é¢„æµ‹å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨\n",
    "4. å¢åŠ æ•°æ®å¢å¼ºæŠ€æœ¯\n",
    "5. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒå¹¶æ”¶é›†åé¦ˆ\n",
    "\n",
    "é¡¹ç›®çŠ¶æ€: {'æˆåŠŸå®Œæˆ' if target_met else 'éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "print(project_summary)\n",
    "\n",
    "# ä¿å­˜é¡¹ç›®æŠ¥å‘Šï¼ˆæ³¨é‡Šæ‰å®é™…ä¿å­˜ï¼‰\n",
    "# report_filename = f\"project_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "# with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "#     f.write(project_summary)\n",
    "# print(f\"\\né¡¹ç›®æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ {PROJECT_CONFIG['project_name']} é¡¹ç›®å®Œæˆï¼\")\n",
    "print(f\"è¿™ä¸ªç»¼åˆæ¡ˆä¾‹å±•ç¤ºäº†å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®æµç¨‹:\")\n",
    "print(f\"ä»æ•°æ®æ¢ç´¢ â†’ é¢„å¤„ç† â†’ æ¨¡å‹é€‰æ‹© â†’ ä¼˜åŒ– â†’ è¯„ä¼° â†’ éƒ¨ç½²å‡†å¤‡\")\n",
    "print(f\"åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥æŒ‰ç…§è¿™ä¸ªæµç¨‹è¿›è¡Œå¼€å‘å’Œéƒ¨ç½²ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7022ba4",
   "metadata": {},
   "source": [
    "## 10. å­¦ä¹ æ€»ç»“ä¸è¿›é˜¶æŒ‡å—\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªå®Œæ•´çš„Scikit-learnæ•™ç¨‹ï¼Œæˆ‘ä»¬æŒæ¡äº†æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæŠ€èƒ½ã€‚è®©æˆ‘ä»¬æ€»ç»“å…³é”®çŸ¥è¯†ç‚¹ï¼Œå¹¶è§„åˆ’è¿›é˜¶å­¦ä¹ è·¯å¾„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 çŸ¥è¯†ä½“ç³»å›é¡¾\n",
    "print(\"=== Scikit-learn å­¦ä¹ æ€»ç»“ä¸è¿›é˜¶æŒ‡å— ===\")\n",
    "\n",
    "# åˆ›å»ºçŸ¥è¯†ä½“ç³»å›¾\n",
    "knowledge_map = {\n",
    "    'æœºå™¨å­¦ä¹ åŸºç¡€': {\n",
    "        'ç›‘ç£å­¦ä¹ ': ['åˆ†ç±»', 'å›å½’'],\n",
    "        'æ— ç›‘ç£å­¦ä¹ ': ['èšç±»', 'é™ç»´'],\n",
    "        'æ¨¡å‹è¯„ä¼°': ['äº¤å‰éªŒè¯', 'è¯„ä¼°æŒ‡æ ‡']\n",
    "    },\n",
    "    'æ•°æ®é¢„å¤„ç†': {\n",
    "        'ç‰¹å¾ç¼©æ”¾': ['StandardScaler', 'MinMaxScaler', 'RobustScaler'],\n",
    "        'åˆ†ç±»ç¼–ç ': ['LabelEncoder', 'OneHotEncoder', 'OrdinalEncoder'],\n",
    "        'ç¼ºå¤±å€¼å¤„ç†': ['SimpleImputer', 'KNNImputer'],\n",
    "        'ç‰¹å¾é€‰æ‹©': ['VarianceThreshold', 'SelectKBest', 'RFE']\n",
    "    },\n",
    "    'åˆ†ç±»ç®—æ³•': {\n",
    "        'çº¿æ€§æ¨¡å‹': ['LogisticRegression'],\n",
    "        'æ ‘æ¨¡å‹': ['DecisionTree', 'RandomForest', 'GradientBoosting'],\n",
    "        'å®ä¾‹å­¦ä¹ ': ['KNeighbors'],\n",
    "        'æ”¯æŒå‘é‡æœº': ['SVC'],\n",
    "        'æœ´ç´ è´å¶æ–¯': ['GaussianNB'],\n",
    "        'ç¥ç»ç½‘ç»œ': ['MLPClassifier']\n",
    "    },\n",
    "    'å›å½’ç®—æ³•': {\n",
    "        'çº¿æ€§å›å½’': ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet'],\n",
    "        'æ ‘å›å½’': ['DecisionTreeRegressor', 'RandomForestRegressor'],\n",
    "        'æ”¯æŒå‘é‡å›å½’': ['SVR']\n",
    "    },\n",
    "    'èšç±»ç®—æ³•': {\n",
    "        'åˆ’åˆ†èšç±»': ['KMeans'],\n",
    "        'å±‚æ¬¡èšç±»': ['AgglomerativeClustering'],\n",
    "        'å¯†åº¦èšç±»': ['DBSCAN'],\n",
    "        'æ··åˆæ¨¡å‹': ['GaussianMixture']\n",
    "    },\n",
    "    'é™ç»´æŠ€æœ¯': {\n",
    "        'çº¿æ€§é™ç»´': ['PCA', 'TruncatedSVD'],\n",
    "        'éçº¿æ€§é™ç»´': ['t-SNE'],\n",
    "        'ç‰¹å¾é€‰æ‹©': ['SelectKBest', 'RFE']\n",
    "    },\n",
    "    'æ¨¡å‹é€‰æ‹©': {\n",
    "        'äº¤å‰éªŒè¯': ['KFold', 'StratifiedKFold'],\n",
    "        'è¶…å‚æ•°ä¼˜åŒ–': ['GridSearchCV', 'RandomizedSearchCV'],\n",
    "        'æ¨¡å‹è¯„ä¼°': ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ“š æœ¬æ•™ç¨‹æ¶µç›–çš„çŸ¥è¯†ä½“ç³»:\")\n",
    "for main_topic, subtopics in knowledge_map.items():\n",
    "    print(f\"\\n{main_topic}:\")\n",
    "    for subtopic, methods in subtopics.items():\n",
    "        print(f\"  â€¢ {subtopic}: {', '.join(methods)}\")\n",
    "\n",
    "# 10.2 æœ€ä½³å®è·µæ€»ç»“\n",
    "print(f\"\\n=== æœºå™¨å­¦ä¹ æœ€ä½³å®è·µæ€»ç»“ ===\")\n",
    "\n",
    "best_practices = {\n",
    "    'æ•°æ®å‡†å¤‡é˜¶æ®µ': [\n",
    "        'å……åˆ†ç†è§£æ•°æ®å’Œä¸šåŠ¡é—®é¢˜',\n",
    "        'è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)',\n",
    "        'æ£€æŸ¥æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§',\n",
    "        'åˆç†å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼',\n",
    "        'ç¡®ä¿æ•°æ®åˆ†å‰²çš„éšæœºæ€§å’Œä»£è¡¨æ€§'\n",
    "    ],\n",
    "    'ç‰¹å¾å·¥ç¨‹é˜¶æ®µ': [\n",
    "        'æ ¹æ®ç®—æ³•ç‰¹æ€§é€‰æ‹©åˆé€‚çš„ç‰¹å¾ç¼©æ”¾æ–¹æ³•',\n",
    "        'æ­£ç¡®ç¼–ç åˆ†ç±»å˜é‡',\n",
    "        'åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾ç»„åˆ',\n",
    "        'ä½¿ç”¨ç‰¹å¾é€‰æ‹©å‡å°‘ç»´åº¦è¯…å’’',\n",
    "        'é¿å…æ•°æ®æ³„éœ²(Data Leakage)'\n",
    "    ],\n",
    "    'æ¨¡å‹é€‰æ‹©é˜¶æ®µ': [\n",
    "        'ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦',\n",
    "        'ä½¿ç”¨å¤šä¸ªç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•',\n",
    "        'é‡è§†æ¨¡å‹çš„å¯è§£é‡Šæ€§éœ€æ±‚',\n",
    "        'è€ƒè™‘è®¡ç®—æ•ˆç‡å’Œéƒ¨ç½²çº¦æŸ',\n",
    "        'å¹³è¡¡åå·®å’Œæ–¹å·®'\n",
    "    ],\n",
    "    'æ¨¡å‹éªŒè¯é˜¶æ®µ': [\n",
    "        'ä½¿ç”¨åˆ†å±‚äº¤å‰éªŒè¯ç¡®ä¿ç»“æœå¯é ',\n",
    "        'å…³æ³¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡è€Œéå•ä¸€æŒ‡æ ‡',\n",
    "        'ç»˜åˆ¶å­¦ä¹ æ›²çº¿è¯Šæ–­è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ',\n",
    "        'åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šæœ€ç»ˆéªŒè¯',\n",
    "        'è¿›è¡Œé”™è¯¯åˆ†æç†è§£æ¨¡å‹ç¼ºé™·'\n",
    "    ],\n",
    "    'è¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µ': [\n",
    "        'å…ˆä½¿ç”¨éšæœºæœç´¢å¿«é€Ÿæ¢ç´¢å‚æ•°ç©ºé—´',\n",
    "        'å†ä½¿ç”¨ç½‘æ ¼æœç´¢ç²¾ç»†ä¼˜åŒ–',\n",
    "        'è®¾ç½®åˆç†çš„æœç´¢èŒƒå›´',\n",
    "        'ä½¿ç”¨åµŒå¥—äº¤å‰éªŒè¯é¿å…è¿‡æ‹Ÿåˆ',\n",
    "        'è€ƒè™‘è®¡ç®—èµ„æºå’Œæ—¶é—´çº¦æŸ'\n",
    "    ],\n",
    "    'æ¨¡å‹éƒ¨ç½²é˜¶æ®µ': [\n",
    "        'ä¿å­˜å®Œæ•´çš„é¢„å¤„ç†æµæ°´çº¿',\n",
    "        'å»ºç«‹æ¨¡å‹ç›‘æ§å’Œé¢„è­¦æœºåˆ¶',\n",
    "        'å‡†å¤‡æ¨¡å‹å›æ»šæ–¹æ¡ˆ',\n",
    "        'å®šæœŸé‡è®­ç»ƒå’Œæ›´æ–°æ¨¡å‹',\n",
    "        'å»ºç«‹A/Bæµ‹è¯•æ¡†æ¶'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for stage, practices in best_practices.items():\n",
    "    print(f\"\\n{stage}:\")\n",
    "    for i, practice in enumerate(practices, 1):\n",
    "        print(f\"  {i}. {practice}\")\n",
    "\n",
    "# 10.3 å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ\n",
    "print(f\"\\n=== å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ ===\")\n",
    "\n",
    "common_issues = {\n",
    "    'æ•°æ®ç›¸å…³é—®é¢˜': {\n",
    "        'ç±»åˆ«ä¸å¹³è¡¡': [\n",
    "            'ä½¿ç”¨åˆ†å±‚æŠ½æ ·',\n",
    "            'è°ƒæ•´ç±»åˆ«æƒé‡(class_weight=\"balanced\")',\n",
    "            'ä½¿ç”¨SMOTEç­‰é‡é‡‡æ ·æŠ€æœ¯',\n",
    "            'é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡(F1, AUCç­‰)'\n",
    "        ],\n",
    "        'ç‰¹å¾ç»´åº¦è¿‡é«˜': [\n",
    "            'ä½¿ç”¨ç‰¹å¾é€‰æ‹©æŠ€æœ¯',\n",
    "            'åº”ç”¨PCAç­‰é™ç»´æ–¹æ³•',\n",
    "            'ä½¿ç”¨L1æ­£åˆ™åŒ–(Lasso)',\n",
    "            'å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®'\n",
    "        ],\n",
    "        'æ•°æ®æ³„éœ²': [\n",
    "            'ç¡®ä¿æ—¶é—´åºåˆ—æ•°æ®çš„æ­£ç¡®åˆ†å‰²',\n",
    "            'é¿å…ä½¿ç”¨æœªæ¥ä¿¡æ¯',\n",
    "            'ä»”ç»†æ£€æŸ¥ç‰¹å¾æ¥æº',\n",
    "            'ä½¿ç”¨ç®¡é“ç¡®ä¿é¢„å¤„ç†é¡ºåº'\n",
    "        ]\n",
    "    },\n",
    "    'æ¨¡å‹æ€§èƒ½é—®é¢˜': {\n",
    "        'è¿‡æ‹Ÿåˆ': [\n",
    "            'å¢åŠ è®­ç»ƒæ•°æ®',\n",
    "            'ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯',\n",
    "            'å‡å°‘æ¨¡å‹å¤æ‚åº¦',\n",
    "            'ä½¿ç”¨é›†æˆæ–¹æ³•',\n",
    "            'åº”ç”¨æ—©åœç­–ç•¥'\n",
    "        ],\n",
    "        'æ¬ æ‹Ÿåˆ': [\n",
    "            'å¢åŠ æ¨¡å‹å¤æ‚åº¦',\n",
    "            'åˆ›å»ºæ›´å¤šç‰¹å¾',\n",
    "            'å‡å°‘æ­£åˆ™åŒ–å¼ºåº¦',\n",
    "            'é€‰æ‹©æ›´å¼ºå¤§çš„ç®—æ³•',\n",
    "            'å¢åŠ è®­ç»ƒæ—¶é—´'\n",
    "        ],\n",
    "        'æ³›åŒ–èƒ½åŠ›å·®': [\n",
    "            'ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°',\n",
    "            'å¢åŠ éªŒè¯æ•°æ®çš„å¤šæ ·æ€§',\n",
    "            'å‡å°‘æ¨¡å‹å¤æ‚åº¦',\n",
    "            'æ”¹è¿›ç‰¹å¾å·¥ç¨‹',\n",
    "            'ä½¿ç”¨é›†æˆå­¦ä¹ '\n",
    "        ]\n",
    "    },\n",
    "    'è®¡ç®—æ•ˆç‡é—®é¢˜': {\n",
    "        'è®­ç»ƒæ—¶é—´è¿‡é•¿': [\n",
    "            'ä½¿ç”¨æ›´å¿«çš„ç®—æ³•',\n",
    "            'å‡å°‘ç‰¹å¾æ•°é‡',\n",
    "            'ä½¿ç”¨æ•°æ®å­é›†',\n",
    "            'å¹¶è¡Œè®¡ç®—(n_jobs=-1)',\n",
    "            'ä½¿ç”¨å¢é‡å­¦ä¹ ç®—æ³•'\n",
    "        ],\n",
    "        'å†…å­˜ä¸è¶³': [\n",
    "            'åˆ†æ‰¹å¤„ç†æ•°æ®',\n",
    "            'ä½¿ç”¨ç¨€ç–çŸ©é˜µ',\n",
    "            'å‡å°‘ç‰¹å¾ç»´åº¦',\n",
    "            'ä½¿ç”¨å†…å­˜æ˜ å°„',\n",
    "            'é€‰æ‹©å†…å­˜å‹å¥½çš„ç®—æ³•'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, issues in common_issues.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for issue, solutions in issues.items():\n",
    "        print(f\"  é—®é¢˜: {issue}\")\n",
    "        for solution in solutions:\n",
    "            print(f\"    â€¢ {solution}\")\n",
    "\n",
    "# 10.4 ç®—æ³•é€‰æ‹©æŒ‡å—\n",
    "print(f\"\\n=== ç®—æ³•é€‰æ‹©æŒ‡å— ===\")\n",
    "\n",
    "algorithm_guide = {\n",
    "    'æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©': {\n",
    "        'äºŒåˆ†ç±»é—®é¢˜': ['LogisticRegression', 'SVM', 'RandomForest'],\n",
    "        'å¤šåˆ†ç±»é—®é¢˜': ['LogisticRegression', 'RandomForest', 'GradientBoosting'],\n",
    "        'å›å½’é—®é¢˜': ['LinearRegression', 'RandomForestRegressor', 'SVR'],\n",
    "        'èšç±»é—®é¢˜': ['KMeans', 'DBSCAN', 'AgglomerativeClustering'],\n",
    "        'é™ç»´é—®é¢˜': ['PCA', 't-SNE', 'SelectKBest']\n",
    "    },\n",
    "    'æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©': {\n",
    "        'å°æ•°æ®é›†(<1000æ ·æœ¬)': ['KNN', 'NaiveBayes', 'LinearRegression'],\n",
    "        'ä¸­ç­‰æ•°æ®é›†(1000-100k)': ['SVM', 'RandomForest', 'GradientBoosting'],\n",
    "        'å¤§æ•°æ®é›†(>100kæ ·æœ¬)': ['LogisticRegression', 'SGD', 'LinearSVM'],\n",
    "        'é«˜ç»´æ•°æ®': ['LinearModels', 'SVM', 'NaiveBayes'],\n",
    "        'éçº¿æ€§å…³ç³»': ['RandomForest', 'SVM(RBF)', 'NeuralNetwork']\n",
    "    },\n",
    "    'æ ¹æ®æ€§èƒ½è¦æ±‚é€‰æ‹©': {\n",
    "        'éœ€è¦æ¦‚ç‡è¾“å‡º': ['LogisticRegression', 'RandomForest', 'GaussianNB'],\n",
    "        'éœ€è¦å¯è§£é‡Šæ€§': ['LinearRegression', 'DecisionTree', 'LogisticRegression'],\n",
    "        'è¿½æ±‚æœ€é«˜ç²¾åº¦': ['GradientBoosting', 'RandomForest', 'SVM'],\n",
    "        'éœ€è¦å¿«é€Ÿé¢„æµ‹': ['LinearModels', 'NaiveBayes', 'KNN'],\n",
    "        'éœ€è¦å¿«é€Ÿè®­ç»ƒ': ['LinearModels', 'NaiveBayes']\n",
    "    }\n",
    "}\n",
    "\n",
    "for criterion, recommendations in algorithm_guide.items():\n",
    "    print(f\"\\n{criterion}:\")\n",
    "    for scenario, algorithms in recommendations.items():\n",
    "        print(f\"  {scenario}: {', '.join(algorithms)}\")\n",
    "\n",
    "# 10.5 è¿›é˜¶å­¦ä¹ è·¯å¾„\n",
    "print(f\"\\n=== è¿›é˜¶å­¦ä¹ è·¯å¾„ ===\")\n",
    "\n",
    "learning_path = {\n",
    "    'æ·±åŒ–Scikit-learn': [\n",
    "        'å­¦ä¹ Pipelineå’ŒColumnTransformerçš„é«˜çº§ç”¨æ³•',\n",
    "        'æŒæ¡è‡ªå®šä¹‰è½¬æ¢å™¨å’Œä¼°è®¡å™¨',\n",
    "        'ç ”ç©¶é›†æˆå­¦ä¹ çš„é«˜çº§æŠ€å·§',\n",
    "        'å­¦ä¹ åŠç›‘ç£å’Œä¸»åŠ¨å­¦ä¹ ',\n",
    "        'æŒæ¡æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•'\n",
    "    ],\n",
    "    'æ‰©å±•åˆ°æ·±åº¦å­¦ä¹ ': [\n",
    "        'å­¦ä¹ TensorFlow/KerasåŸºç¡€',\n",
    "        'æŒæ¡PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶',\n",
    "        'ç†è§£å·ç§¯ç¥ç»ç½‘ç»œ(CNN)',\n",
    "        'å­¦ä¹ å¾ªç¯ç¥ç»ç½‘ç»œ(RNN/LSTM)',\n",
    "        'æ¢ç´¢Transformeræ¶æ„'\n",
    "    ],\n",
    "    'ç‰¹åŒ–é¢†åŸŸåº”ç”¨': [\n",
    "        'è‡ªç„¶è¯­è¨€å¤„ç†(NLP): NLTK, spaCy, transformers',\n",
    "        'è®¡ç®—æœºè§†è§‰: OpenCV, PIL, torchvision',\n",
    "        'æ¨èç³»ç»Ÿ: surprise, lightfm',\n",
    "        'æ—¶é—´åºåˆ—: statsmodels, prophet',\n",
    "        'å¼ºåŒ–å­¦ä¹ : OpenAI Gym, stable-baselines3'\n",
    "    ],\n",
    "    'å·¥ç¨‹åŒ–å’Œéƒ¨ç½²': [\n",
    "        'å­¦ä¹ MLOpså·¥å…·é“¾: MLflow, DVC, Kubeflow',\n",
    "        'æŒæ¡æ¨¡å‹éƒ¨ç½²: Flask, FastAPI, Docker',\n",
    "        'å­¦ä¹ äº‘å¹³å°æœåŠ¡: AWS SageMaker, GCP AI Platform',\n",
    "        'æŒæ¡æ¨¡å‹ç›‘æ§å’ŒA/Bæµ‹è¯•',\n",
    "        'å­¦ä¹ åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ : Dask, Ray'\n",
    "    ],\n",
    "    'ç†è®ºåŸºç¡€åŠ å¼º': [\n",
    "        'æ·±å…¥ç†è§£ç»Ÿè®¡å­¦ä¹ ç†è®º',\n",
    "        'å­¦ä¹ ä¼˜åŒ–ç®—æ³•åŸç†',\n",
    "        'æŒæ¡æ¦‚ç‡å›¾æ¨¡å‹',\n",
    "        'ç ”ç©¶å› æœæ¨ç†æ–¹æ³•',\n",
    "        'å­¦ä¹ è´å¶æ–¯æœºå™¨å­¦ä¹ '\n",
    "    ]\n",
    "}\n",
    "\n",
    "for area, topics in learning_path.items():\n",
    "    print(f\"\\n{area}:\")\n",
    "    for i, topic in enumerate(topics, 1):\n",
    "        print(f\"  {i}. {topic}\")\n",
    "\n",
    "# 10.6 å®è·µé¡¹ç›®å»ºè®®\n",
    "print(f\"\\n=== å®è·µé¡¹ç›®å»ºè®® ===\")\n",
    "\n",
    "project_suggestions = {\n",
    "    'åˆçº§é¡¹ç›®(å·©å›ºåŸºç¡€)': [\n",
    "        'æˆ¿ä»·é¢„æµ‹: å›å½’åˆ†æå’Œç‰¹å¾å·¥ç¨‹',\n",
    "        'å®¢æˆ·ç»†åˆ†: èšç±»åˆ†æå’Œä¸šåŠ¡è§£é‡Š',\n",
    "        'åƒåœ¾é‚®ä»¶åˆ†ç±»: æ–‡æœ¬åˆ†ç±»å’Œç‰¹å¾æå–',\n",
    "        'è‚¡ç¥¨ä»·æ ¼é¢„æµ‹: æ—¶é—´åºåˆ—åˆ†æ',\n",
    "        'å›¾åƒåˆ†ç±»: ä¼ ç»ŸMLæ–¹æ³•vsæ·±åº¦å­¦ä¹ å¯¹æ¯”'\n",
    "    ],\n",
    "    'ä¸­çº§é¡¹ç›®(ç»¼åˆåº”ç”¨)': [\n",
    "        'æ¨èç³»ç»Ÿ: ååŒè¿‡æ»¤å’Œå†…å®¹æ¨è',\n",
    "        'æ¬ºè¯ˆæ£€æµ‹: ä¸å¹³è¡¡æ•°æ®å¤„ç†',\n",
    "        'æƒ…æ„Ÿåˆ†æ: NLPå’Œæœºå™¨å­¦ä¹ ç»“åˆ',\n",
    "        'é”€å”®é¢„æµ‹: å¤šå…ƒæ—¶é—´åºåˆ—åˆ†æ',\n",
    "        'ç”¨æˆ·æµå¤±é¢„æµ‹: ç”Ÿå­˜åˆ†æå’Œåˆ†ç±»æ¨¡å‹'\n",
    "    ],\n",
    "    'é«˜çº§é¡¹ç›®(å·¥ç¨‹åŒ–)': [\n",
    "        'ç«¯åˆ°ç«¯MLç®¡é“: ä»æ•°æ®åˆ°éƒ¨ç½²',\n",
    "        'å®æ—¶æ¨èç³»ç»Ÿ: åœ¨çº¿å­¦ä¹ å’Œå†·å¯åŠ¨',\n",
    "        'å¤šæ¨¡æ€å­¦ä¹ : æ–‡æœ¬+å›¾åƒ+è¡¨æ ¼æ•°æ®',\n",
    "        'AutoMLç³»ç»Ÿ: è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©',\n",
    "        'A/Bæµ‹è¯•å¹³å°: å®éªŒè®¾è®¡å’Œå› æœæ¨ç†'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for level, projects in project_suggestions.items():\n",
    "    print(f\"\\n{level}:\")\n",
    "    for i, project in enumerate(projects, 1):\n",
    "        print(f\"  {i}. {project}\")\n",
    "\n",
    "# 10.7 èµ„æºæ¨è\n",
    "print(f\"\\n=== å­¦ä¹ èµ„æºæ¨è ===\")\n",
    "\n",
    "resources = {\n",
    "    'å®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹': [\n",
    "        'Scikit-learnå®˜æ–¹æ–‡æ¡£: https://scikit-learn.org/',\n",
    "        'Scikit-learnæ•™ç¨‹: https://scikit-learn.org/stable/tutorial/',\n",
    "        'Pythonæ•°æ®ç§‘å­¦æ‰‹å†Œ: Jake VanderPlas',\n",
    "        'Hands-On Machine Learning: AurÃ©lien GÃ©ron'\n",
    "    ],\n",
    "    'åœ¨çº¿è¯¾ç¨‹': [\n",
    "        'Andrew Ngæœºå™¨å­¦ä¹ è¯¾ç¨‹ (Coursera)',\n",
    "        'Fast.aiå®ç”¨æœºå™¨å­¦ä¹ è¯¾ç¨‹',\n",
    "        'edX MITæœºå™¨å­¦ä¹ è¯¾ç¨‹',\n",
    "        'Kaggle Learnå…è´¹è¯¾ç¨‹'\n",
    "    ],\n",
    "    'å®è·µå¹³å°': [\n",
    "        'Kaggleç«èµ›å’Œæ•°æ®é›†',\n",
    "        'Google Colabå…è´¹GPUç¯å¢ƒ',\n",
    "        'GitHubå¼€æºé¡¹ç›®å­¦ä¹ ',\n",
    "        'Papers With Codeè®ºæ–‡ä»£ç å®ç°'\n",
    "    ],\n",
    "    'ç¤¾åŒºå’Œè®ºå›': [\n",
    "        'Stack Overflowé—®ç­”',\n",
    "        'Reddit r/MachineLearning',\n",
    "        'çŸ¥ä¹æœºå™¨å­¦ä¹ è¯é¢˜',\n",
    "        'Towards Data Science (Medium)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, resource_list in resources.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for resource in resource_list:\n",
    "        print(f\"  â€¢ {resource}\")\n",
    "\n",
    "# 10.8 ç»“è¯­\n",
    "print(f\"\\n=== ç»“è¯­ ===\")\n",
    "\n",
    "conclusion = \\\"\\\"\\\"\n",
    "ğŸ“ æ­å–œä½ å®Œæˆäº†è¿™ä¸ªå…¨é¢çš„Scikit-learnæ•™ç¨‹ï¼\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªæ•™ç¨‹ï¼Œä½ å·²ç»æŒæ¡äº†ï¼š\n",
    "âœ… æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’Œå·¥ä½œæµç¨‹\n",
    "âœ… æ•°æ®é¢„å¤„ç†çš„å„ç§æŠ€æœ¯å’Œæœ€ä½³å®è·µ\n",
    "âœ… ä¸»è¦æœºå™¨å­¦ä¹ ç®—æ³•çš„åŸç†å’Œåº”ç”¨\n",
    "âœ… æ¨¡å‹é€‰æ‹©ã€è¯„ä¼°å’Œä¼˜åŒ–çš„æ–¹æ³•\n",
    "âœ… å®Œæ•´é¡¹ç›®çš„å¼€å‘æµç¨‹\n",
    "\n",
    "è®°ä½ï¼Œæœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå®è·µæ€§å¾ˆå¼ºçš„å­¦ç§‘ï¼Œç†è®ºå­¦ä¹ åªæ˜¯å¼€å§‹ã€‚\n",
    "çœŸæ­£çš„æˆé•¿æ¥è‡ªäºï¼š\n",
    "â€¢ åŠ¨æ‰‹å®è·µå„ç§é¡¹ç›®\n",
    "â€¢ å‚ä¸å¼€æºç¤¾åŒº\n",
    "â€¢ æŒç»­å…³æ³¨æ–°æŠ€æœ¯å‘å±•\n",
    "â€¢ å°†å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­\n",
    "\n",
    "æœºå™¨å­¦ä¹ é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œä¿æŒå¥½å¥‡å¿ƒå’Œå­¦ä¹ çƒ­æƒ…ï¼Œ\n",
    "ä½ å°†åœ¨è¿™ä¸ªæ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸä¸­ä¸æ–­æˆé•¿ï¼\n",
    "\n",
    "ğŸš€ ç°åœ¨å°±å¼€å§‹ä½ çš„æœºå™¨å­¦ä¹ ä¹‹æ—…å§ï¼\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "print(conclusion)\n",
    "\n",
    "# åˆ›å»ºå­¦ä¹ æ£€æŸ¥æ¸…å•\n",
    "print(f\"\\nğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•:\")\n",
    "checklist = [\n",
    "    \"ç†è§£ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«\",\n",
    "    \"èƒ½å¤Ÿè¿›è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†\",\n",
    "    \"æŒæ¡è‡³å°‘3ç§åˆ†ç±»ç®—æ³•çš„ä½¿ç”¨\",\n",
    "    \"æŒæ¡è‡³å°‘2ç§å›å½’ç®—æ³•çš„ä½¿ç”¨\", \n",
    "    \"èƒ½å¤Ÿä½¿ç”¨èšç±»ç®—æ³•è¿›è¡Œå®¢æˆ·ç»†åˆ†\",\n",
    "    \"ç†è§£PCAç­‰é™ç»´æŠ€æœ¯çš„åŸç†å’Œåº”ç”¨\",\n",
    "    \"èƒ½å¤Ÿä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½\",\n",
    "    \"æŒæ¡ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–\",\n",
    "    \"èƒ½å¤Ÿè§£é‡Šæ¨¡å‹ç»“æœå’Œä¸šåŠ¡ä»·å€¼\",\n",
    "    \"å®Œæˆè‡³å°‘ä¸€ä¸ªç«¯åˆ°ç«¯çš„MLé¡¹ç›®\"\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"  {i:2d}. â–¡ {item}\")\n",
    "\n",
    "print(f\"\\nå‹¾é€‰å®Œæˆçš„é¡¹ç›®ï¼Œè§„åˆ’ä¸‹ä¸€æ­¥å­¦ä¹ è®¡åˆ’ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcdcba",
   "metadata": {},
   "source": [
    "## ğŸ“š æœºå™¨å­¦ä¹ ä¸Scikit-learnå­¦ä¹ èµ„æº\n",
    "\n",
    "### ğŸ¯ Scikit-learnå®˜æ–¹èµ„æº\n",
    "- [**Scikit-learnå®˜æ–¹æ–‡æ¡£**](https://scikit-learn.org/stable/) - æœ€æƒå¨çš„APIæ–‡æ¡£å’Œç”¨æˆ·æŒ‡å—\n",
    "- [**Scikit-learn Examples**](https://scikit-learn.org/stable/auto_examples/) - å®˜æ–¹ç¤ºä¾‹ä»£ç åº“\n",
    "- [**Scikit-learn User Guide**](https://scikit-learn.org/stable/user_guide.html) - è¯¦ç»†çš„ç®—æ³•ä»‹ç»å’Œä½¿ç”¨æŒ‡å—\n",
    "- [**Scikit-learn Tutorials**](https://scikit-learn.org/stable/tutorial/) - å®˜æ–¹æ•™ç¨‹ç³»åˆ—\n",
    "\n",
    "### ğŸ“– æœºå™¨å­¦ä¹ ç»å…¸æ•™ç¨‹\n",
    "- [**Machine Learning Course by Andrew Ng**](https://www.coursera.org/learn/machine-learning) - Courseraæœ€ç»å…¸çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹\n",
    "- [**CS229 Stanford ML**](http://cs229.stanford.edu/) - æ–¯å¦ç¦å¤§å­¦æœºå™¨å­¦ä¹ è¯¾ç¨‹èµ„æ–™\n",
    "- [**æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹**](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html) - å°å¤§ä¸­æ–‡æœºå™¨å­¦ä¹ è¯¾ç¨‹\n",
    "- [**MIT 6.034 Artificial Intelligence**](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/) - MITäººå·¥æ™ºèƒ½è¯¾ç¨‹\n",
    "\n",
    "### ğŸ“š æ¨èæ•™æå’Œä¹¦ç±\n",
    "- [**Hands-On Machine Learning**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) - AurÃ©lien GÃ©ronè‘—ï¼Œå®æˆ˜æ€§å¾ˆå¼º\n",
    "- [**The Elements of Statistical Learning**](https://hastie.su.stanford.edu/ElemStatLearn/) - ç»Ÿè®¡å­¦ä¹ ç»å…¸æ•™æ(å…è´¹)\n",
    "- [**Pattern Recognition and Machine Learning**](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) - Christopher Bishopçš„ç»å…¸æ•™æ\n",
    "- [**Introduction to Statistical Learning**](https://www.statlearning.com/) - ç»Ÿè®¡å­¦ä¹ å¯¼è®º(å…è´¹åœ¨çº¿ç‰ˆ)\n",
    "\n",
    "### ğŸ› ï¸ å®è·µå’Œé¡¹ç›®\n",
    "- [**Kaggle Learn**](https://www.kaggle.com/learn) - å…è´¹çš„æœºå™¨å­¦ä¹ å¾®è¯¾ç¨‹\n",
    "- [**Machine Learning Mastery**](https://machinelearningmastery.com/) - Jason Brownleeçš„å®æˆ˜æ•™ç¨‹\n",
    "- [**Towards Data Science**](https://towardsdatascience.com/) - Mediumä¸Šçš„æ•°æ®ç§‘å­¦æ–‡ç« \n",
    "- [**Papers with Code**](https://paperswithcode.com/) - è®ºæ–‡+ä»£ç å®ç°\n",
    "\n",
    "### ğŸ† ç«èµ›å’Œæ•°æ®é›†\n",
    "- [**Kaggle Competitions**](https://www.kaggle.com/competitions) - æ•°æ®ç§‘å­¦ç«èµ›å¹³å°\n",
    "- [**UCI ML Repository**](https://archive.ics.uci.edu/ml/) - ç»å…¸æœºå™¨å­¦ä¹ æ•°æ®é›†\n",
    "- [**Google Dataset Search**](https://datasetsearch.research.google.com/) - Googleæ•°æ®é›†æœç´¢\n",
    "- [**Awesome Public Datasets**](https://github.com/awesomedata/awesome-public-datasets) - ä¼˜è´¨å…¬å¼€æ•°æ®é›†\n",
    "\n",
    "### ğŸ”¬ ç®—æ³•æ·±å…¥å­¦ä¹ \n",
    "- [**Algorithm Visualizations**](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html) - ç®—æ³•å¯è§†åŒ–\n",
    "- [**Machine Learning Yearning**](https://www.deeplearning.ai/machine-learning-yearning/) - Andrew Ngçš„å®è·µæŒ‡å—\n",
    "- [**Interpretable ML Book**](https://christophm.github.io/interpretable-ml-book/) - å¯è§£é‡Šæœºå™¨å­¦ä¹ \n",
    "- [**Feature Engineering Book**](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) - ç‰¹å¾å·¥ç¨‹æŒ‡å—\n",
    "\n",
    "### ğŸ Pythonæ•°æ®ç§‘å­¦ç”Ÿæ€\n",
    "- [**Pandas Documentation**](https://pandas.pydata.org/docs/) - æ•°æ®å¤„ç†åˆ©å™¨\n",
    "- [**NumPy User Guide**](https://numpy.org/doc/stable/user/) - æ•°å€¼è®¡ç®—åŸºç¡€\n",
    "- [**Matplotlib Gallery**](https://matplotlib.org/stable/gallery/) - å¯è§†åŒ–ç¤ºä¾‹\n",
    "- [**Seaborn Tutorial**](https://seaborn.pydata.org/tutorial.html) - ç»Ÿè®¡å¯è§†åŒ–\n",
    "\n",
    "### ğŸ“Š å¯è§†åŒ–å’Œè§£é‡Š\n",
    "- [**Plotly Python**](https://plotly.com/python/) - äº¤äº’å¼å¯è§†åŒ–\n",
    "- [**SHAP (SHapley Additive exPlanations)**](https://shap.readthedocs.io/) - æ¨¡å‹è§£é‡Šå·¥å…·\n",
    "- [**LIME**](https://github.com/marcotcr/lime) - å±€éƒ¨å¯è§£é‡Šæœºå™¨å­¦ä¹ \n",
    "- [**Yellowbrick**](https://www.scikit-yb.org/) - æœºå™¨å­¦ä¹ å¯è§†åŒ–\n",
    "\n",
    "### ğŸ‡¨ğŸ‡³ ä¸­æ–‡å­¦ä¹ èµ„æº\n",
    "- [**æœºå™¨å­¦ä¹ å®æˆ˜**](https://github.com/apachecn/MachineLearning) - ApacheCNä¸­æ–‡æ•™ç¨‹\n",
    "- [**æœºå™¨å­¦ä¹ è¥¿ç“œä¹¦**](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm) - å‘¨å¿—åæ•™æˆè‘—ä½œ\n",
    "- [**ç»Ÿè®¡å­¦ä¹ æ–¹æ³•**](https://book.douban.com/subject/10590856/) - æèˆªæ•™æˆè‘—ä½œ\n",
    "- [**Pythonæœºå™¨å­¦ä¹ åŸºç¡€æ•™ç¨‹**](https://github.com/amueller/introduction_to_ml_with_python) - ä¸­æ–‡ç‰ˆé…å¥—èµ„æº\n",
    "\n",
    "### ğŸ“ åœ¨çº¿è¯¾ç¨‹å¹³å°\n",
    "- [**edX MIT Introduction to ML**](https://www.edx.org/course/introduction-to-machine-learning) - MITæœºå™¨å­¦ä¹ å¯¼è®º\n",
    "- [**Udacity ML Nanodegree**](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t) - æœºå™¨å­¦ä¹ çº³ç±³å­¦ä½\n",
    "- [**DataCamp**](https://www.datacamp.com/) - æ•°æ®ç§‘å­¦åœ¨çº¿å­¦ä¹ \n",
    "- [**Coursera ML Specialization**](https://www.coursera.org/specializations/machine-learning-introduction) - æ–°ç‰ˆæœºå™¨å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹\n",
    "\n",
    "### ğŸ§ª å®éªŒå’Œå·¥å…·\n",
    "- [**Google Colab**](https://colab.research.google.com/) - å…è´¹çš„äº‘ç«¯Jupyterç¯å¢ƒ\n",
    "- [**Jupyter Notebooks Gallery**](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks) - ä¼˜è´¨Jupyterç¬”è®°æœ¬\n",
    "- [**MLflow**](https://mlflow.org/) - æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†\n",
    "- [**Weights & Biases**](https://wandb.ai/) - å®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–\n",
    "\n",
    "### ğŸ“± ç§»åŠ¨å­¦ä¹ \n",
    "- [**Machine Learning Mastery Blog**](https://machinelearningmastery.com/blog/) - å®šæœŸæ›´æ–°çš„æŠ€æœ¯åšå®¢\n",
    "- [**KDnuggets**](https://www.kdnuggets.com/) - æ•°æ®ç§‘å­¦æ–°é—»å’Œæ•™ç¨‹\n",
    "- [**Analytics Vidhya**](https://www.analyticsvidhya.com/) - æ•°æ®ç§‘å­¦ç¤¾åŒº\n",
    "- [**DataScienceCentral**](https://www.datasciencecentral.com/) - æ•°æ®ç§‘å­¦èµ„è®¯å¹³å°\n",
    "\n",
    "è®°ä½ï¼š**æœºå™¨å­¦ä¹ æ˜¯ç†è®ºä¸å®è·µçš„ç»“åˆï¼Œå¤šåŠ¨æ‰‹ã€å¤šå®éªŒæ‰æ˜¯ç‹é“ï¼** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
